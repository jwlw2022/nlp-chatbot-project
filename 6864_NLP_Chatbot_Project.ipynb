{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "6864_NLP_Chatbot_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwlw2022/nlp-chatbot-project/blob/main/6864_NLP_Chatbot_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKujomfNfXLN"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prUa1Udrb3Vi",
        "outputId": "4d0732bc-3391-4c28-e7c2-28d009527aac"
      },
      "source": [
        "# %%bash\n",
        "# Logistics #2: install the transformers package, create a folder, download the dataset and a patch\n",
        "!pip install pytorch-pretrained-bert\n",
        "!pip -q install transformers\n",
        "!pip -q install datasets\n",
        "!pip -q install tqdm\n",
        "!pip -q install sentencepiece "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 18.5MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 25.4MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 24.2MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 18.6MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 16.1MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 18.3MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 14.1MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 15.1MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 13.5MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 14.6MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 14.6MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 14.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 14.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/40/df7a3acc7e0916632c4cd689ccbd98f9edfcb2740b7244033442f44c658e/boto3-1.17.72-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 19.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.1+cu101)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 7.4MB/s \n",
            "\u001b[?25hCollecting botocore<1.21.0,>=1.20.72\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/e7/9572f20a7eabaaf1b289d1647c388dc165831478b156902539f310f08f02/botocore-1.20.72-py2.py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 20.9MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.72->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.72->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.20.72 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.17.72 botocore-1.20.72 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.4.2\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 15.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 49.1MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 54.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 225kB 13.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 24.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 26.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 15.5MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXbQMJ1ZfdOh"
      },
      "source": [
        "# Pretrained tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXVSBuG_eWBW"
      },
      "source": [
        "# import transformers\n",
        "\n",
        "# Use a pretrained tokenizer with CLASS.from_pretrained() function\n",
        "# tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-cased')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BAhfXsifgTN"
      },
      "source": [
        "# Download PersonaChat dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxHQxGgYZWM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54ddcd23-a0a8-4f05-ce8c-da468494a754"
      },
      "source": [
        "import json\n",
        "from pytorch_pretrained_bert import cached_path\n",
        "\n",
        "url = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
        "\n",
        "# Download and load JSON dataset\n",
        "personachat_file = cached_path(url)\n",
        "with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    dataset = json.loads(f.read())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 209850483/209850483 [00:08<00:00, 24717712.17B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEfiMNY-0oat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46298098-a6c4-4891-8c13-1c0900411fef"
      },
      "source": [
        "for i in range(5):\n",
        "    print('Dialogue ', i)\n",
        "    # print('Persona: ')\n",
        "    # for persona in dataset['train'][i]['personality']:\n",
        "    #     print(persona)\n",
        "    print('Utterances: ')\n",
        "    for dialogue in dataset['train'][i]['utterances']:\n",
        "        print(dialogue['history'][-1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dialogue  0\n",
            "Utterances: \n",
            "hi , how are you doing ? i'm getting ready to do some cheetah chasing to stay in shape .\n",
            "i am ! for my hobby i like to do canning or some whittling .\n",
            "that's neat . when i was in high school i placed 6th in 100m dash !\n",
            "i do not . but i do have a favorite meat since that is all i eat exclusively .\n",
            "i would have to say its prime rib . do you have any favorite foods ?\n",
            "do you have anything planned for today ? i think i am going to do some canning .\n",
            "i think i will can some jam . do you also play footfall for fun ?\n",
            "Dialogue  1\n",
            "Utterances: \n",
            "hi , how are you doing today ?\n",
            "wow , four sisters . just watching game of thrones .\n",
            "i agree . what do you do for a living ?\n",
            "interesting . i'm a website designer . pretty much spend all my time on the computer .\n",
            "that's awesome . i have always had a love for technology .\n",
            "i really enjoy free diving , how about you , have any hobbies ?\n",
            "that's nice . moms are pretty cool too .\n",
            "Dialogue  2\n",
            "Utterances: \n",
            "we all live in a yellow submarine , a yellow submarine . morning !\n",
            "lol . i am shy , anything to break the ice , and i am a beatles fan .\n",
            "really ? what shows ? i like tv , it makes me forget i do not like my family\n",
            "i did too . i do not get along with mine . they have no class .\n",
            "put the lime in the coconut as well . . .\n",
            "i prefer mojitos . watermelon or cucumber .\n",
            "Dialogue  3\n",
            "Utterances: \n",
            "hi ! i work as a gourmet cook .\n",
            "really . but , i can sing pitch perfect .\n",
            "great ! i had won an award for spelling bee .\n",
            "okay but i was published in new yorker once\n",
            "i have not . i can cook any word you want me to\n",
            "i'm asian and have no hair .\n",
            "i love carrots . i eat carrots like a horse .\n",
            "i work as a gourmet cook who also has a pitch perfect voice .\n",
            "Dialogue  4\n",
            "Utterances: \n",
            "how are you doing today\n",
            "i like to watch kids\n",
            "what do you weld ? houses ?\n",
            "what is your secret that you have\n",
            "how does that feel for you\n",
            "i bet that it does\n",
            "i watch kids for a living\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhdDvpg3-FcI",
        "outputId": "e5735aa1-fed9-4ebb-afbf-678da258114b"
      },
      "source": [
        "def tokenize(dataset):\n",
        "    train_tokens = []\n",
        "    for i in range(len(dataset['train'])):  # dialogues\n",
        "        for dialogue in dataset['train'][i]['utterances']:\n",
        "            tokens = dialogue['history'][-1].split(' ')\n",
        "            train_tokens.append(tokens)\n",
        "    \n",
        "    valid_tokens = []\n",
        "    for i in range(len(dataset['valid'])):  # dialogues\n",
        "        for dialogue in dataset['valid'][i]['utterances']:\n",
        "            tokens = dialogue['history'][-1].split(' ')\n",
        "            valid_tokens.append(tokens)\n",
        "\n",
        "    train_source = []\n",
        "    train_target = []\n",
        "    for i in range(len(train_tokens)-2):\n",
        "        copy = train_tokens[i].copy()\n",
        "        copy.extend(['|'])\n",
        "        copy.extend(train_tokens[i+1])\n",
        "        train_source.append(copy)\n",
        "        train_target.append(train_tokens[i+2])\n",
        "\n",
        "    valid_source = []\n",
        "    valid_target = []\n",
        "    for i in range(len(valid_tokens)-2):\n",
        "        copy = valid_tokens[i].copy()\n",
        "        copy.extend(['|'])\n",
        "        copy.extend(valid_tokens[i+1])\n",
        "        valid_source.append(copy)\n",
        "        valid_target.append(valid_tokens[i+2])\n",
        "\n",
        "    return train_source, train_target, valid_source, valid_target\n",
        "\n",
        "print(len(dataset['train']))\n",
        "train_source, train_target, valid_source, valid_target = tokenize(dataset)\n",
        "print(train_source[0])\n",
        "print(len(train_source))\n",
        "print(len(train_target))\n",
        "print(len(valid_source))\n",
        "print(len(valid_target))\n",
        "# print(train_target)\n",
        "# print(valid_source)\n",
        "# print(valid_target)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17878\n",
            "['hi', ',', 'how', 'are', 'you', 'doing', '?', \"i'm\", 'getting', 'ready', 'to', 'do', 'some', 'cheetah', 'chasing', 'to', 'stay', 'in', 'shape', '.', '|', 'i', 'am', '!', 'for', 'my', 'hobby', 'i', 'like', 'to', 'do', 'canning', 'or', 'some', 'whittling', '.']\n",
            "131436\n",
            "131436\n",
            "7799\n",
            "7799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7IfNMtlEYQ7",
        "outputId": "6dc2fd57-c616-44df-bd7e-44d0922ee385"
      },
      "source": [
        "# a = max(list(map(len, train_source)))\n",
        "# print(a)\n",
        "\n",
        "a = max([len(x) for x in train_source])\n",
        "print(a)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkLOulsuge-2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6fgzQE6hA8p"
      },
      "source": [
        "# Seq2Seq Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvKACvyBRH2q"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBa2QRPi7AIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b117cc39-9b49-4b4a-ee59-f756855c6990"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "MODEL_FOLDER = \"/content/drive/My Drive/mit-6864/hw3\"\n",
        "!mkdir -p \"/content/drive/My Drive/mit-6864/hw3\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzzPi7ouR9rJ",
        "outputId": "4407286e-09d9-467d-d916-800ac9c3ebc5"
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/mit-6864/hw3.git\n",
        "mkdir -p /content/hw3/data\n",
        "\n",
        "pip install sacrebleu"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: portalocker, sacrebleu\n",
            "Successfully installed portalocker-2.0.0 sacrebleu-1.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hw3'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suepLPcGR27_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14f1bd35-62a3-4603-830d-17663530f8e4"
      },
      "source": [
        "# Download data\n",
        "DATA_DIR = \"/content/hw3/data\"\n",
        "\n",
        "# !wget -nv -O \"$DATA_DIR/train.en\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
        "# !wget -nv -O \"$DATA_DIR/train.vi\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n",
        "!wget -nv -O \"$DATA_DIR/vocab.en\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en\n",
        "# !wget -nv -O \"$DATA_DIR/vocab.vi\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.vi"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-12 20:33:10 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en [139741/139741] -> \"/content/hw3/data/vocab.en\" [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsGbOGWTs3sB",
        "outputId": "328ab56a-e446-4497-9eb3-7a2b4d262305"
      },
      "source": [
        "words = set()\n",
        "for sentence in train_source:\n",
        "  words.update(sentence)\n",
        "for sentence in train_target:\n",
        "  words.update(sentence)\n",
        "for sentence in valid_source:\n",
        "  words.update(sentence)\n",
        "for sentence in valid_target:\n",
        "  words.update(sentence)\n",
        "print(len(words))\n",
        "with open('/content/hw3/data/vocab.txt', 'w') as writefile:\n",
        "    writefile.write('\\n'.join(words))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFWyGs6kRxD7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "b5bb595b-4293-4707-b339-8225a95f5eb2"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/hw3\")\n",
        "\n",
        "import lab_utils\n",
        "\n",
        "import torch \n",
        "# !pip install torch==1.6.0 torchvision==0.7.0\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "assert device == \"cuda\"   # use gpu whenever you can!\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "from lab_utils import read_vocab_file, read_sentence_file, filter_data, show_some_data_stats\n",
        "\n",
        "# src_vocab_set = read_vocab_file(\"vocab.vi\")\n",
        "# trg_vocab_set = read_vocab_file(\"vocab.en\")\n",
        "trg_vocab_set = list(words)\n",
        "src_vocab_set = trg_vocab_set\n",
        "\n",
        "train_src_sentences_list = train_source\n",
        "train_trg_sentences_list = train_target\n",
        "assert len(train_src_sentences_list) == len(train_trg_sentences_list)\n",
        "\n",
        "# test_src_sentences_list = read_sentence_file(\"tst2013.vi\")\n",
        "# test_trg_sentences_list = read_sentence_file(\"tst2013.en\")\n",
        "test_src_sentences_list = valid_source\n",
        "test_trg_sentences_list = valid_target\n",
        "assert len(test_src_sentences_list) == len(test_trg_sentences_list)\n",
        "\n",
        "# Filter out sentences over 48 words long\n",
        "MAX_SENT_LENGTH = 45\n",
        "MAX_SENT_LENGTH_PLUS_SOS_EOS = 46\n",
        "\n",
        "train_src_sentences_list, train_trg_sentences_list = filter_data(\n",
        "    train_src_sentences_list, train_trg_sentences_list, MAX_SENT_LENGTH)\n",
        "test_src_sentences_list, test_trg_sentences_list = filter_data(\n",
        "    test_src_sentences_list, test_trg_sentences_list, MAX_SENT_LENGTH)\n",
        "\n",
        "# We take 10% of training data as validation set.\n",
        "num_val = int(len(train_src_sentences_list) * 0.1)\n",
        "val_src_sentences_list = train_src_sentences_list[:num_val]\n",
        "val_trg_sentences_list = train_trg_sentences_list[:num_val]\n",
        "train_src_sentences_list = train_src_sentences_list[num_val:]\n",
        "train_trg_sentences_list = train_trg_sentences_list[num_val:]\n",
        "\n",
        "show_some_data_stats(train_src_sentences_list, val_src_sentences_list, \n",
        "                     test_src_sentences_list, train_trg_sentences_list,\n",
        "                     src_vocab_set, trg_vocab_set)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training (src, trg) sentence pairs: 118280\n",
            "Number of validation (src, trg) sentence pairs: 13142\n",
            "Number of testing (src, trg) sentence pairs: 7799\n",
            "Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 18598\n",
            "Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 18598\n",
            "Training sentence avg. length: 22 \n",
            "Training sentence length at 95-percentile: 34\n",
            "Training sentence length distribution (x-axis is length range and y-axis is count):\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Example Vietnamese input: ['that', 'is', 'great', '.', 'keep', 'up', 'with', 'news', 'plus', 'stay', 'in', 'shape', '|', 'hi', '.', 'sorry', 'in', 'advance', 'my', 'english', 'is', 'not', 'that', 'great', '.']\n",
            "Its target English output: [\"i'm\", 'okay', '.', 'i', 'just', 'took', 'my', 'dog', 'for', 'a', 'walk', '.', 'you', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDcyruI0UcXe"
      },
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "# These IDs are reserved.\n",
        "PAD_INDEX = 0\n",
        "UNK_INDEX = 1\n",
        "SOS_INDEX = 2\n",
        "EOS_INDEX = 3\n",
        "\n",
        "\n",
        "class MTDataset(data.Dataset):\n",
        "  def __init__(self, src_sentences, src_vocabs, trg_sentences, trg_vocabs,\n",
        "               sampling=1.):\n",
        "    self.src_sentences = src_sentences[:int(len(src_sentences) * sampling)]\n",
        "    self.trg_sentences = trg_sentences[:int(len(src_sentences) * sampling)]\n",
        "\n",
        "    self.max_src_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n",
        "    self.max_trg_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n",
        "\n",
        "    self.src_vocabs = src_vocabs\n",
        "    self.trg_vocabs = trg_vocabs\n",
        "\n",
        "    self.src_v2id = {v : i for i, v in enumerate(src_vocabs)}\n",
        "    self.src_id2v = {val : key for key, val in self.src_v2id.items()}\n",
        "    self.trg_v2id = {v : i for i, v in enumerate(trg_vocabs)}\n",
        "    self.trg_id2v = {val : key for key, val in self.trg_v2id.items()}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.src_sentences)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    src_sent = self.src_sentences[index]\n",
        "    src_len = len(src_sent) + 1\n",
        "    src_id = []\n",
        "    for w in src_sent:\n",
        "      if w == '|':\n",
        "        # src_id.append(EOS_INDEX)\n",
        "        # src_id.append(SOS_INDEX)\n",
        "        pass\n",
        "      else:\n",
        "        if w not in self.src_vocabs:\n",
        "          w = '<unk>'\n",
        "        src_id.append(self.src_v2id[w])\n",
        "    src_id = ([SOS_INDEX] + src_id + [EOS_INDEX] + [PAD_INDEX] *\n",
        "               (self.max_src_seq_length - src_len))\n",
        "    #src_id = (src_id + [PAD_INDEX] * (self.max_src_seq_length - src_len))\n",
        "\n",
        "    trg_sent = self.trg_sentences[index]\n",
        "    trg_len = len(trg_sent) + 2\n",
        "    trg_id = []\n",
        "    for w in trg_sent:\n",
        "      if w not in self.trg_vocabs:\n",
        "        w = '<unk>'\n",
        "      trg_id.append(self.trg_v2id[w])\n",
        "    trg_id = ([SOS_INDEX] + trg_id + [EOS_INDEX] + [PAD_INDEX] *\n",
        "               (self.max_trg_seq_length - trg_len))\n",
        "    #trg_id = (trg_id + [PAD_INDEX] * (self.max_trg_seq_length - trg_len))\n",
        "\n",
        "    return torch.tensor(src_id), src_len, torch.tensor(trg_id), trg_len"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1p99-z4UeJH"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xuHHZDDhCD9"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, dropout=0.):\n",
        "    \"\"\"\n",
        "    Inputs: \n",
        "      - `input_size`: an int representing the RNN input size.\n",
        "      - `hidden_size`: an int representing the RNN hidden size.\n",
        "      - `dropout`: a float representing the dropout rate during training. Note\n",
        "          that for 1-layer RNN this has no effect since dropout only applies to\n",
        "          outputs of intermediate layers.\n",
        "    \"\"\"\n",
        "    \n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    # --------- Your code here --------- #\n",
        "    # feel free to use a pre-implemented pytorch GRU\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    self.rnn = nn.GRU(input_size=input_size,\n",
        "                      hidden_size=hidden_size,\n",
        "                      num_layers=3,\n",
        "                      batch_first=True,\n",
        "                      dropout=dropout)\n",
        "    \n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "  def forward(self, inputs, lengths):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of source\n",
        "          sentences.\n",
        "      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n",
        "          lengths of `inputs`.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "        (batch_size, max_seq_length, hidden_size).\n",
        "      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n",
        "\n",
        "      Hint: `outputs` and `finals` are both standard GRU outputs.\n",
        "    \"\"\"\n",
        "    outputs = None\n",
        "    finals = None\n",
        "    \n",
        "    # --------- Your code here --------- #\n",
        "    # hint: you probably want to pack the inputs and outputs (see note below)\n",
        "    #       https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
        "    # hint2: given the shape of the inputs and outputs, \n",
        "    #        it might be helpful to specify batch_first=True (also in __init___)\n",
        "    # hint3: MAX_SENT_LENGTH_PLUS_SOS_EOS is a global variable that exists if \n",
        "    #        you ever need to specify a total_length for outputs\n",
        "\n",
        "    packed_sequence = pack_padded_sequence(inputs,\n",
        "                                           lengths.cpu(),\n",
        "                                           batch_first=True,\n",
        "                                           enforce_sorted=False)\n",
        "    outputs, finals = self.rnn(packed_sequence)\n",
        "    outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
        "    \n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "    return outputs, finals"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOGgHO0HRSiW"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCzMw3ghhKcw"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  \"\"\"An RNN decoder without attention.\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, dropout=0.):\n",
        "    \"\"\"\n",
        "      Inputs:\n",
        "        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n",
        "    \"\"\"\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    # --------- Your code here --------- #\n",
        "    # hint: you need more layers than the encoder\n",
        "    #       again, feel free to use pytorch implemetnations\n",
        "    #       https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "    \n",
        "    # To initialize from the final encoder state.\n",
        "\n",
        "    self.rnn = nn.GRU(input_size=input_size,\n",
        "                      hidden_size=hidden_size,\n",
        "                      num_layers=3,\n",
        "                      batch_first=True,\n",
        "                      dropout=dropout)\n",
        "\n",
        "    self.bridge = nn.Linear(hidden_size, input_size, bias=True)\n",
        "\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "  def forward_step(self, prev_embed, hidden):\n",
        "    \"\"\"Helper function for forward below:\n",
        "       Perform a single decoder step (1 word).\n",
        "\n",
        "       Inputs:\n",
        "      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size)\n",
        "          representing the padded embedded word vectors at this step in training\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the current hidden state.\n",
        "\n",
        "      Returns:\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the current decoder hidden state.\n",
        "      - `pre_output`: a 3d-tensor of shape (batch_size, 1, hidden_size)\n",
        "          representing the total decoder output for one step\n",
        "    \"\"\"\n",
        "    pre_output = None\n",
        "    # --------- Your code here --------- #\n",
        "    \n",
        "    pre_output, hidden = self.rnn(prev_embed, hidden)\n",
        "\n",
        "    # --------- Your code ends --------- #\n",
        "    return hidden, pre_output\n",
        "\n",
        "  def forward(self, inputs, encoder_finals, hidden=None, max_len=None):\n",
        "    \"\"\"Unroll the decoder one step at a time.\n",
        "\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of target\n",
        "          sentences (for teacher-forcing during training).\n",
        "      - `encoder_finals`: a 3d-tensor of shape\n",
        "          (num_enc_layers, batch_size, hidden_size) representing the final\n",
        "          encoder hidden states used to initialize the initial decoder hidden\n",
        "          states.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the value to be used to initialize the initial decoder hidden states.\n",
        "          If None, then use `encoder_finals`.\n",
        "      - `max_len`: an int representing the maximum decoding length.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the raw\n",
        "          decoder outputs (before converting to a `trg_vocab_size`-dim vector).\n",
        "          We will convert it later in a `Generator` below.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the last decoder hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # The maximum number of steps to unroll the RNN.\n",
        "    if max_len is None:\n",
        "      max_len = inputs.size(1)\n",
        "\n",
        "    # Initialize decoder hidden state.\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(encoder_finals)\n",
        "\n",
        "    outputs = []\n",
        "    \n",
        "    # --------- Your code here --------- #\n",
        "\n",
        "    # Unroll the decoder RNN for `max_len` steps.\n",
        "    # hint: use the above helper function forward_step that \n",
        "    #       performs a single decoder step (1 word).\n",
        "\n",
        "    for i in range(max_len):\n",
        "        prev_embed = inputs[:, i:i+1, :] # get embeddings from inputs\n",
        "        hidden, pre_output = self.forward_step(prev_embed, hidden)\n",
        "        outputs.append(pre_output)\n",
        "    \n",
        "    # final output concatenates outputs for each word\n",
        "    outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "    return hidden, outputs\n",
        "\n",
        "  def init_hidden(self, encoder_finals):\n",
        "    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n",
        "    state.\"\"\"\n",
        "    decoder_init_hiddens = torch.tanh(self.bridge(encoder_finals))\n",
        "\n",
        "    return decoder_init_hiddens"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_g_Mh7mRULr"
      },
      "source": [
        "## EncoderDecoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtN_OJxAhLVH"
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  \"\"\"A standard Encoder-Decoder architecture without attention.\n",
        "  \"\"\"\n",
        "  def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `encoder`: an `Encoder` object.\n",
        "      - `decoder`: a `Decoder` object.\n",
        "      - `src_embed`: an nn.Embedding object representing the lookup table for\n",
        "          input (source) sentences.\n",
        "      - `trg_embed`: an nn.Embedding object representing the lookup table for\n",
        "          output (target) sentences.\n",
        "      - `generator`: a `Generator` object. Essentially a linear mapping. See\n",
        "          the next code cell.\n",
        "    \"\"\"\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.trg_embed = trg_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, src_ids, trg_ids, src_lengths):\n",
        "    \"\"\"Take in and process masked source and target sequences.\n",
        "\n",
        "    Inputs:\n",
        "      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of source sentences of word ids.\n",
        "      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of target sentences of word ids.\n",
        "      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n",
        "        sequence length of `src_ids`.\n",
        "\n",
        "    Returns the decoder outputs, see the above cell.\n",
        "    \"\"\"\n",
        "    encoder_hiddens, encoder_finals = self.encode(src_ids, src_lengths)\n",
        "    del encoder_hiddens   # unused\n",
        "    return self.decode(encoder_finals, trg_ids[:, :-1])\n",
        "\n",
        "  def encode(self, src_ids, src_lengths):\n",
        "    return self.encoder(self.src_embed(src_ids), src_lengths)\n",
        "    \n",
        "  def decode(self, encoder_finals, trg_ids, decoder_hidden=None):\n",
        "    return self.decoder(self.trg_embed(trg_ids), encoder_finals, decoder_hidden)"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSwpEe1ORV4-"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx55R2LihLcp"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
        "  def __init__(self, hidden_size, vocab_size):\n",
        "    super(Generator, self).__init__()\n",
        "    self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv_504_2RZF3"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmTIw_g8TfW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b810bb26-d550-427a-8577-08cdd0bbfb4e"
      },
      "source": [
        "from torch.utils import data\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# You can try on a smaller training set by setting a smaller `sampling`.\n",
        "train_set = MTDataset(train_src_sentences_list, src_vocab_set,\n",
        "                      train_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "# print('TRAIN')\n",
        "# for i in range(len(train_set)):\n",
        "#   if len(train_set[i][0]) != 48 or len(train_set[i][2]) != 48:\n",
        "#     print(train_set[i])\n",
        "#     break\n",
        "# print(len(train_set))\n",
        "# print(train_set[0])\n",
        "# print(len(train_set[0][0])) # source\n",
        "# print(len(train_set[0][2])) # target\n",
        "# print(train_set[126])\n",
        "# print(len(train_set[126][0])) # source\n",
        "# print(len(train_set[126][2])) # target\n",
        "train_data_loader = data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                    num_workers=8, shuffle=True)\n",
        "\n",
        "val_set = MTDataset(val_src_sentences_list, src_vocab_set,\n",
        "                    val_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "# print('VAL')\n",
        "# for i in range(len(val_set)):\n",
        "#   if len(val_set[i][0]) != 48 or len(val_set[i][2]) != 48:\n",
        "#     print(val_set[i])\n",
        "#     break\n",
        "val_data_loader = data.DataLoader(val_set, batch_size=batch_size, num_workers=8,\n",
        "                                  shuffle=False)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9-IjIoghKan"
      },
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class SimpleLossCompute:\n",
        "  \"\"\"A simple loss compute and train function.\"\"\"\n",
        "\n",
        "  def __init__(self, generator, criterion, opt=None):\n",
        "    self.generator = generator\n",
        "    self.criterion = criterion\n",
        "    self.opt = opt\n",
        "\n",
        "  def __call__(self, x, y, norm):\n",
        "    x = self.generator(x)\n",
        "    loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
        "                          y.contiguous().view(-1))\n",
        "    loss = loss / norm\n",
        "\n",
        "    if self.opt is not None:  # training mode\n",
        "      loss.backward()          \n",
        "      self.opt.step()\n",
        "      self.opt.zero_grad()\n",
        "\n",
        "    return loss.data.item() * norm\n",
        "\n",
        "\n",
        "def run_epoch(data_loader, model, loss_compute, print_every):\n",
        "  \"\"\"Standard Training and Logging Function\"\"\"\n",
        "\n",
        "  total_tokens = 0\n",
        "  total_loss = 0\n",
        "\n",
        "  for i, (src_ids_BxT, src_lengths_B, trg_ids_BxL, trg_lengths_B) in enumerate(data_loader):\n",
        "    # We define some notations here to help you understand the loaded tensor\n",
        "    # shapes:\n",
        "    #   `B`: batch size\n",
        "    #   `T`: max sequence length of source sentences\n",
        "    #   `L`: max sequence length of target sentences; due to our preprocessing\n",
        "    #        in the beginning, `L` == `T` == 50\n",
        "    # An example of `src_ids_BxT` (when B = 2):\n",
        "    #   [[2, 4, 6, 7, ..., 4, 3, 0, 0, 0],\n",
        "    #    [2, 8, 6, 5, ..., 9, 5, 4, 3, 0]]\n",
        "    # The corresponding `src_lengths_B` would be [47, 49].\n",
        "    # Note that SOS_INDEX == 2, EOS_INDEX == 3, and PAD_INDEX = 0.\n",
        "\n",
        "    src_ids_BxT = src_ids_BxT.to(device)\n",
        "    src_lengths_B = src_lengths_B.to(device)\n",
        "    trg_ids_BxL = trg_ids_BxL.to(device)\n",
        "\n",
        "    del trg_lengths_B   # unused\n",
        "\n",
        "    _, output = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n",
        "\n",
        "    loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:],\n",
        "                        norm=src_ids_BxT.size(0))\n",
        "    total_loss += loss\n",
        "    total_tokens += (trg_ids_BxL[:, 1:] != PAD_INDEX).data.sum().item()\n",
        "\n",
        "    if model.training and i % print_every == 0:\n",
        "      print(\"Epoch Step: %d Loss: %f\" % (i, loss / src_ids_BxT.size(0)))\n",
        "\n",
        "  return math.exp(total_loss / float(total_tokens))\n",
        "\n",
        "\n",
        "def train(model, num_epochs, learning_rate, print_every):\n",
        "  # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when\n",
        "  # computing the loss.\n",
        "  criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
        "  optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Keep track of dev ppl for each epoch.\n",
        "  dev_ppls = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print(\"Epoch\", epoch)\n",
        "\n",
        "    model.train()\n",
        "    train_ppl = run_epoch(data_loader=train_data_loader, model=model,\n",
        "                          loss_compute=SimpleLossCompute(model.generator,\n",
        "                                                         criterion, optim),\n",
        "                          print_every=print_every)\n",
        "        \n",
        "    model.eval()\n",
        "    with torch.no_grad():      \n",
        "      dev_ppl = run_epoch(data_loader=val_data_loader, model=model,\n",
        "                          loss_compute=SimpleLossCompute(model.generator,\n",
        "                                                         criterion, None),\n",
        "                          print_every=print_every)\n",
        "      print(\"Validation perplexity: %f\" % dev_ppl)\n",
        "      dev_ppls.append(dev_ppl)\n",
        "        \n",
        "  return dev_ppls"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg0HolDAhKYJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3f6460b0-b0f8-4dcc-951a-7ba7e981c24e"
      },
      "source": [
        "device = 'cuda'\n",
        "\n",
        "# Hyperparameters for contructing the encoder-decoder model.\n",
        "embed_size = 256   # Each word will be represented as a `embed_size`-dim vector.\n",
        "hidden_size = 256  # RNN hidden size.\n",
        "dropout = 0.2\n",
        "\n",
        "pure_seq2seq = EncoderDecoder(\n",
        "    encoder=Encoder(embed_size, hidden_size, dropout=dropout),\n",
        "    decoder=Decoder(embed_size, hidden_size, dropout=dropout),\n",
        "    src_embed=nn.Embedding(len(src_vocab_set), embed_size),\n",
        "    trg_embed=nn.Embedding(len(trg_vocab_set), embed_size),\n",
        "    generator=Generator(hidden_size, len(trg_vocab_set))).to(device)\n",
        "\n",
        "train_model = True\n",
        "if train_model:\n",
        "  # Start training. The returned `dev_ppls` is a list of dev perplexity for each\n",
        "  # epoch.\n",
        "  pure_dev_ppls = train(pure_seq2seq, num_epochs=10, learning_rate=1e-3,\n",
        "                        print_every=100)\n",
        "  \n",
        "  torch.save(pure_seq2seq.state_dict(), MODEL_FOLDER+\"/\" + \"pure_seq2seq.pt\")\n",
        "\n",
        "  # Plot perplexity\n",
        "  lab_utils.plot_perplexity(pure_dev_ppls)\n",
        "else:\n",
        "  pure_seq2seq.load_state_dict(torch.load(MODEL_FOLDER+\"/\" + \"pure_seq2seq.pt\"))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 0 Loss: 115.288734\n",
            "Epoch Step: 100 Loss: 70.040306\n",
            "Epoch Step: 200 Loss: 68.361130\n",
            "Epoch Step: 300 Loss: 59.927689\n",
            "Epoch Step: 400 Loss: 57.736328\n",
            "Epoch Step: 500 Loss: 49.946785\n",
            "Epoch Step: 600 Loss: 50.618969\n",
            "Epoch Step: 700 Loss: 52.062267\n",
            "Epoch Step: 800 Loss: 52.103497\n",
            "Epoch Step: 900 Loss: 54.552235\n",
            "Validation perplexity: 69.957525\n",
            "Epoch 1\n",
            "Epoch Step: 0 Loss: 50.077312\n",
            "Epoch Step: 100 Loss: 52.202248\n",
            "Epoch Step: 200 Loss: 48.450439\n",
            "Epoch Step: 300 Loss: 45.890629\n",
            "Epoch Step: 400 Loss: 46.830956\n",
            "Epoch Step: 500 Loss: 46.588886\n",
            "Epoch Step: 600 Loss: 46.330894\n",
            "Epoch Step: 700 Loss: 46.684837\n",
            "Epoch Step: 800 Loss: 44.182678\n",
            "Epoch Step: 900 Loss: 42.509033\n",
            "Validation perplexity: 49.814913\n",
            "Epoch 2\n",
            "Epoch Step: 0 Loss: 45.457096\n",
            "Epoch Step: 100 Loss: 46.547241\n",
            "Epoch Step: 200 Loss: 43.050648\n",
            "Epoch Step: 300 Loss: 45.277775\n",
            "Epoch Step: 400 Loss: 40.396210\n",
            "Epoch Step: 500 Loss: 45.367744\n",
            "Epoch Step: 600 Loss: 45.199005\n",
            "Epoch Step: 700 Loss: 43.944557\n",
            "Epoch Step: 800 Loss: 42.443745\n",
            "Epoch Step: 900 Loss: 45.999672\n",
            "Validation perplexity: 44.668722\n",
            "Epoch 3\n",
            "Epoch Step: 0 Loss: 42.303219\n",
            "Epoch Step: 100 Loss: 46.726006\n",
            "Epoch Step: 200 Loss: 41.141037\n",
            "Epoch Step: 300 Loss: 41.043106\n",
            "Epoch Step: 400 Loss: 39.151314\n",
            "Epoch Step: 500 Loss: 42.415123\n",
            "Epoch Step: 600 Loss: 43.684460\n",
            "Epoch Step: 700 Loss: 41.599113\n",
            "Epoch Step: 800 Loss: 42.534279\n",
            "Epoch Step: 900 Loss: 47.369694\n",
            "Validation perplexity: 42.626106\n",
            "Epoch 4\n",
            "Epoch Step: 0 Loss: 39.136143\n",
            "Epoch Step: 100 Loss: 41.148228\n",
            "Epoch Step: 200 Loss: 44.170029\n",
            "Epoch Step: 300 Loss: 39.704395\n",
            "Epoch Step: 400 Loss: 44.451591\n",
            "Epoch Step: 500 Loss: 40.474648\n",
            "Epoch Step: 600 Loss: 40.530952\n",
            "Epoch Step: 700 Loss: 41.468735\n",
            "Epoch Step: 800 Loss: 41.570290\n",
            "Epoch Step: 900 Loss: 43.644947\n",
            "Validation perplexity: 41.828172\n",
            "Epoch 5\n",
            "Epoch Step: 0 Loss: 41.376736\n",
            "Epoch Step: 100 Loss: 34.881168\n",
            "Epoch Step: 200 Loss: 35.912373\n",
            "Epoch Step: 300 Loss: 40.781708\n",
            "Epoch Step: 400 Loss: 41.478142\n",
            "Epoch Step: 500 Loss: 41.836948\n",
            "Epoch Step: 600 Loss: 40.950256\n",
            "Epoch Step: 700 Loss: 43.173962\n",
            "Epoch Step: 800 Loss: 42.643570\n",
            "Epoch Step: 900 Loss: 43.426571\n",
            "Validation perplexity: 41.491055\n",
            "Epoch 6\n",
            "Epoch Step: 0 Loss: 41.446823\n",
            "Epoch Step: 100 Loss: 38.681736\n",
            "Epoch Step: 200 Loss: 38.783344\n",
            "Epoch Step: 300 Loss: 40.970200\n",
            "Epoch Step: 400 Loss: 40.956978\n",
            "Epoch Step: 500 Loss: 39.564480\n",
            "Epoch Step: 600 Loss: 39.327114\n",
            "Epoch Step: 700 Loss: 36.822746\n",
            "Epoch Step: 800 Loss: 41.617313\n",
            "Epoch Step: 900 Loss: 39.247189\n",
            "Validation perplexity: 41.263355\n",
            "Epoch 7\n",
            "Epoch Step: 0 Loss: 38.285294\n",
            "Epoch Step: 100 Loss: 40.575657\n",
            "Epoch Step: 200 Loss: 36.130234\n",
            "Epoch Step: 300 Loss: 37.322334\n",
            "Epoch Step: 400 Loss: 39.465946\n",
            "Epoch Step: 500 Loss: 38.689579\n",
            "Epoch Step: 600 Loss: 36.963974\n",
            "Epoch Step: 700 Loss: 40.584621\n",
            "Epoch Step: 800 Loss: 40.347126\n",
            "Epoch Step: 900 Loss: 39.084145\n",
            "Validation perplexity: 41.715447\n",
            "Epoch 8\n",
            "Epoch Step: 0 Loss: 38.116619\n",
            "Epoch Step: 100 Loss: 35.850800\n",
            "Epoch Step: 200 Loss: 35.979538\n",
            "Epoch Step: 300 Loss: 37.215710\n",
            "Epoch Step: 400 Loss: 38.698395\n",
            "Epoch Step: 500 Loss: 37.513531\n",
            "Epoch Step: 600 Loss: 34.908463\n",
            "Epoch Step: 700 Loss: 41.662796\n",
            "Epoch Step: 800 Loss: 40.048737\n",
            "Epoch Step: 900 Loss: 35.743286\n",
            "Validation perplexity: 42.074699\n",
            "Epoch 9\n",
            "Epoch Step: 0 Loss: 36.388039\n",
            "Epoch Step: 100 Loss: 38.227161\n",
            "Epoch Step: 200 Loss: 38.637260\n",
            "Epoch Step: 300 Loss: 35.608337\n",
            "Epoch Step: 400 Loss: 37.362740\n",
            "Epoch Step: 500 Loss: 37.726452\n",
            "Epoch Step: 600 Loss: 35.777584\n",
            "Epoch Step: 700 Loss: 36.902222\n",
            "Epoch Step: 800 Loss: 36.531052\n",
            "Epoch Step: 900 Loss: 40.996681\n",
            "Validation perplexity: 42.494818\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-B83Wyihlor"
      },
      "source": [
        "def greedy_decode(model, src_ids, src_lengths, max_len):\n",
        "  \"\"\"Greedily decode a sentence for EncoderDecoder. Make sure to chop off the \n",
        "     EOS token!\"\"\"\n",
        "\n",
        "  with torch.no_grad():\n",
        "    _, encoder_finals = model.encode(src_ids, src_lengths)\n",
        "    prev_y = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n",
        "\n",
        "  output = []\n",
        "  hidden = None\n",
        "  \n",
        "  # --------- Your code here --------- #\n",
        "\n",
        "  for i in range(max_len):\n",
        "      with torch.no_grad():\n",
        "          hidden, out = model.decode(encoder_finals, prev_y, hidden)\n",
        "          output.append(torch.argmax(model.generator(out)))\n",
        "          prev_y[0][0] = output[-1]\n",
        "\n",
        "          if output[-1] == EOS_INDEX:\n",
        "              output = output[:-1]\n",
        "              break\n",
        "\n",
        "  # --------- Your code ends --------- #\n",
        "\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7IYcUjgdjeO"
      },
      "source": [
        "example_set = MTDataset(val_src_sentences_list, src_vocab_set,\n",
        "                        val_trg_sentences_list, trg_vocab_set)\n",
        "example_data_loader = data.DataLoader(example_set, batch_size=1, num_workers=1,\n",
        "                                      shuffle=False)\n",
        "\n",
        "\n",
        "print(\"EncoderDecoder Results:\")\n",
        "lab_utils.print_examples(pure_seq2seq, src_vocab_set, trg_vocab_set,\n",
        "                         example_data_loader, greedy_decode, n=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXaBISR_riud"
      },
      "source": [
        "# ATTENTION\n",
        "Source code: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyn4R5Kprm6y"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjP2Juxv3b-W"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXKnfQTY3hf6"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpsdvHq338cO"
      },
      "source": [
        "## Training Attention Encoder-Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOV5f-3w3jXt"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JghSgf9j37Qo"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYfypZmi4Fr6"
      },
      "source": [
        "## Plotting results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XndPKWys4DS9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25nN0eZe4H98"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN4GUusO4Hka"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5xawQF84QjM"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpb1c9oP4RFR"
      },
      "source": [
        "## Training and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_qAn5VV4T0x"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc_TcYRc4Xo4"
      },
      "source": [
        "## Visualizing Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6fv6Bsq4W9u"
      },
      "source": [
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJWM6Ycd4bx-"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
        "\n",
        "evaluateAndShowAttention(\"elle est trop petit .\")\n",
        "\n",
        "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
        "\n",
        "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}