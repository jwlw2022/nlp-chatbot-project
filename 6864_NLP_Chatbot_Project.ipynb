{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "6864_NLP_Chatbot_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwlw2022/nlp-chatbot-project/blob/main/6864_NLP_Chatbot_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKujomfNfXLN"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prUa1Udrb3Vi",
        "outputId": "73edf5bd-07c6-4daf-95b4-f018fcc8f561"
      },
      "source": [
        "# %%bash\n",
        "# Logistics #2: install the transformers package, create a folder, download the dataset and a patch\n",
        "!pip install pytorch-pretrained-bert\n",
        "!pip -q install transformers\n",
        "!pip -q install datasets\n",
        "!pip -q install tqdm\n",
        "!pip -q install sentencepiece "
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 13.4MB/s \n",
            "\u001b[?25hCollecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/cc/46d1a45fb0f7fb751216b1e00cfc4437e7e882a249212403eab176fdb22f/boto3-1.17.73.tar.gz (98kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 9.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Collecting botocore<1.21.0,>=1.20.73\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/0b/3af8d5c26c2e2c305a08db0ab564081c0dc3b85b41d0b795055200d4215e/botocore-1.20.73-py2.py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 22.0MB/s \n",
            "\u001b[?25hCollecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 10.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.73->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.73->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "Building wheels for collected packages: boto3\n",
            "  Building wheel for boto3 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for boto3: filename=boto3-1.17.73-py2.py3-none-any.whl size=128919 sha256=58192f80d7fb46c18997e9620c1dd05c3868feead8a531cc69167005e55a2433\n",
            "  Stored in directory: /root/.cache/pip/wheels/99/7a/6f/1ebd1a2b65e9a831cbb889b3cb7836cf926dacfb20eb01bfc5\n",
            "Successfully built boto3\n",
            "\u001b[31mERROR: botocore 1.20.73 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.17.73 botocore-1.20.73 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.4.2\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 14.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 67.3MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 49.2MB/s \n",
            "\u001b[K     |████████████████████████████████| 225kB 14.6MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 31.7MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 27.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 14.6MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXbQMJ1ZfdOh"
      },
      "source": [
        "# Pretrained tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXVSBuG_eWBW"
      },
      "source": [
        "# import transformers\n",
        "\n",
        "# Use a pretrained tokenizer with CLASS.from_pretrained() function\n",
        "# tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-cased')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BAhfXsifgTN"
      },
      "source": [
        "# Download PersonaChat dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxHQxGgYZWM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5251683-a40e-4dea-f9bb-d07a537ff036"
      },
      "source": [
        "import json\n",
        "from pytorch_pretrained_bert import cached_path\n",
        "\n",
        "url = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
        "\n",
        "# Download and load JSON dataset\n",
        "personachat_file = cached_path(url)\n",
        "with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    dataset = json.loads(f.read())"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 209850483/209850483 [00:07<00:00, 26882520.93B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEfiMNY-0oat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "997f0e50-1ad5-47ff-8efb-51ba2fa95a2d"
      },
      "source": [
        "for i in range(5):\n",
        "    print('Dialogue ', i)\n",
        "    # print('Persona: ')\n",
        "    # for persona in dataset['train'][i]['personality']:\n",
        "    #     print(persona)\n",
        "    print('Utterances: ')\n",
        "    for dialogue in dataset['train'][i]['utterances']:\n",
        "        print(dialogue['history'][-1])"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dialogue  0\n",
            "Utterances: \n",
            "hi , how are you doing ? i'm getting ready to do some cheetah chasing to stay in shape .\n",
            "i am ! for my hobby i like to do canning or some whittling .\n",
            "that's neat . when i was in high school i placed 6th in 100m dash !\n",
            "i do not . but i do have a favorite meat since that is all i eat exclusively .\n",
            "i would have to say its prime rib . do you have any favorite foods ?\n",
            "do you have anything planned for today ? i think i am going to do some canning .\n",
            "i think i will can some jam . do you also play footfall for fun ?\n",
            "Dialogue  1\n",
            "Utterances: \n",
            "hi , how are you doing today ?\n",
            "wow , four sisters . just watching game of thrones .\n",
            "i agree . what do you do for a living ?\n",
            "interesting . i'm a website designer . pretty much spend all my time on the computer .\n",
            "that's awesome . i have always had a love for technology .\n",
            "i really enjoy free diving , how about you , have any hobbies ?\n",
            "that's nice . moms are pretty cool too .\n",
            "Dialogue  2\n",
            "Utterances: \n",
            "we all live in a yellow submarine , a yellow submarine . morning !\n",
            "lol . i am shy , anything to break the ice , and i am a beatles fan .\n",
            "really ? what shows ? i like tv , it makes me forget i do not like my family\n",
            "i did too . i do not get along with mine . they have no class .\n",
            "put the lime in the coconut as well . . .\n",
            "i prefer mojitos . watermelon or cucumber .\n",
            "Dialogue  3\n",
            "Utterances: \n",
            "hi ! i work as a gourmet cook .\n",
            "really . but , i can sing pitch perfect .\n",
            "great ! i had won an award for spelling bee .\n",
            "okay but i was published in new yorker once\n",
            "i have not . i can cook any word you want me to\n",
            "i'm asian and have no hair .\n",
            "i love carrots . i eat carrots like a horse .\n",
            "i work as a gourmet cook who also has a pitch perfect voice .\n",
            "Dialogue  4\n",
            "Utterances: \n",
            "how are you doing today\n",
            "i like to watch kids\n",
            "what do you weld ? houses ?\n",
            "what is your secret that you have\n",
            "how does that feel for you\n",
            "i bet that it does\n",
            "i watch kids for a living\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhdDvpg3-FcI",
        "outputId": "709fce1a-081e-4882-f74a-865bd7c1cf55"
      },
      "source": [
        "def tokenize(dataset):\n",
        "    train_tokens = []\n",
        "    for i in range(len(dataset['train'])):  # dialogues\n",
        "        for dialogue in dataset['train'][i]['utterances']:\n",
        "            tokens = dialogue['history'][-1].split(' ')\n",
        "            train_tokens.append(tokens)\n",
        "    \n",
        "    valid_tokens = []\n",
        "    for i in range(len(dataset['valid'])):  # dialogues\n",
        "        for dialogue in dataset['valid'][i]['utterances']:\n",
        "            tokens = dialogue['history'][-1].split(' ')\n",
        "            valid_tokens.append(tokens)\n",
        "\n",
        "    train_source = []\n",
        "    train_target = []\n",
        "    for i in range(len(train_tokens)-2):\n",
        "        copy = train_tokens[i].copy()\n",
        "        copy.extend(['|'])\n",
        "        copy.extend(train_tokens[i+1])\n",
        "        train_source.append(copy)\n",
        "        train_target.append(train_tokens[i+2])\n",
        "\n",
        "    valid_source = []\n",
        "    valid_target = []\n",
        "    for i in range(len(valid_tokens)-2):\n",
        "        copy = valid_tokens[i].copy()\n",
        "        copy.extend(['|'])\n",
        "        copy.extend(valid_tokens[i+1])\n",
        "        valid_source.append(copy)\n",
        "        valid_target.append(valid_tokens[i+2])\n",
        "\n",
        "    return train_source, train_target, valid_source, valid_target\n",
        "\n",
        "print(len(dataset['train']))\n",
        "train_source, train_target, valid_source, valid_target = tokenize(dataset)\n",
        "print(train_source[0])\n",
        "print(len(train_source))\n",
        "print(len(train_target))\n",
        "print(len(valid_source))\n",
        "print(len(valid_target))\n",
        "# print(train_target)\n",
        "# print(valid_source)\n",
        "# print(valid_target)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17878\n",
            "['hi', ',', 'how', 'are', 'you', 'doing', '?', \"i'm\", 'getting', 'ready', 'to', 'do', 'some', 'cheetah', 'chasing', 'to', 'stay', 'in', 'shape', '.', '|', 'i', 'am', '!', 'for', 'my', 'hobby', 'i', 'like', 'to', 'do', 'canning', 'or', 'some', 'whittling', '.']\n",
            "131436\n",
            "131436\n",
            "7799\n",
            "7799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7IfNMtlEYQ7",
        "outputId": "a1020ccb-3d45-4015-94f3-20d3038017a6"
      },
      "source": [
        "# a = max(list(map(len, train_source)))\n",
        "# print(a)\n",
        "\n",
        "a = max([len(x) for x in train_source])\n",
        "print(a)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "119\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkLOulsuge-2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6fgzQE6hA8p"
      },
      "source": [
        "# Seq2Seq Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvKACvyBRH2q"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBa2QRPi7AIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1783b30-afce-4a30-e8b2-598d28fce6c1"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "MODEL_FOLDER = \"/content/drive/My Drive/mit-6864/hw3\"\n",
        "!mkdir -p \"/content/drive/My Drive/mit-6864/hw3\""
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzzPi7ouR9rJ",
        "outputId": "ae3410be-696c-4703-feac-f4617494b83a"
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/mit-6864/hw3.git\n",
        "mkdir -p /content/hw3/data\n",
        "\n",
        "pip install sacrebleu"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: portalocker, sacrebleu\n",
            "Successfully installed portalocker-2.0.0 sacrebleu-1.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hw3'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suepLPcGR27_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c7bdd2a-2272-4cca-c6fa-0035e1d59474"
      },
      "source": [
        "# Download data\n",
        "DATA_DIR = \"/content/hw3/data\"\n",
        "\n",
        "# !wget -nv -O \"$DATA_DIR/train.en\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
        "# !wget -nv -O \"$DATA_DIR/train.vi\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n",
        "!wget -nv -O \"$DATA_DIR/vocab.en\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en\n",
        "# !wget -nv -O \"$DATA_DIR/vocab.vi\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.vi"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-14 20:53:02 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en [139741/139741] -> \"/content/hw3/data/vocab.en\" [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsGbOGWTs3sB",
        "outputId": "429b69ec-176c-46ad-ac0d-6afa5034d5f5"
      },
      "source": [
        "words = set()\n",
        "for sentence in train_source:\n",
        "  words.update(sentence)\n",
        "for sentence in train_target:\n",
        "  words.update(sentence)\n",
        "for sentence in valid_source:\n",
        "  words.update(sentence)\n",
        "for sentence in valid_target:\n",
        "  words.update(sentence)\n",
        "print(len(words))\n",
        "with open('/content/hw3/data/vocab.txt', 'w') as writefile:\n",
        "    writefile.write('\\n'.join(words))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFWyGs6kRxD7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "470c7f85-dc2a-42e5-a26b-7cd5872fa8df"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/hw3\")\n",
        "\n",
        "import lab_utils\n",
        "\n",
        "import torch \n",
        "# !pip install torch==1.6.0 torchvision==0.7.0\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "assert device == \"cuda\"   # use gpu whenever you can!\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "from lab_utils import read_vocab_file, read_sentence_file, filter_data, show_some_data_stats\n",
        "\n",
        "# src_vocab_set = read_vocab_file(\"vocab.vi\")\n",
        "# trg_vocab_set = read_vocab_file(\"vocab.en\")\n",
        "trg_vocab_set = list(words)\n",
        "# trg_vocab_set = ['<pad>'] + ['<unk>'] + ['<s>'] + ['</s>'] + list(words)\n",
        "src_vocab_set = trg_vocab_set\n",
        "\n",
        "train_src_sentences_list = train_source\n",
        "train_trg_sentences_list = train_target\n",
        "assert len(train_src_sentences_list) == len(train_trg_sentences_list)\n",
        "\n",
        "# test_src_sentences_list = read_sentence_file(\"tst2013.vi\")\n",
        "# test_trg_sentences_list = read_sentence_file(\"tst2013.en\")\n",
        "test_src_sentences_list = valid_source\n",
        "test_trg_sentences_list = valid_target\n",
        "assert len(test_src_sentences_list) == len(test_trg_sentences_list)\n",
        "\n",
        "# Filter out sentences over 48 words long\n",
        "MAX_SENT_LENGTH = 45\n",
        "MAX_SENT_LENGTH_PLUS_SOS_EOS = 46\n",
        "\n",
        "train_src_sentences_list, train_trg_sentences_list = filter_data(\n",
        "    train_src_sentences_list, train_trg_sentences_list, MAX_SENT_LENGTH)\n",
        "test_src_sentences_list, test_trg_sentences_list = filter_data(\n",
        "    test_src_sentences_list, test_trg_sentences_list, MAX_SENT_LENGTH)\n",
        "\n",
        "# We take 10% of training data as validation set.\n",
        "num_val = int(len(train_src_sentences_list) * 0.1)\n",
        "val_src_sentences_list = train_src_sentences_list[:num_val]\n",
        "val_trg_sentences_list = train_trg_sentences_list[:num_val]\n",
        "train_src_sentences_list = train_src_sentences_list[num_val:]\n",
        "train_trg_sentences_list = train_trg_sentences_list[num_val:]\n",
        "\n",
        "show_some_data_stats(train_src_sentences_list, val_src_sentences_list, \n",
        "                     test_src_sentences_list, train_trg_sentences_list,\n",
        "                     src_vocab_set, trg_vocab_set)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training (src, trg) sentence pairs: 118280\n",
            "Number of validation (src, trg) sentence pairs: 13142\n",
            "Number of testing (src, trg) sentence pairs: 7799\n",
            "Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 18598\n",
            "Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 18598\n",
            "Training sentence avg. length: 22 \n",
            "Training sentence length at 95-percentile: 34\n",
            "Training sentence length distribution (x-axis is length range and y-axis is count):\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Example Vietnamese input: ['that', 'is', 'great', '.', 'keep', 'up', 'with', 'news', 'plus', 'stay', 'in', 'shape', '|', 'hi', '.', 'sorry', 'in', 'advance', 'my', 'english', 'is', 'not', 'that', 'great', '.']\n",
            "Its target English output: [\"i'm\", 'okay', '.', 'i', 'just', 'took', 'my', 'dog', 'for', 'a', 'walk', '.', 'you', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDcyruI0UcXe"
      },
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "# These IDs are reserved.\n",
        "PAD_INDEX = 0\n",
        "UNK_INDEX = 1\n",
        "SOS_INDEX = 2\n",
        "EOS_INDEX = 3\n",
        "\n",
        "\n",
        "class MTDataset(data.Dataset):\n",
        "  def __init__(self, src_sentences, src_vocabs, trg_sentences, trg_vocabs,\n",
        "               sampling=1.):\n",
        "    self.src_sentences = src_sentences[:int(len(src_sentences) * sampling)]\n",
        "    self.trg_sentences = trg_sentences[:int(len(src_sentences) * sampling)]\n",
        "\n",
        "    self.max_src_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n",
        "    self.max_trg_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n",
        "\n",
        "    self.src_vocabs = src_vocabs\n",
        "    self.trg_vocabs = trg_vocabs\n",
        "\n",
        "    self.src_v2id = {v : i for i, v in enumerate(src_vocabs)}\n",
        "    self.src_id2v = {val : key for key, val in self.src_v2id.items()}\n",
        "    self.trg_v2id = {v : i for i, v in enumerate(trg_vocabs)}\n",
        "    self.trg_id2v = {val : key for key, val in self.trg_v2id.items()}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.src_sentences)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    src_sent = self.src_sentences[index]\n",
        "    src_len = len(src_sent) + 1\n",
        "    src_id = []\n",
        "    for w in src_sent:\n",
        "      if w == '|':\n",
        "        # src_id.append(EOS_INDEX)\n",
        "        # src_id.append(SOS_INDEX)\n",
        "        pass\n",
        "      else:\n",
        "        if w not in self.src_vocabs:\n",
        "          w = '<unk>'\n",
        "        src_id.append(self.src_v2id[w])\n",
        "    src_id = ([SOS_INDEX] + src_id + [EOS_INDEX] + [PAD_INDEX] *\n",
        "               (self.max_src_seq_length - src_len))\n",
        "    #src_id = (src_id + [PAD_INDEX] * (self.max_src_seq_length - src_len))\n",
        "\n",
        "    trg_sent = self.trg_sentences[index]\n",
        "    trg_len = len(trg_sent) + 2\n",
        "    trg_id = []\n",
        "    for w in trg_sent:\n",
        "      if w not in self.trg_vocabs:\n",
        "        w = '<unk>'\n",
        "      trg_id.append(self.trg_v2id[w])\n",
        "    trg_id = ([SOS_INDEX] + trg_id + [EOS_INDEX] + [PAD_INDEX] *\n",
        "               (self.max_trg_seq_length - trg_len))\n",
        "    #trg_id = (trg_id + [PAD_INDEX] * (self.max_trg_seq_length - trg_len))\n",
        "\n",
        "    return torch.tensor(src_id), src_len, torch.tensor(trg_id), trg_len"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1p99-z4UeJH"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xuHHZDDhCD9"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, dropout=0.):\n",
        "    \"\"\"\n",
        "    Inputs: \n",
        "      - `input_size`: an int representing the RNN input size.\n",
        "      - `hidden_size`: an int representing the RNN hidden size.\n",
        "      - `dropout`: a float representing the dropout rate during training. Note\n",
        "          that for 1-layer RNN this has no effect since dropout only applies to\n",
        "          outputs of intermediate layers.\n",
        "    \"\"\"\n",
        "    \n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    # --------- Your code here --------- #\n",
        "    # feel free to use a pre-implemented pytorch GRU\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    self.rnn = nn.GRU(input_size=input_size,\n",
        "                      hidden_size=hidden_size,\n",
        "                      num_layers=3,\n",
        "                      batch_first=True,\n",
        "                      dropout=dropout)\n",
        "    \n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "  def forward(self, inputs, lengths):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of source\n",
        "          sentences.\n",
        "      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n",
        "          lengths of `inputs`.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "        (batch_size, max_seq_length, hidden_size).\n",
        "      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n",
        "\n",
        "      Hint: `outputs` and `finals` are both standard GRU outputs.\n",
        "    \"\"\"\n",
        "    outputs = None\n",
        "    finals = None\n",
        "    \n",
        "    # --------- Your code here --------- #\n",
        "    # hint: you probably want to pack the inputs and outputs (see note below)\n",
        "    #       https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
        "    # hint2: given the shape of the inputs and outputs, \n",
        "    #        it might be helpful to specify batch_first=True (also in __init___)\n",
        "    # hint3: MAX_SENT_LENGTH_PLUS_SOS_EOS is a global variable that exists if \n",
        "    #        you ever need to specify a total_length for outputs\n",
        "\n",
        "    packed_sequence = pack_padded_sequence(inputs,\n",
        "                                           lengths.cpu(),\n",
        "                                           batch_first=True,\n",
        "                                           enforce_sorted=False)\n",
        "    outputs, finals = self.rnn(packed_sequence)\n",
        "    outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
        "    \n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "    return outputs, finals"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOGgHO0HRSiW"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCzMw3ghhKcw"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  \"\"\"An RNN decoder without attention.\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, dropout=0.):\n",
        "    \"\"\"\n",
        "      Inputs:\n",
        "        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n",
        "    \"\"\"\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    # --------- Your code here --------- #\n",
        "    # hint: you need more layers than the encoder\n",
        "    #       again, feel free to use pytorch implemetnations\n",
        "    #       https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "    \n",
        "    # To initialize from the final encoder state.\n",
        "\n",
        "    self.rnn = nn.GRU(input_size=input_size,\n",
        "                      hidden_size=hidden_size,\n",
        "                      num_layers=3,\n",
        "                      batch_first=True,\n",
        "                      dropout=dropout)\n",
        "\n",
        "    self.bridge = nn.Linear(hidden_size, input_size, bias=True)\n",
        "\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "  def forward_step(self, prev_embed, hidden):\n",
        "    \"\"\"Helper function for forward below:\n",
        "       Perform a single decoder step (1 word).\n",
        "\n",
        "       Inputs:\n",
        "      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size)\n",
        "          representing the padded embedded word vectors at this step in training\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the current hidden state.\n",
        "\n",
        "      Returns:\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the current decoder hidden state.\n",
        "      - `pre_output`: a 3d-tensor of shape (batch_size, 1, hidden_size)\n",
        "          representing the total decoder output for one step\n",
        "    \"\"\"\n",
        "    pre_output = None\n",
        "    # --------- Your code here --------- #\n",
        "    \n",
        "    pre_output, hidden = self.rnn(prev_embed, hidden)\n",
        "\n",
        "    # --------- Your code ends --------- #\n",
        "    return hidden, pre_output\n",
        "\n",
        "  def forward(self, inputs, encoder_finals, hidden=None, max_len=None):\n",
        "    \"\"\"Unroll the decoder one step at a time.\n",
        "\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of target\n",
        "          sentences (for teacher-forcing during training).\n",
        "      - `encoder_finals`: a 3d-tensor of shape\n",
        "          (num_enc_layers, batch_size, hidden_size) representing the final\n",
        "          encoder hidden states used to initialize the initial decoder hidden\n",
        "          states.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the value to be used to initialize the initial decoder hidden states.\n",
        "          If None, then use `encoder_finals`.\n",
        "      - `max_len`: an int representing the maximum decoding length.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the raw\n",
        "          decoder outputs (before converting to a `trg_vocab_size`-dim vector).\n",
        "          We will convert it later in a `Generator` below.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the last decoder hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # The maximum number of steps to unroll the RNN.\n",
        "    if max_len is None:\n",
        "      max_len = inputs.size(1)\n",
        "\n",
        "    # Initialize decoder hidden state.\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(encoder_finals)\n",
        "\n",
        "    outputs = []\n",
        "    \n",
        "    # --------- Your code here --------- #\n",
        "\n",
        "    # Unroll the decoder RNN for `max_len` steps.\n",
        "    # hint: use the above helper function forward_step that \n",
        "    #       performs a single decoder step (1 word).\n",
        "\n",
        "    for i in range(max_len):\n",
        "        prev_embed = inputs[:, i:i+1, :] # get embeddings from inputs\n",
        "        hidden, pre_output = self.forward_step(prev_embed, hidden)\n",
        "        outputs.append(pre_output)\n",
        "    \n",
        "    # final output concatenates outputs for each word\n",
        "    outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "    return hidden, outputs\n",
        "\n",
        "  def init_hidden(self, encoder_finals):\n",
        "    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n",
        "    state.\"\"\"\n",
        "    decoder_init_hiddens = torch.tanh(self.bridge(encoder_finals))\n",
        "\n",
        "    return decoder_init_hiddens"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmzMRvO0fhbU"
      },
      "source": [
        "# class CustomAttnDecoder(nn.Module):\n",
        "#     \"\"\"An RNN decoder without attention.\"\"\"\n",
        "\n",
        "#     def __init__(self, input_size, hidden_size, dropout=0.):\n",
        "#         \"\"\"\n",
        "#           Inputs:\n",
        "#             - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n",
        "#         \"\"\"\n",
        "#         super(Decoder, self).__init__()\n",
        "\n",
        "#         # --------- Your code here --------- #\n",
        "#         # hint: you need more layers than the encoder\n",
        "#         #       again, feel free to use pytorch implemetnations\n",
        "#         #       https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "        \n",
        "#         # To initialize from the final encoder state.\n",
        "\n",
        "#         self.rnn = nn.GRU(input_size=input_size,\n",
        "#                           hidden_size=hidden_size,\n",
        "#                           num_layers=3,\n",
        "#                           batch_first=True,\n",
        "#                           dropout=dropout)\n",
        "\n",
        "#         self.bridge = nn.Linear(hidden_size, input_size, bias=True)\n",
        "#         self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "#         self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "#         self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "#         self.dropout = nn.Dropout(self.dropout_p)\n",
        "#         self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "#         self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "#     def forward(self, input, hidden, encoder_outputs):\n",
        "#         embedded = self.embedding(input).view(1, 1, -1)\n",
        "#         embedded = self.dropout(embedded)\n",
        "\n",
        "#         attn_weights = F.softmax(\n",
        "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "#                                  encoder_outputs.unsqueeze(0))\n",
        "\n",
        "#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "#         output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "#         output = F.relu(output)\n",
        "#         output, hidden = self.gru(output, hidden)\n",
        "\n",
        "#         output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "#         return output, hidden, attn_weights\n",
        "\n",
        "#     def initHidden(self):\n",
        "#         return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_g_Mh7mRULr"
      },
      "source": [
        "## EncoderDecoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtN_OJxAhLVH"
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  \"\"\"A standard Encoder-Decoder architecture without attention.\n",
        "  \"\"\"\n",
        "  def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `encoder`: an `Encoder` object.\n",
        "      - `decoder`: a `Decoder` object.\n",
        "      - `src_embed`: an nn.Embedding object representing the lookup table for\n",
        "          input (source) sentences.\n",
        "      - `trg_embed`: an nn.Embedding object representing the lookup table for\n",
        "          output (target) sentences.\n",
        "      - `generator`: a `Generator` object. Essentially a linear mapping. See\n",
        "          the next code cell.\n",
        "    \"\"\"\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.trg_embed = trg_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, src_ids, trg_ids, src_lengths):\n",
        "    \"\"\"Take in and process masked source and target sequences.\n",
        "\n",
        "    Inputs:\n",
        "      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of source sentences of word ids.\n",
        "      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of target sentences of word ids.\n",
        "      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n",
        "        sequence length of `src_ids`.\n",
        "\n",
        "    Returns the decoder outputs, see the above cell.\n",
        "    \"\"\"\n",
        "    encoder_hiddens, encoder_finals = self.encode(src_ids, src_lengths)\n",
        "    del encoder_hiddens   # unused\n",
        "    return self.decode(encoder_finals, trg_ids[:, :-1])\n",
        "\n",
        "  def encode(self, src_ids, src_lengths):\n",
        "    return self.encoder(self.src_embed(src_ids), src_lengths)\n",
        "    \n",
        "  def decode(self, encoder_finals, trg_ids, decoder_hidden=None):\n",
        "    return self.decoder(self.trg_embed(trg_ids), encoder_finals, decoder_hidden)"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSwpEe1ORV4-"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx55R2LihLcp"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
        "  def __init__(self, hidden_size, vocab_size):\n",
        "    super(Generator, self).__init__()\n",
        "    self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv_504_2RZF3"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmTIw_g8TfW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acdebc67-35eb-4a47-c281-7dd47c4c5031"
      },
      "source": [
        "from torch.utils import data\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# You can try on a smaller training set by setting a smaller `sampling`.\n",
        "train_set = MTDataset(train_src_sentences_list, src_vocab_set,\n",
        "                      train_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "# print('TRAIN')\n",
        "# for i in range(len(train_set)):\n",
        "#   if len(train_set[i][0]) != 48 or len(train_set[i][2]) != 48:\n",
        "#     print(train_set[i])\n",
        "#     break\n",
        "# print(len(train_set))\n",
        "# print(train_set[0])\n",
        "# print(len(train_set[0][0])) # source\n",
        "# print(len(train_set[0][2])) # target\n",
        "# print(train_set[126])\n",
        "# print(len(train_set[126][0])) # source\n",
        "# print(len(train_set[126][2])) # target\n",
        "train_data_loader = data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                    num_workers=8, shuffle=True)\n",
        "\n",
        "val_set = MTDataset(val_src_sentences_list, src_vocab_set,\n",
        "                    val_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "# print('VAL')\n",
        "# for i in range(len(val_set)):\n",
        "#   if len(val_set[i][0]) != 48 or len(val_set[i][2]) != 48:\n",
        "#     print(val_set[i])\n",
        "#     break\n",
        "val_data_loader = data.DataLoader(val_set, batch_size=batch_size, num_workers=8,\n",
        "                                  shuffle=False)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9-IjIoghKan"
      },
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class SimpleLossCompute:\n",
        "  \"\"\"A simple loss compute and train function.\"\"\"\n",
        "\n",
        "  def __init__(self, generator, criterion, opt=None):\n",
        "    self.generator = generator\n",
        "    self.criterion = criterion\n",
        "    self.opt = opt\n",
        "\n",
        "  def __call__(self, x, y, norm):\n",
        "    x = self.generator(x)\n",
        "    loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
        "                          y.contiguous().view(-1))\n",
        "    loss = loss / norm\n",
        "\n",
        "    if self.opt is not None:  # training mode\n",
        "      loss.backward()          \n",
        "      self.opt.step()\n",
        "      self.opt.zero_grad()\n",
        "\n",
        "    return loss.data.item() * norm\n",
        "\n",
        "\n",
        "def run_epoch(data_loader, model, loss_compute, print_every):\n",
        "  \"\"\"Standard Training and Logging Function\"\"\"\n",
        "\n",
        "  total_tokens = 0\n",
        "  total_loss = 0\n",
        "\n",
        "  for i, (src_ids_BxT, src_lengths_B, trg_ids_BxL, trg_lengths_B) in enumerate(data_loader):\n",
        "    # We define some notations here to help you understand the loaded tensor\n",
        "    # shapes:\n",
        "    #   `B`: batch size\n",
        "    #   `T`: max sequence length of source sentences\n",
        "    #   `L`: max sequence length of target sentences; due to our preprocessing\n",
        "    #        in the beginning, `L` == `T` == 50\n",
        "    # An example of `src_ids_BxT` (when B = 2):\n",
        "    #   [[2, 4, 6, 7, ..., 4, 3, 0, 0, 0],\n",
        "    #    [2, 8, 6, 5, ..., 9, 5, 4, 3, 0]]\n",
        "    # The corresponding `src_lengths_B` would be [47, 49].\n",
        "    # Note that SOS_INDEX == 2, EOS_INDEX == 3, and PAD_INDEX = 0.\n",
        "\n",
        "    src_ids_BxT = src_ids_BxT.to(device)\n",
        "    src_lengths_B = src_lengths_B.to(device)\n",
        "    trg_ids_BxL = trg_ids_BxL.to(device)\n",
        "\n",
        "    del trg_lengths_B   # unused\n",
        "\n",
        "    _, output = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n",
        "\n",
        "    loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:],\n",
        "                        norm=src_ids_BxT.size(0))\n",
        "    total_loss += loss\n",
        "    total_tokens += (trg_ids_BxL[:, 1:] != PAD_INDEX).data.sum().item()\n",
        "\n",
        "    if model.training and i % print_every == 0:\n",
        "      print(\"Epoch Step: %d Loss: %f\" % (i, loss / src_ids_BxT.size(0)))\n",
        "\n",
        "  return math.exp(total_loss / float(total_tokens))\n",
        "\n",
        "\n",
        "def train(model, num_epochs, learning_rate, print_every):\n",
        "  # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when\n",
        "  # computing the loss.\n",
        "  criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
        "  optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Keep track of dev ppl for each epoch.\n",
        "  dev_ppls = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print(\"Epoch\", epoch)\n",
        "\n",
        "    model.train()\n",
        "    train_ppl = run_epoch(data_loader=train_data_loader, model=model,\n",
        "                          loss_compute=SimpleLossCompute(model.generator,\n",
        "                                                         criterion, optim),\n",
        "                          print_every=print_every)\n",
        "        \n",
        "    model.eval()\n",
        "    with torch.no_grad():      \n",
        "      dev_ppl = run_epoch(data_loader=val_data_loader, model=model,\n",
        "                          loss_compute=SimpleLossCompute(model.generator,\n",
        "                                                         criterion, None),\n",
        "                          print_every=print_every)\n",
        "      print(\"Validation perplexity: %f\" % dev_ppl)\n",
        "      dev_ppls.append(dev_ppl)\n",
        "        \n",
        "  return dev_ppls"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg0HolDAhKYJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a538ab3e-9fce-4e52-a885-5e483dd63946"
      },
      "source": [
        "device = 'cuda'\n",
        "\n",
        "# Hyperparameters for contructing the encoder-decoder model.\n",
        "embed_size = 256   # Each word will be represented as a `embed_size`-dim vector.\n",
        "hidden_size = 256  # RNN hidden size.\n",
        "dropout = 0.2\n",
        "\n",
        "pure_seq2seq = EncoderDecoder(\n",
        "    encoder=Encoder(embed_size, hidden_size, dropout=dropout),\n",
        "    decoder=Decoder(embed_size, hidden_size, dropout=dropout),\n",
        "    src_embed=nn.Embedding(len(src_vocab_set), embed_size),\n",
        "    trg_embed=nn.Embedding(len(trg_vocab_set), embed_size),\n",
        "    generator=Generator(hidden_size, len(trg_vocab_set))).to(device)\n",
        "\n",
        "train_model = True\n",
        "if train_model:\n",
        "  # Start training. The returned `dev_ppls` is a list of dev perplexity for each\n",
        "  # epoch.\n",
        "  pure_dev_ppls = train(pure_seq2seq, num_epochs=10, learning_rate=1e-3,\n",
        "                        print_every=100)\n",
        "  \n",
        "  torch.save(pure_seq2seq.state_dict(), MODEL_FOLDER+\"/\" + \"pure_seq2seq.pt\")\n",
        "\n",
        "  # Plot perplexity\n",
        "  lab_utils.plot_perplexity(pure_dev_ppls)\n",
        "else:\n",
        "  pure_seq2seq.load_state_dict(torch.load(MODEL_FOLDER+\"/\" + \"pure_seq2seq.pt\"))"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 0 Loss: 115.382004\n",
            "Epoch Step: 100 Loss: 70.380028\n",
            "Epoch Step: 200 Loss: 68.666046\n",
            "Epoch Step: 300 Loss: 60.782181\n",
            "Epoch Step: 400 Loss: 59.025742\n",
            "Epoch Step: 500 Loss: 50.602470\n",
            "Epoch Step: 600 Loss: 50.945358\n",
            "Epoch Step: 700 Loss: 51.625740\n",
            "Epoch Step: 800 Loss: 51.908401\n",
            "Epoch Step: 900 Loss: 54.276394\n",
            "Validation perplexity: 68.532183\n",
            "Epoch 1\n",
            "Epoch Step: 0 Loss: 48.273937\n",
            "Epoch Step: 100 Loss: 47.770195\n",
            "Epoch Step: 200 Loss: 47.969116\n",
            "Epoch Step: 300 Loss: 45.230770\n",
            "Epoch Step: 400 Loss: 48.096794\n",
            "Epoch Step: 500 Loss: 44.795879\n",
            "Epoch Step: 600 Loss: 47.298199\n",
            "Epoch Step: 700 Loss: 48.091175\n",
            "Epoch Step: 800 Loss: 45.679020\n",
            "Epoch Step: 900 Loss: 45.994843\n",
            "Validation perplexity: 50.019517\n",
            "Epoch 2\n",
            "Epoch Step: 0 Loss: 45.982803\n",
            "Epoch Step: 100 Loss: 45.328621\n",
            "Epoch Step: 200 Loss: 43.204750\n",
            "Epoch Step: 300 Loss: 42.794636\n",
            "Epoch Step: 400 Loss: 36.729496\n",
            "Epoch Step: 500 Loss: 44.322735\n",
            "Epoch Step: 600 Loss: 44.323795\n",
            "Epoch Step: 700 Loss: 45.487827\n",
            "Epoch Step: 800 Loss: 39.345356\n",
            "Epoch Step: 900 Loss: 39.677017\n",
            "Validation perplexity: 45.310869\n",
            "Epoch 3\n",
            "Epoch Step: 0 Loss: 42.577507\n",
            "Epoch Step: 100 Loss: 42.406879\n",
            "Epoch Step: 200 Loss: 47.566772\n",
            "Epoch Step: 300 Loss: 40.256737\n",
            "Epoch Step: 400 Loss: 44.647114\n",
            "Epoch Step: 500 Loss: 41.218815\n",
            "Epoch Step: 600 Loss: 42.111229\n",
            "Epoch Step: 700 Loss: 40.825569\n",
            "Epoch Step: 800 Loss: 44.348804\n",
            "Epoch Step: 900 Loss: 43.252125\n",
            "Validation perplexity: 43.014802\n",
            "Epoch 4\n",
            "Epoch Step: 0 Loss: 39.493027\n",
            "Epoch Step: 100 Loss: 41.532265\n",
            "Epoch Step: 200 Loss: 39.624241\n",
            "Epoch Step: 300 Loss: 43.099331\n",
            "Epoch Step: 400 Loss: 42.115425\n",
            "Epoch Step: 500 Loss: 37.751225\n",
            "Epoch Step: 600 Loss: 41.352283\n",
            "Epoch Step: 700 Loss: 45.402523\n",
            "Epoch Step: 800 Loss: 44.265053\n",
            "Epoch Step: 900 Loss: 39.777992\n",
            "Validation perplexity: 42.205467\n",
            "Epoch 5\n",
            "Epoch Step: 0 Loss: 39.294743\n",
            "Epoch Step: 100 Loss: 40.298042\n",
            "Epoch Step: 200 Loss: 40.904854\n",
            "Epoch Step: 300 Loss: 40.370533\n",
            "Epoch Step: 400 Loss: 41.235065\n",
            "Epoch Step: 500 Loss: 40.273918\n",
            "Epoch Step: 600 Loss: 39.969032\n",
            "Epoch Step: 700 Loss: 37.919334\n",
            "Epoch Step: 800 Loss: 46.766685\n",
            "Epoch Step: 900 Loss: 41.293293\n",
            "Validation perplexity: 41.952421\n",
            "Epoch 6\n",
            "Epoch Step: 0 Loss: 39.069805\n",
            "Epoch Step: 100 Loss: 38.682312\n",
            "Epoch Step: 200 Loss: 37.811615\n",
            "Epoch Step: 300 Loss: 40.859623\n",
            "Epoch Step: 400 Loss: 37.799633\n",
            "Epoch Step: 500 Loss: 40.407501\n",
            "Epoch Step: 600 Loss: 43.097191\n",
            "Epoch Step: 700 Loss: 40.243706\n",
            "Epoch Step: 800 Loss: 41.125816\n",
            "Epoch Step: 900 Loss: 42.271908\n",
            "Validation perplexity: 41.906130\n",
            "Epoch 7\n",
            "Epoch Step: 0 Loss: 39.749695\n",
            "Epoch Step: 100 Loss: 39.326954\n",
            "Epoch Step: 200 Loss: 41.928940\n",
            "Epoch Step: 300 Loss: 41.028900\n",
            "Epoch Step: 400 Loss: 41.027054\n",
            "Epoch Step: 500 Loss: 42.738560\n",
            "Epoch Step: 600 Loss: 42.726974\n",
            "Epoch Step: 700 Loss: 38.593197\n",
            "Epoch Step: 800 Loss: 40.980225\n",
            "Epoch Step: 900 Loss: 42.065948\n",
            "Validation perplexity: 41.891272\n",
            "Epoch 8\n",
            "Epoch Step: 0 Loss: 38.367817\n",
            "Epoch Step: 100 Loss: 37.334160\n",
            "Epoch Step: 200 Loss: 39.001568\n",
            "Epoch Step: 300 Loss: 38.658764\n",
            "Epoch Step: 400 Loss: 40.614872\n",
            "Epoch Step: 500 Loss: 39.638203\n",
            "Epoch Step: 600 Loss: 36.987301\n",
            "Epoch Step: 700 Loss: 39.405117\n",
            "Epoch Step: 800 Loss: 39.952118\n",
            "Epoch Step: 900 Loss: 39.555088\n",
            "Validation perplexity: 42.568327\n",
            "Epoch 9\n",
            "Epoch Step: 0 Loss: 38.904110\n",
            "Epoch Step: 100 Loss: 40.343208\n",
            "Epoch Step: 200 Loss: 37.163372\n",
            "Epoch Step: 300 Loss: 37.893867\n",
            "Epoch Step: 400 Loss: 39.172760\n",
            "Epoch Step: 500 Loss: 40.394894\n",
            "Epoch Step: 600 Loss: 35.858849\n",
            "Epoch Step: 700 Loss: 40.386234\n",
            "Epoch Step: 800 Loss: 36.470131\n",
            "Epoch Step: 900 Loss: 40.139462\n",
            "Validation perplexity: 42.900199\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-B83Wyihlor"
      },
      "source": [
        "def greedy_decode(model, src_ids, src_lengths, max_len):\n",
        "  \"\"\"Greedily decode a sentence for EncoderDecoder. Make sure to chop off the \n",
        "     EOS token!\"\"\"\n",
        "\n",
        "  with torch.no_grad():\n",
        "    _, encoder_finals = model.encode(src_ids, src_lengths)\n",
        "    prev_y = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n",
        "\n",
        "  output = []\n",
        "  hidden = None\n",
        "  \n",
        "  # --------- Your code here --------- #\n",
        "\n",
        "  for i in range(max_len):\n",
        "      with torch.no_grad():\n",
        "          hidden, out = model.decode(encoder_finals, prev_y, hidden)\n",
        "          output.append(torch.argmax(model.generator(out)))\n",
        "          prev_y[0][0] = output[-1]\n",
        "\n",
        "          if output[-1] == EOS_INDEX:\n",
        "              output = output[:-1]\n",
        "              break\n",
        "\n",
        "  # --------- Your code ends --------- #\n",
        "\n",
        "  return output"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7IYcUjgdjeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c558bde2-5fba-4ee5-bc08-1dbeca4ffd2e"
      },
      "source": [
        "example_set = MTDataset(val_src_sentences_list, src_vocab_set,\n",
        "                        val_trg_sentences_list, trg_vocab_set)\n",
        "example_data_loader = data.DataLoader(example_set, batch_size=1, num_workers=1,\n",
        "                                      shuffle=False)\n",
        "\n",
        "\n",
        "print(\"EncoderDecoder Results:\")\n",
        "lab_utils.print_examples(pure_seq2seq, src_vocab_set, trg_vocab_set,\n",
        "                         example_data_loader, greedy_decode, n=50)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EncoderDecoder Results:\n",
            "Example #1\n",
            "Src :  hi , how are you doing ? i'm getting ready to do some cheetah chasing to stay in shape . i am ! for my hobby i like to do canning or some whittling .\n",
            "Trg :  that's neat . when i was in high school i placed 6th in 100m dash !\n",
            "Pred:  i like to play guitar , but i do not have any pets .\n",
            "\n",
            "Example #2\n",
            "Src :  i am ! for my hobby i like to do canning or some whittling . that's neat . when i was in high school i placed 6th in 100m dash !\n",
            "Trg :  i do not . but i do have a favorite meat since that is all i eat exclusively .\n",
            "Pred:  i am a teacher , i am a teacher , i am a teacher .\n",
            "\n",
            "Example #3\n",
            "Src :  that's neat . when i was in high school i placed 6th in 100m dash ! i do not . but i do have a favorite meat since that is all i eat exclusively .\n",
            "Trg :  i would have to say its prime rib . do you have any favorite foods ?\n",
            "Pred:  i like to play basketball . i like to play with my dog .\n",
            "\n",
            "Example #4\n",
            "Src :  i do not . but i do have a favorite meat since that is all i eat exclusively . i would have to say its prime rib . do you have any favorite foods ?\n",
            "Trg :  do you have anything planned for today ? i think i am going to do some canning .\n",
            "Pred:  i like to play basketball . i like to play basketball .\n",
            "\n",
            "Example #5\n",
            "Src :  i would have to say its prime rib . do you have any favorite foods ? do you have anything planned for today ? i think i am going to do some canning .\n",
            "Trg :  i think i will can some jam . do you also play footfall for fun ?\n",
            "Pred:  i like to play guitar . i like to play guitar .\n",
            "\n",
            "Example #6\n",
            "Src :  do you have anything planned for today ? i think i am going to do some canning . i think i will can some jam . do you also play footfall for fun ?\n",
            "Trg :  hi , how are you doing today ?\n",
            "Pred:  i like to play basketball . i like to play basketball .\n",
            "\n",
            "Example #7\n",
            "Src :  i think i will can some jam . do you also play footfall for fun ? hi , how are you doing today ?\n",
            "Trg :  wow , four sisters . just watching game of thrones .\n",
            "Pred:  i am doing well . just got back from a run .\n",
            "\n",
            "Example #8\n",
            "Src :  hi , how are you doing today ? wow , four sisters . just watching game of thrones .\n",
            "Trg :  i agree . what do you do for a living ?\n",
            "Pred:  i like to play basketball .\n",
            "\n",
            "Example #9\n",
            "Src :  wow , four sisters . just watching game of thrones . i agree . what do you do for a living ?\n",
            "Trg :  interesting . i'm a website designer . pretty much spend all my time on the computer .\n",
            "Pred:  i like to play basketball .\n",
            "\n",
            "Example #10\n",
            "Src :  i agree . what do you do for a living ? interesting . i'm a website designer . pretty much spend all my time on the computer .\n",
            "Trg :  that's awesome . i have always had a love for technology .\n",
            "Pred:  i like to play basketball . i like to play with my friends .\n",
            "\n",
            "Example #11\n",
            "Src :  interesting . i'm a website designer . pretty much spend all my time on the computer . that's awesome . i have always had a love for technology .\n",
            "Trg :  i really enjoy free diving , how about you , have any hobbies ?\n",
            "Pred:  i like to play basketball . i like to play tennis .\n",
            "\n",
            "Example #12\n",
            "Src :  that's awesome . i have always had a love for technology . i really enjoy free diving , how about you , have any hobbies ?\n",
            "Trg :  that's nice . moms are pretty cool too .\n",
            "Pred:  i like to play basketball . i like to play basketball .\n",
            "\n",
            "Example #13\n",
            "Src :  i really enjoy free diving , how about you , have any hobbies ? that's nice . moms are pretty cool too .\n",
            "Trg :  we all live in a yellow submarine , a yellow submarine . morning !\n",
            "Pred:  i like to play guitar , but i do not like to read .\n",
            "\n",
            "Example #14\n",
            "Src :  that's nice . moms are pretty cool too . we all live in a yellow submarine , a yellow submarine . morning !\n",
            "Trg :  lol . i am shy , anything to break the ice , and i am a beatles fan .\n",
            "Pred:  __ SILENCE __\n",
            "\n",
            "Example #15\n",
            "Src :  we all live in a yellow submarine , a yellow submarine . morning ! lol . i am shy , anything to break the ice , and i am a beatles fan .\n",
            "Trg :  really ? what shows ? i like tv , it makes me forget i do not like my family\n",
            "Pred:  i like to play guitar . i like to play guitar .\n",
            "\n",
            "Example #16\n",
            "Src :  lol . i am shy , anything to break the ice , and i am a beatles fan . really ? what shows ? i like tv , it makes me forget i do not like my family\n",
            "Trg :  i did too . i do not get along with mine . they have no class .\n",
            "Pred:  i like to play with my dog , but i do not have a favorite color .\n",
            "\n",
            "Example #17\n",
            "Src :  really ? what shows ? i like tv , it makes me forget i do not like my family i did too . i do not get along with mine . they have no class .\n",
            "Trg :  put the lime in the coconut as well . . .\n",
            "Pred:  __ SILENCE __\n",
            "\n",
            "Example #18\n",
            "Src :  i did too . i do not get along with mine . they have no class . put the lime in the coconut as well . . .\n",
            "Trg :  i prefer mojitos . watermelon or cucumber .\n",
            "Pred:  __ SILENCE __\n",
            "\n",
            "Example #19\n",
            "Src :  put the lime in the coconut as well . . . i prefer mojitos . watermelon or cucumber .\n",
            "Trg :  hi ! i work as a gourmet cook .\n",
            "Pred:  __ SILENCE __\n",
            "\n",
            "Example #20\n",
            "Src :  i prefer mojitos . watermelon or cucumber . hi ! i work as a gourmet cook .\n",
            "Trg :  really . but , i can sing pitch perfect .\n",
            "Pred:  i like to play basketball . i like to play guitar .\n",
            "\n",
            "Example #21\n",
            "Src :  hi ! i work as a gourmet cook . really . but , i can sing pitch perfect .\n",
            "Trg :  great ! i had won an award for spelling bee .\n",
            "Pred:  i like to play guitar . i like to play with my dog .\n",
            "\n",
            "Example #22\n",
            "Src :  really . but , i can sing pitch perfect . great ! i had won an award for spelling bee .\n",
            "Trg :  okay but i was published in new yorker once\n",
            "Pred:  i am a teacher . i am a teacher .\n",
            "\n",
            "Example #23\n",
            "Src :  great ! i had won an award for spelling bee . okay but i was published in new yorker once\n",
            "Trg :  i have not . i can cook any word you want me to\n",
            "Pred:  i am a teacher . i am a teacher . i am a teacher .\n",
            "\n",
            "Example #24\n",
            "Src :  okay but i was published in new yorker once i have not . i can cook any word you want me to\n",
            "Trg :  i'm asian and have no hair .\n",
            "Pred:  i like to play basketball . i like to play basketball\n",
            "\n",
            "Example #25\n",
            "Src :  i have not . i can cook any word you want me to i'm asian and have no hair .\n",
            "Trg :  i love carrots . i eat carrots like a horse .\n",
            "Pred:  i like to watch tv with my friends .\n",
            "\n",
            "Example #26\n",
            "Src :  i'm asian and have no hair . i love carrots . i eat carrots like a horse .\n",
            "Trg :  i work as a gourmet cook who also has a pitch perfect voice .\n",
            "Pred:  __ SILENCE __\n",
            "\n",
            "Example #27\n",
            "Src :  i love carrots . i eat carrots like a horse . i work as a gourmet cook who also has a pitch perfect voice .\n",
            "Trg :  how are you doing today\n",
            "Pred:  i am a teacher . i am a teacher .\n",
            "\n",
            "Example #28\n",
            "Src :  i work as a gourmet cook who also has a pitch perfect voice . how are you doing today\n",
            "Trg :  i like to watch kids\n",
            "Pred:  i am doing great , just got back from a hike\n",
            "\n",
            "Example #29\n",
            "Src :  how are you doing today i like to watch kids\n",
            "Trg :  what do you weld ? houses ?\n",
            "Pred:  i like to watch tv\n",
            "\n",
            "Example #30\n",
            "Src :  i like to watch kids what do you weld ? houses ?\n",
            "Trg :  what is your secret that you have\n",
            "Pred:  i like to play piano\n",
            "\n",
            "Example #31\n",
            "Src :  what do you weld ? houses ? what is your secret that you have\n",
            "Trg :  how does that feel for you\n",
            "Pred:  i like to play guitar . i like to play games\n",
            "\n",
            "Example #32\n",
            "Src :  what is your secret that you have how does that feel for you\n",
            "Trg :  i bet that it does\n",
            "Pred:  i like to play piano\n",
            "\n",
            "Example #33\n",
            "Src :  how does that feel for you i bet that it does\n",
            "Trg :  i watch kids for a living\n",
            "Pred:  i like to watch tv\n",
            "\n",
            "Example #34\n",
            "Src :  i bet that it does i watch kids for a living\n",
            "Trg :  hello friend , how is it going\n",
            "Pred:  i like to watch tv\n",
            "\n",
            "Example #35\n",
            "Src :  i watch kids for a living hello friend , how is it going\n",
            "Trg :  i'm great enjoying the football season\n",
            "Pred:  i am doing great , just got back from a run .\n",
            "\n",
            "Example #36\n",
            "Src :  hello friend , how is it going i'm great enjoying the football season\n",
            "Trg :  you work for a funeral home ?\n",
            "Pred:  i am a teacher , i am a teacher\n",
            "\n",
            "Example #37\n",
            "Src :  i'm great enjoying the football season you work for a funeral home ?\n",
            "Trg :  lol , i can imagine . i'll be reading a lot when football is over\n",
            "Pred:  i am a teacher , i am a teacher\n",
            "\n",
            "Example #38\n",
            "Src :  you work for a funeral home ? lol , i can imagine . i'll be reading a lot when football is over\n",
            "Trg :  ok i see , that's your halloween costume\n",
            "Pred:  __ SILENCE __\n",
            "\n",
            "Example #39\n",
            "Src :  lol , i can imagine . i'll be reading a lot when football is over ok i see , that's your halloween costume\n",
            "Trg :  i like anything to do with mystery\n",
            "Pred:  i am a teacher , i am a teacher , i am a teacher\n",
            "\n",
            "Example #40\n",
            "Src :  ok i see , that's your halloween costume i like anything to do with mystery\n",
            "Trg :  lol , oh i see , taught you own it\n",
            "Pred:  __ SILENCE __\n",
            "\n",
            "Example #41\n",
            "Src :  i like anything to do with mystery lol , oh i see , taught you own it\n",
            "Trg :  well i like sherlock holmes and others\n",
            "Pred:  __ SILENCE __\n",
            "\n",
            "Example #42\n",
            "Src :  lol , oh i see , taught you own it well i like sherlock holmes and others\n",
            "Trg :  rock on , i'm listening to my favorite band guns and roses .\n",
            "Pred:  i am a teacher , i am a teacher , i am a teacher\n",
            "\n",
            "Example #43\n",
            "Src :  well i like sherlock holmes and others rock on , i'm listening to my favorite band guns and roses .\n",
            "Trg :  of course . i love to listen to rock .\n",
            "Pred:  i like to play guitar , but i do not like to watch tv .\n",
            "\n",
            "Example #44\n",
            "Src :  rock on , i'm listening to my favorite band guns and roses . of course . i love to listen to rock .\n",
            "Trg :  well i'm into black everything . so at least it wouldn't show on my black carpet .\n",
            "Pred:  i like to play with my dog . i love it .\n",
            "\n",
            "Example #45\n",
            "Src :  of course . i love to listen to rock . well i'm into black everything . so at least it wouldn't show on my black carpet .\n",
            "Trg :  i've a black car , purse , wear all black .\n",
            "Pred:  i like to play basketball .\n",
            "\n",
            "Example #46\n",
            "Src :  well i'm into black everything . so at least it wouldn't show on my black carpet . i've a black car , purse , wear all black .\n",
            "Trg :  wow , does he live there or work ?\n",
            "Pred:  i like to play basketball .\n",
            "\n",
            "Example #47\n",
            "Src :  i've a black car , purse , wear all black . wow , does he live there or work ?\n",
            "Trg :  have you visited him there before ?\n",
            "Pred:  i like to play basketball .\n",
            "\n",
            "Example #48\n",
            "Src :  wow , does he live there or work ? have you visited him there before ?\n",
            "Trg :  sounds a bit scary . i ve never been .\n",
            "Pred:  i like to play basketball , but i do not like to eat healthy\n",
            "\n",
            "Example #49\n",
            "Src :  have you visited him there before ? sounds a bit scary . i ve never been .\n",
            "Trg :  hi . how are you doing today ?\n",
            "Pred:  i like to play basketball . i like to play with my dog .\n",
            "\n",
            "Example #50\n",
            "Src :  sounds a bit scary . i ve never been . hi . how are you doing today ?\n",
            "Trg :  i'm alright . i just got done writing .\n",
            "Pred:  i am doing well . just got back from a run .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXaBISR_riud"
      },
      "source": [
        "# ATTENTION\n",
        "Source code: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyn4R5Kprm6y"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjP2Juxv3b-W"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXKnfQTY3hf6"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpsdvHq338cO"
      },
      "source": [
        "## Training Attention Encoder-Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOV5f-3w3jXt"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JghSgf9j37Qo"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYfypZmi4Fr6"
      },
      "source": [
        "## Plotting results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XndPKWys4DS9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25nN0eZe4H98"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN4GUusO4Hka"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5xawQF84QjM"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpb1c9oP4RFR"
      },
      "source": [
        "## Training and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_qAn5VV4T0x"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc_TcYRc4Xo4"
      },
      "source": [
        "## Visualizing Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6fv6Bsq4W9u"
      },
      "source": [
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJWM6Ycd4bx-"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
        "\n",
        "evaluateAndShowAttention(\"elle est trop petit .\")\n",
        "\n",
        "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
        "\n",
        "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}