{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "6864_NLP_Chatbot_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwlw2022/nlp-chatbot-project/blob/main/6864_NLP_Chatbot_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKujomfNfXLN"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prUa1Udrb3Vi",
        "outputId": "11bd7a16-91d6-4009-c309-dc18d8fc01e0"
      },
      "source": [
        "# %%bash\n",
        "# Logistics #2: install the transformers package, create a folder, download the dataset and a patch\n",
        "!pip install pytorch-pretrained-bert\n",
        "!pip -q install transformers\n",
        "!pip -q install datasets\n",
        "!pip -q install tqdm\n",
        "!pip -q install sentencepiece "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pytorch-pretrained-bert in /usr/local/lib/python3.7/dist-packages (0.6.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.17.69)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.1+cu101)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: botocore<1.21.0,>=1.20.69 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (1.20.69)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.10.0)\n",
            "Requirement already satisfied: s3transfer<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert) (0.4.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.69->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.69->boto3->pytorch-pretrained-bert) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXbQMJ1ZfdOh"
      },
      "source": [
        "# Pretrained tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXVSBuG_eWBW"
      },
      "source": [
        "# import transformers\n",
        "\n",
        "# Use a pretrained tokenizer with CLASS.from_pretrained() function\n",
        "# tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BAhfXsifgTN"
      },
      "source": [
        "# Download PersonaChat dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxHQxGgYZWM8"
      },
      "source": [
        "import json\n",
        "from pytorch_pretrained_bert import cached_path\n",
        "\n",
        "url = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
        "\n",
        "# Download and load JSON dataset\n",
        "personachat_file = cached_path(url)\n",
        "with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    dataset = json.loads(f.read())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEfiMNY-0oat"
      },
      "source": [
        "for i in range(5):\n",
        "    print('Dialogue ', i)\n",
        "    # print('Persona: ')\n",
        "    # for persona in dataset['train'][i]['personality']:\n",
        "    #     print(persona)\n",
        "    print('Utterances: ')\n",
        "    for dialogue in dataset['train'][i]['utterances']:\n",
        "        print(dialogue['history'][-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhdDvpg3-FcI",
        "outputId": "722737b3-1b75-45f7-f611-79cc220a812d"
      },
      "source": [
        "def tokenize(dataset):\n",
        "    train_tokens = []\n",
        "    for i in range(len(dataset['train'])):  # dialogues\n",
        "        for dialogue in dataset['train'][i]['utterances']:\n",
        "            tokens = dialogue['history'][-1].split(' ')\n",
        "            train_tokens.append(tokens)\n",
        "    \n",
        "    valid_tokens = []\n",
        "    for i in range(len(dataset['valid'])):  # dialogues\n",
        "        for dialogue in dataset['valid'][i]['utterances']:\n",
        "            tokens = dialogue['history'][-1].split(' ')\n",
        "            valid_tokens.append(tokens)\n",
        "\n",
        "    train_source = []\n",
        "    train_target = []\n",
        "    for i in range(len(train_tokens)-2):\n",
        "        copy = train_tokens[i].copy()\n",
        "        copy.extend(['|'])\n",
        "        copy.extend(train_tokens[i+1])\n",
        "        train_source.append(copy)\n",
        "        train_target.append(train_tokens[i+2])\n",
        "\n",
        "    valid_source = []\n",
        "    valid_target = []\n",
        "    for i in range(len(valid_tokens)-2):\n",
        "        copy = valid_tokens[i].copy()\n",
        "        copy.extend(['|'])\n",
        "        copy.extend(valid_tokens[i+1])\n",
        "        valid_source.append(copy)\n",
        "        valid_target.append(valid_tokens[i+2])\n",
        "\n",
        "    return train_source, train_target, valid_source, valid_target\n",
        "\n",
        "print(len(dataset['train']))\n",
        "train_source, train_target, valid_source, valid_target = tokenize(dataset)\n",
        "print(train_source[0])\n",
        "print(len(train_source))\n",
        "print(len(train_target))\n",
        "print(len(valid_source))\n",
        "print(len(valid_target))\n",
        "# print(train_target)\n",
        "# print(valid_source)\n",
        "# print(valid_target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17878\n",
            "['hi', ',', 'how', 'are', 'you', 'doing', '?', \"i'm\", 'getting', 'ready', 'to', 'do', 'some', 'cheetah', 'chasing', 'to', 'stay', 'in', 'shape', '.', '|', 'i', 'am', '!', 'for', 'my', 'hobby', 'i', 'like', 'to', 'do', 'canning', 'or', 'some', 'whittling', '.']\n",
            "131436\n",
            "131436\n",
            "7799\n",
            "7799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7IfNMtlEYQ7",
        "outputId": "89d2d99c-30b0-4c80-a55d-b9e0dff94e16"
      },
      "source": [
        "# a = max(list(map(len, train_source)))\n",
        "# print(a)\n",
        "\n",
        "a = max([len(x) for x in train_source])\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "118\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkLOulsuge-2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6fgzQE6hA8p"
      },
      "source": [
        "# Seq2Seq Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvKACvyBRH2q"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBa2QRPi7AIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4561ebfc-8ab9-4b9a-c7cf-ee76ff2b2196"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "MODEL_FOLDER = \"/content/drive/My Drive/mit-6864/hw3\"\n",
        "!mkdir -p \"/content/drive/My Drive/mit-6864/hw3\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzzPi7ouR9rJ",
        "outputId": "e71d8180-19cb-4936-9f2e-4a64c30b0052"
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/mit-6864/hw3.git\n",
        "mkdir -p /content/hw3/data\n",
        "\n",
        "pip install sacrebleu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "  Using cached https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl\n",
            "Collecting portalocker==2.0.0\n",
            "  Using cached https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: portalocker, sacrebleu\n",
            "Successfully installed portalocker-2.0.0 sacrebleu-1.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'hw3' already exists and is not an empty directory.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suepLPcGR27_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "029329f3-ab9b-4d2e-b6d4-9b9ab2b00cf2"
      },
      "source": [
        "# Download data\n",
        "DATA_DIR = \"/content/hw3/data\"\n",
        "\n",
        "# !wget -nv -O \"$DATA_DIR/train.en\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
        "# !wget -nv -O \"$DATA_DIR/train.vi\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n",
        "!wget -nv -O \"$DATA_DIR/vocab.en\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en\n",
        "# !wget -nv -O \"$DATA_DIR/vocab.vi\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.vi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-08 00:56:31 URL:https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en [139741/139741] -> \"/content/hw3/data/vocab.en\" [1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsGbOGWTs3sB",
        "outputId": "daa12a5d-9b82-436a-9c53-7f1bc9416a8f"
      },
      "source": [
        "words = set()\n",
        "for sentence in train_source:\n",
        "  words.update(sentence)\n",
        "for sentence in train_target:\n",
        "  words.update(sentence)\n",
        "for sentence in valid_source:\n",
        "  words.update(sentence)\n",
        "for sentence in valid_target:\n",
        "  words.update(sentence)\n",
        "print(len(words))\n",
        "with open('/content/hw3/data/vocab.txt', 'w') as writefile:\n",
        "    writefile.write('\\n'.join(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18597\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFWyGs6kRxD7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "db45e45d-c763-404d-b19a-078a8bc7243a"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/hw3\")\n",
        "\n",
        "import lab_utils\n",
        "\n",
        "import torch \n",
        "# !pip install torch==1.6.0 torchvision==0.7.0\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "assert device == \"cuda\"   # use gpu whenever you can!\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "from lab_utils import read_vocab_file, read_sentence_file, filter_data, show_some_data_stats\n",
        "\n",
        "# src_vocab_set = read_vocab_file(\"vocab.vi\")\n",
        "trg_vocab_set = read_vocab_file(\"vocab.en\")\n",
        "trg_vocab_set = list(words)\n",
        "src_vocab_set = trg_vocab_set\n",
        "\n",
        "train_src_sentences_list = train_source\n",
        "train_trg_sentences_list = train_target\n",
        "assert len(train_src_sentences_list) == len(train_trg_sentences_list)\n",
        "\n",
        "# test_src_sentences_list = read_sentence_file(\"tst2013.vi\")\n",
        "# test_trg_sentences_list = read_sentence_file(\"tst2013.en\")\n",
        "test_src_sentences_list = valid_source\n",
        "test_trg_sentences_list = valid_target\n",
        "assert len(test_src_sentences_list) == len(test_trg_sentences_list)\n",
        "\n",
        "# Filter out sentences over 48 words long\n",
        "MAX_SENT_LENGTH = 45\n",
        "MAX_SENT_LENGTH_PLUS_SOS_EOS = 46\n",
        "\n",
        "train_src_sentences_list, train_trg_sentences_list = filter_data(\n",
        "    train_src_sentences_list, train_trg_sentences_list, MAX_SENT_LENGTH)\n",
        "test_src_sentences_list, test_trg_sentences_list = filter_data(\n",
        "    test_src_sentences_list, test_trg_sentences_list, MAX_SENT_LENGTH)\n",
        "\n",
        "# We take 10% of training data as validation set.\n",
        "num_val = int(len(train_src_sentences_list) * 0.1)\n",
        "val_src_sentences_list = train_src_sentences_list[:num_val]\n",
        "val_trg_sentences_list = train_trg_sentences_list[:num_val]\n",
        "train_src_sentences_list = train_src_sentences_list[num_val:]\n",
        "train_trg_sentences_list = train_trg_sentences_list[num_val:]\n",
        "\n",
        "show_some_data_stats(train_src_sentences_list, val_src_sentences_list, \n",
        "                     test_src_sentences_list, train_trg_sentences_list,\n",
        "                     src_vocab_set, trg_vocab_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training (src, trg) sentence pairs: 118280\n",
            "Number of validation (src, trg) sentence pairs: 13142\n",
            "Number of testing (src, trg) sentence pairs: 7799\n",
            "Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 18598\n",
            "Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 18598\n",
            "Training sentence avg. length: 22 \n",
            "Training sentence length at 95-percentile: 34\n",
            "Training sentence length distribution (x-axis is length range and y-axis is count):\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Example Vietnamese input: ['that', 'is', 'great', '.', 'keep', 'up', 'with', 'news', 'plus', 'stay', 'in', 'shape', '|', 'hi', '.', 'sorry', 'in', 'advance', 'my', 'english', 'is', 'not', 'that', 'great', '.']\n",
            "Its target English output: [\"i'm\", 'okay', '.', 'i', 'just', 'took', 'my', 'dog', 'for', 'a', 'walk', '.', 'you', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDcyruI0UcXe"
      },
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "# These IDs are reserved.\n",
        "PAD_INDEX = 0\n",
        "UNK_INDEX = 1\n",
        "SOS_INDEX = 2\n",
        "EOS_INDEX = 3\n",
        "\n",
        "\n",
        "class MTDataset(data.Dataset):\n",
        "  def __init__(self, src_sentences, src_vocabs, trg_sentences, trg_vocabs,\n",
        "               sampling=1.):\n",
        "    self.src_sentences = src_sentences[:int(len(src_sentences) * sampling)]\n",
        "    self.trg_sentences = trg_sentences[:int(len(src_sentences) * sampling)]\n",
        "\n",
        "    self.max_src_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n",
        "    self.max_trg_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n",
        "\n",
        "    self.src_vocabs = src_vocabs\n",
        "    self.trg_vocabs = trg_vocabs\n",
        "\n",
        "    self.src_v2id = {v : i for i, v in enumerate(src_vocabs)}\n",
        "    self.src_id2v = {val : key for key, val in self.src_v2id.items()}\n",
        "    self.trg_v2id = {v : i for i, v in enumerate(trg_vocabs)}\n",
        "    self.trg_id2v = {val : key for key, val in self.trg_v2id.items()}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.src_sentences)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    src_sent = self.src_sentences[index]\n",
        "    src_len = len(src_sent) + 1\n",
        "    src_id = []\n",
        "    for w in src_sent:\n",
        "      if w == '|':\n",
        "        # src_id.append(EOS_INDEX)\n",
        "        # src_id.append(SOS_INDEX)\n",
        "        pass\n",
        "      else:\n",
        "        if w not in self.src_vocabs:\n",
        "          w = '<unk>'\n",
        "        src_id.append(self.src_v2id[w])\n",
        "    src_id = ([SOS_INDEX] + src_id + [EOS_INDEX] + [PAD_INDEX] *\n",
        "               (self.max_src_seq_length - src_len))\n",
        "    #src_id = (src_id + [PAD_INDEX] * (self.max_src_seq_length - src_len))\n",
        "\n",
        "    trg_sent = self.trg_sentences[index]\n",
        "    trg_len = len(trg_sent) + 2\n",
        "    trg_id = []\n",
        "    for w in trg_sent:\n",
        "      if w not in self.trg_vocabs:\n",
        "        w = '<unk>'\n",
        "      trg_id.append(self.trg_v2id[w])\n",
        "    trg_id = ([SOS_INDEX] + trg_id + [EOS_INDEX] + [PAD_INDEX] *\n",
        "               (self.max_trg_seq_length - trg_len))\n",
        "    #trg_id = (trg_id + [PAD_INDEX] * (self.max_trg_seq_length - trg_len))\n",
        "\n",
        "    return torch.tensor(src_id), src_len, torch.tensor(trg_id), trg_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1p99-z4UeJH"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xuHHZDDhCD9"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, dropout=0.):\n",
        "    \"\"\"\n",
        "    Inputs: \n",
        "      - `input_size`: an int representing the RNN input size.\n",
        "      - `hidden_size`: an int representing the RNN hidden size.\n",
        "      - `dropout`: a float representing the dropout rate during training. Note\n",
        "          that for 1-layer RNN this has no effect since dropout only applies to\n",
        "          outputs of intermediate layers.\n",
        "    \"\"\"\n",
        "    \n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    # --------- Your code here --------- #\n",
        "    # feel free to use a pre-implemented pytorch GRU\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    self.rnn = nn.GRU(input_size=input_size,\n",
        "                      hidden_size=hidden_size,\n",
        "                      num_layers=3,\n",
        "                      batch_first=True,\n",
        "                      dropout=dropout)\n",
        "    \n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "  def forward(self, inputs, lengths):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of source\n",
        "          sentences.\n",
        "      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n",
        "          lengths of `inputs`.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "        (batch_size, max_seq_length, hidden_size).\n",
        "      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n",
        "\n",
        "      Hint: `outputs` and `finals` are both standard GRU outputs.\n",
        "    \"\"\"\n",
        "    outputs = None\n",
        "    finals = None\n",
        "    \n",
        "    # --------- Your code here --------- #\n",
        "    # hint: you probably want to pack the inputs and outputs (see note below)\n",
        "    #       https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
        "    # hint2: given the shape of the inputs and outputs, \n",
        "    #        it might be helpful to specify batch_first=True (also in __init___)\n",
        "    # hint3: MAX_SENT_LENGTH_PLUS_SOS_EOS is a global variable that exists if \n",
        "    #        you ever need to specify a total_length for outputs\n",
        "\n",
        "    packed_sequence = pack_padded_sequence(inputs,\n",
        "                                           lengths.cpu(),\n",
        "                                           batch_first=True,\n",
        "                                           enforce_sorted=False)\n",
        "    outputs, finals = self.rnn(packed_sequence)\n",
        "    outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
        "    \n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "    return outputs, finals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOGgHO0HRSiW"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCzMw3ghhKcw"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  \"\"\"An RNN decoder without attention.\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, dropout=0.):\n",
        "    \"\"\"\n",
        "      Inputs:\n",
        "        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n",
        "    \"\"\"\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    # --------- Your code here --------- #\n",
        "    # hint: you need more layers than the encoder\n",
        "    #       again, feel free to use pytorch implemetnations\n",
        "    #       https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "    \n",
        "    # To initialize from the final encoder state.\n",
        "\n",
        "    self.rnn = nn.GRU(input_size=input_size,\n",
        "                      hidden_size=hidden_size,\n",
        "                      num_layers=3,\n",
        "                      batch_first=True,\n",
        "                      dropout=dropout)\n",
        "\n",
        "    self.bridge = nn.Linear(hidden_size, input_size, bias=True)\n",
        "\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "  def forward_step(self, prev_embed, hidden):\n",
        "    \"\"\"Helper function for forward below:\n",
        "       Perform a single decoder step (1 word).\n",
        "\n",
        "       Inputs:\n",
        "      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size)\n",
        "          representing the padded embedded word vectors at this step in training\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the current hidden state.\n",
        "\n",
        "      Returns:\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the current decoder hidden state.\n",
        "      - `pre_output`: a 3d-tensor of shape (batch_size, 1, hidden_size)\n",
        "          representing the total decoder output for one step\n",
        "    \"\"\"\n",
        "    pre_output = None\n",
        "    # --------- Your code here --------- #\n",
        "    \n",
        "    pre_output, hidden = self.rnn(prev_embed, hidden)\n",
        "\n",
        "    # --------- Your code ends --------- #\n",
        "    return hidden, pre_output\n",
        "\n",
        "  def forward(self, inputs, encoder_finals, hidden=None, max_len=None):\n",
        "    \"\"\"Unroll the decoder one step at a time.\n",
        "\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of target\n",
        "          sentences (for teacher-forcing during training).\n",
        "      - `encoder_finals`: a 3d-tensor of shape\n",
        "          (num_enc_layers, batch_size, hidden_size) representing the final\n",
        "          encoder hidden states used to initialize the initial decoder hidden\n",
        "          states.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the value to be used to initialize the initial decoder hidden states.\n",
        "          If None, then use `encoder_finals`.\n",
        "      - `max_len`: an int representing the maximum decoding length.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the raw\n",
        "          decoder outputs (before converting to a `trg_vocab_size`-dim vector).\n",
        "          We will convert it later in a `Generator` below.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the last decoder hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # The maximum number of steps to unroll the RNN.\n",
        "    if max_len is None:\n",
        "      max_len = inputs.size(1)\n",
        "\n",
        "    # Initialize decoder hidden state.\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(encoder_finals)\n",
        "\n",
        "    outputs = []\n",
        "    \n",
        "    # --------- Your code here --------- #\n",
        "\n",
        "    # Unroll the decoder RNN for `max_len` steps.\n",
        "    # hint: use the above helper function forward_step that \n",
        "    #       performs a single decoder step (1 word).\n",
        "\n",
        "    for i in range(max_len):\n",
        "        prev_embed = inputs[:, i:i+1, :] # get embeddings from inputs\n",
        "        hidden, pre_output = self.forward_step(prev_embed, hidden)\n",
        "        outputs.append(pre_output)\n",
        "    \n",
        "    # final output concatenates outputs for each word\n",
        "    outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "    return hidden, outputs\n",
        "\n",
        "  def init_hidden(self, encoder_finals):\n",
        "    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n",
        "    state.\"\"\"\n",
        "    decoder_init_hiddens = torch.tanh(self.bridge(encoder_finals))\n",
        "\n",
        "    return decoder_init_hiddens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_g_Mh7mRULr"
      },
      "source": [
        "## EncoderDecoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtN_OJxAhLVH"
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  \"\"\"A standard Encoder-Decoder architecture without attention.\n",
        "  \"\"\"\n",
        "  def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `encoder`: an `Encoder` object.\n",
        "      - `decoder`: a `Decoder` object.\n",
        "      - `src_embed`: an nn.Embedding object representing the lookup table for\n",
        "          input (source) sentences.\n",
        "      - `trg_embed`: an nn.Embedding object representing the lookup table for\n",
        "          output (target) sentences.\n",
        "      - `generator`: a `Generator` object. Essentially a linear mapping. See\n",
        "          the next code cell.\n",
        "    \"\"\"\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.trg_embed = trg_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, src_ids, trg_ids, src_lengths):\n",
        "    \"\"\"Take in and process masked source and target sequences.\n",
        "\n",
        "    Inputs:\n",
        "      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of source sentences of word ids.\n",
        "      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of target sentences of word ids.\n",
        "      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n",
        "        sequence length of `src_ids`.\n",
        "\n",
        "    Returns the decoder outputs, see the above cell.\n",
        "    \"\"\"\n",
        "    encoder_hiddens, encoder_finals = self.encode(src_ids, src_lengths)\n",
        "    del encoder_hiddens   # unused\n",
        "    return self.decode(encoder_finals, trg_ids[:, :-1])\n",
        "\n",
        "  def encode(self, src_ids, src_lengths):\n",
        "    return self.encoder(self.src_embed(src_ids), src_lengths)\n",
        "    \n",
        "  def decode(self, encoder_finals, trg_ids, decoder_hidden=None):\n",
        "    return self.decoder(self.trg_embed(trg_ids), encoder_finals, decoder_hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSwpEe1ORV4-"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx55R2LihLcp"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
        "  def __init__(self, hidden_size, vocab_size):\n",
        "    super(Generator, self).__init__()\n",
        "    self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv_504_2RZF3"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmTIw_g8TfW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "140514b7-969e-48fd-d19f-a0084056759d"
      },
      "source": [
        "from torch.utils import data\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# You can try on a smaller training set by setting a smaller `sampling`.\n",
        "train_set = MTDataset(train_src_sentences_list, src_vocab_set,\n",
        "                      train_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "# print('TRAIN')\n",
        "# for i in range(len(train_set)):\n",
        "#   if len(train_set[i][0]) != 48 or len(train_set[i][2]) != 48:\n",
        "#     print(train_set[i])\n",
        "#     break\n",
        "# print(len(train_set))\n",
        "# print(train_set[0])\n",
        "# print(len(train_set[0][0])) # source\n",
        "# print(len(train_set[0][2])) # target\n",
        "# print(train_set[126])\n",
        "# print(len(train_set[126][0])) # source\n",
        "# print(len(train_set[126][2])) # target\n",
        "train_data_loader = data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                    num_workers=8, shuffle=True)\n",
        "\n",
        "val_set = MTDataset(val_src_sentences_list, src_vocab_set,\n",
        "                    val_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "# print('VAL')\n",
        "# for i in range(len(val_set)):\n",
        "#   if len(val_set[i][0]) != 48 or len(val_set[i][2]) != 48:\n",
        "#     print(val_set[i])\n",
        "#     break\n",
        "val_data_loader = data.DataLoader(val_set, batch_size=batch_size, num_workers=8,\n",
        "                                  shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9-IjIoghKan"
      },
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class SimpleLossCompute:\n",
        "  \"\"\"A simple loss compute and train function.\"\"\"\n",
        "\n",
        "  def __init__(self, generator, criterion, opt=None):\n",
        "    self.generator = generator\n",
        "    self.criterion = criterion\n",
        "    self.opt = opt\n",
        "\n",
        "  def __call__(self, x, y, norm):\n",
        "    x = self.generator(x)\n",
        "    loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
        "                          y.contiguous().view(-1))\n",
        "    loss = loss / norm\n",
        "\n",
        "    if self.opt is not None:  # training mode\n",
        "      loss.backward()          \n",
        "      self.opt.step()\n",
        "      self.opt.zero_grad()\n",
        "\n",
        "    return loss.data.item() * norm\n",
        "\n",
        "\n",
        "def run_epoch(data_loader, model, loss_compute, print_every):\n",
        "  \"\"\"Standard Training and Logging Function\"\"\"\n",
        "\n",
        "  total_tokens = 0\n",
        "  total_loss = 0\n",
        "\n",
        "  for i, (src_ids_BxT, src_lengths_B, trg_ids_BxL, trg_lengths_B) in enumerate(data_loader):\n",
        "    # We define some notations here to help you understand the loaded tensor\n",
        "    # shapes:\n",
        "    #   `B`: batch size\n",
        "    #   `T`: max sequence length of source sentences\n",
        "    #   `L`: max sequence length of target sentences; due to our preprocessing\n",
        "    #        in the beginning, `L` == `T` == 50\n",
        "    # An example of `src_ids_BxT` (when B = 2):\n",
        "    #   [[2, 4, 6, 7, ..., 4, 3, 0, 0, 0],\n",
        "    #    [2, 8, 6, 5, ..., 9, 5, 4, 3, 0]]\n",
        "    # The corresponding `src_lengths_B` would be [47, 49].\n",
        "    # Note that SOS_INDEX == 2, EOS_INDEX == 3, and PAD_INDEX = 0.\n",
        "\n",
        "    src_ids_BxT = src_ids_BxT.to(device)\n",
        "    src_lengths_B = src_lengths_B.to(device)\n",
        "    trg_ids_BxL = trg_ids_BxL.to(device)\n",
        "\n",
        "    del trg_lengths_B   # unused\n",
        "\n",
        "    _, output = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n",
        "\n",
        "    loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:],\n",
        "                        norm=src_ids_BxT.size(0))\n",
        "    total_loss += loss\n",
        "    total_tokens += (trg_ids_BxL[:, 1:] != PAD_INDEX).data.sum().item()\n",
        "\n",
        "    if model.training and i % print_every == 0:\n",
        "      print(\"Epoch Step: %d Loss: %f\" % (i, loss / src_ids_BxT.size(0)))\n",
        "\n",
        "  return math.exp(total_loss / float(total_tokens))\n",
        "\n",
        "\n",
        "def train(model, num_epochs, learning_rate, print_every):\n",
        "  # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when\n",
        "  # computing the loss.\n",
        "  criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
        "  optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Keep track of dev ppl for each epoch.\n",
        "  dev_ppls = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print(\"Epoch\", epoch)\n",
        "\n",
        "    model.train()\n",
        "    train_ppl = run_epoch(data_loader=train_data_loader, model=model,\n",
        "                          loss_compute=SimpleLossCompute(model.generator,\n",
        "                                                         criterion, optim),\n",
        "                          print_every=print_every)\n",
        "        \n",
        "    model.eval()\n",
        "    with torch.no_grad():      \n",
        "      dev_ppl = run_epoch(data_loader=val_data_loader, model=model,\n",
        "                          loss_compute=SimpleLossCompute(model.generator,\n",
        "                                                         criterion, None),\n",
        "                          print_every=print_every)\n",
        "      print(\"Validation perplexity: %f\" % dev_ppl)\n",
        "      dev_ppls.append(dev_ppl)\n",
        "        \n",
        "  return dev_ppls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg0HolDAhKYJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8853c1fe-3d55-4ac0-9443-ef99e4e22ad6"
      },
      "source": [
        "device = 'cuda'\n",
        "\n",
        "# Hyperparameters for contructing the encoder-decoder model.\n",
        "embed_size = 256   # Each word will be represented as a `embed_size`-dim vector.\n",
        "hidden_size = 256  # RNN hidden size.\n",
        "dropout = 0.2\n",
        "\n",
        "pure_seq2seq = EncoderDecoder(\n",
        "    encoder=Encoder(embed_size, hidden_size, dropout=dropout),\n",
        "    decoder=Decoder(embed_size, hidden_size, dropout=dropout),\n",
        "    src_embed=nn.Embedding(len(src_vocab_set), embed_size),\n",
        "    trg_embed=nn.Embedding(len(trg_vocab_set), embed_size),\n",
        "    generator=Generator(hidden_size, len(trg_vocab_set))).to(device)\n",
        "\n",
        "train_model = True\n",
        "if train_model:\n",
        "  # Start training. The returned `dev_ppls` is a list of dev perplexity for each\n",
        "  # epoch.\n",
        "  pure_dev_ppls = train(pure_seq2seq, num_epochs=10, learning_rate=1e-3,\n",
        "                        print_every=100)\n",
        "  \n",
        "  torch.save(pure_seq2seq.state_dict(), MODEL_FOLDER+\"/\" + \"pure_seq2seq.pt\")\n",
        "\n",
        "  # Plot perplexity\n",
        "  lab_utils.plot_perplexity(pure_dev_ppls)\n",
        "else:\n",
        "  pure_seq2seq.load_state_dict(torch.load(MODEL_FOLDER+\"/\" + \"pure_seq2seq.pt\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 0 Loss: 117.883675\n",
            "Epoch Step: 100 Loss: 65.083038\n",
            "Epoch Step: 200 Loss: 64.198868\n",
            "Epoch Step: 300 Loss: 64.097275\n",
            "Epoch Step: 400 Loss: 56.428658\n",
            "Epoch Step: 500 Loss: 54.829350\n",
            "Epoch Step: 600 Loss: 54.169552\n",
            "Epoch Step: 700 Loss: 52.958080\n",
            "Epoch Step: 800 Loss: 50.300880\n",
            "Epoch Step: 900 Loss: 51.760853\n",
            "Validation perplexity: 67.449277\n",
            "Epoch 1\n",
            "Epoch Step: 0 Loss: 48.367493\n",
            "Epoch Step: 100 Loss: 49.633091\n",
            "Epoch Step: 200 Loss: 47.009251\n",
            "Epoch Step: 300 Loss: 42.468651\n",
            "Epoch Step: 400 Loss: 47.288044\n",
            "Epoch Step: 500 Loss: 45.578819\n",
            "Epoch Step: 600 Loss: 46.119358\n",
            "Epoch Step: 700 Loss: 46.286873\n",
            "Epoch Step: 800 Loss: 47.248459\n",
            "Epoch Step: 900 Loss: 48.795914\n",
            "Validation perplexity: 49.655947\n",
            "Epoch 2\n",
            "Epoch Step: 0 Loss: 45.269875\n",
            "Epoch Step: 100 Loss: 40.598915\n",
            "Epoch Step: 200 Loss: 40.558556\n",
            "Epoch Step: 300 Loss: 45.208336\n",
            "Epoch Step: 400 Loss: 44.029789\n",
            "Epoch Step: 500 Loss: 42.085899\n",
            "Epoch Step: 600 Loss: 43.947327\n",
            "Epoch Step: 700 Loss: 45.412441\n",
            "Epoch Step: 800 Loss: 44.456085\n",
            "Epoch Step: 900 Loss: 43.989864\n",
            "Validation perplexity: 45.200487\n",
            "Epoch 3\n",
            "Epoch Step: 0 Loss: 43.511395\n",
            "Epoch Step: 100 Loss: 40.861984\n",
            "Epoch Step: 200 Loss: 43.249241\n",
            "Epoch Step: 300 Loss: 41.316719\n",
            "Epoch Step: 400 Loss: 41.006306\n",
            "Epoch Step: 500 Loss: 43.022148\n",
            "Epoch Step: 600 Loss: 43.529873\n",
            "Epoch Step: 700 Loss: 41.109043\n",
            "Epoch Step: 800 Loss: 41.589813\n",
            "Epoch Step: 900 Loss: 44.410954\n",
            "Validation perplexity: 42.992829\n",
            "Epoch 4\n",
            "Epoch Step: 0 Loss: 38.060047\n",
            "Epoch Step: 100 Loss: 40.029613\n",
            "Epoch Step: 200 Loss: 43.028259\n",
            "Epoch Step: 300 Loss: 40.244190\n",
            "Epoch Step: 400 Loss: 38.645893\n",
            "Epoch Step: 500 Loss: 40.785000\n",
            "Epoch Step: 600 Loss: 43.680340\n",
            "Epoch Step: 700 Loss: 42.234344\n",
            "Epoch Step: 800 Loss: 40.845314\n",
            "Epoch Step: 900 Loss: 40.329250\n",
            "Validation perplexity: 41.959166\n",
            "Epoch 5\n",
            "Epoch Step: 0 Loss: 40.718529\n",
            "Epoch Step: 100 Loss: 40.095329\n",
            "Epoch Step: 200 Loss: 39.767952\n",
            "Epoch Step: 300 Loss: 37.950523\n",
            "Epoch Step: 400 Loss: 37.506279\n",
            "Epoch Step: 500 Loss: 40.374886\n",
            "Epoch Step: 600 Loss: 40.115547\n",
            "Epoch Step: 700 Loss: 44.211365\n",
            "Epoch Step: 800 Loss: 41.203590\n",
            "Epoch Step: 900 Loss: 38.056725\n",
            "Validation perplexity: 41.629558\n",
            "Epoch 6\n",
            "Epoch Step: 0 Loss: 38.505356\n",
            "Epoch Step: 100 Loss: 39.396542\n",
            "Epoch Step: 200 Loss: 41.662010\n",
            "Epoch Step: 300 Loss: 40.492104\n",
            "Epoch Step: 400 Loss: 40.014343\n",
            "Epoch Step: 500 Loss: 36.879696\n",
            "Epoch Step: 600 Loss: 41.005322\n",
            "Epoch Step: 700 Loss: 37.219582\n",
            "Epoch Step: 800 Loss: 41.644993\n",
            "Epoch Step: 900 Loss: 42.245872\n",
            "Validation perplexity: 41.737130\n",
            "Epoch 7\n",
            "Epoch Step: 0 Loss: 38.078468\n",
            "Epoch Step: 100 Loss: 39.687798\n",
            "Epoch Step: 200 Loss: 38.782043\n",
            "Epoch Step: 300 Loss: 36.913979\n",
            "Epoch Step: 400 Loss: 37.320805\n",
            "Epoch Step: 500 Loss: 42.257019\n",
            "Epoch Step: 600 Loss: 38.862431\n",
            "Epoch Step: 700 Loss: 37.172024\n",
            "Epoch Step: 800 Loss: 37.873959\n",
            "Epoch Step: 900 Loss: 37.831635\n",
            "Validation perplexity: 41.732650\n",
            "Epoch 8\n",
            "Epoch Step: 0 Loss: 36.718655\n",
            "Epoch Step: 100 Loss: 39.761055\n",
            "Epoch Step: 200 Loss: 39.396862\n",
            "Epoch Step: 300 Loss: 39.673096\n",
            "Epoch Step: 400 Loss: 40.855450\n",
            "Epoch Step: 500 Loss: 38.869495\n",
            "Epoch Step: 600 Loss: 37.961548\n",
            "Epoch Step: 700 Loss: 39.976295\n",
            "Epoch Step: 800 Loss: 39.849224\n",
            "Epoch Step: 900 Loss: 34.254387\n",
            "Validation perplexity: 42.311229\n",
            "Epoch 9\n",
            "Epoch Step: 0 Loss: 39.173183\n",
            "Epoch Step: 100 Loss: 38.279366\n",
            "Epoch Step: 200 Loss: 37.093765\n",
            "Epoch Step: 300 Loss: 36.696995\n",
            "Epoch Step: 400 Loss: 35.769764\n",
            "Epoch Step: 500 Loss: 37.855431\n",
            "Epoch Step: 600 Loss: 37.078270\n",
            "Epoch Step: 700 Loss: 40.617962\n",
            "Epoch Step: 800 Loss: 38.539471\n",
            "Epoch Step: 900 Loss: 37.734837\n",
            "Validation perplexity: 42.693599\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-B83Wyihlor"
      },
      "source": [
        "def greedy_decode(model, src_ids, src_lengths, max_len):\n",
        "  \"\"\"Greedily decode a sentence for EncoderDecoder. Make sure to chop off the \n",
        "     EOS token!\"\"\"\n",
        "\n",
        "  with torch.no_grad():\n",
        "    _, encoder_finals = model.encode(src_ids, src_lengths)\n",
        "    prev_y = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n",
        "\n",
        "  output = []\n",
        "  hidden = None\n",
        "  \n",
        "  # --------- Your code here --------- #\n",
        "\n",
        "  for i in range(max_len):\n",
        "      with torch.no_grad():\n",
        "          hidden, out = model.decode(encoder_finals, prev_y, hidden)\n",
        "          output.append(torch.argmax(model.generator(out)))\n",
        "          prev_y[0][0] = output[-1]\n",
        "\n",
        "          if output[-1] == EOS_INDEX:\n",
        "              output = output[:-1]\n",
        "              break\n",
        "\n",
        "  # --------- Your code ends --------- #\n",
        "\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7IYcUjgdjeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4d03aae-e055-4b30-8696-c61e9529592a"
      },
      "source": [
        "example_set = MTDataset(val_src_sentences_list, src_vocab_set,\n",
        "                        val_trg_sentences_list, trg_vocab_set)\n",
        "example_data_loader = data.DataLoader(example_set, batch_size=1, num_workers=1,\n",
        "                                      shuffle=False)\n",
        "\n",
        "\n",
        "print(\"EncoderDecoder Results:\")\n",
        "lab_utils.print_examples(pure_seq2seq, src_vocab_set, trg_vocab_set,\n",
        "                         example_data_loader, greedy_decode, n=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EncoderDecoder Results:\n",
            "Example #1\n",
            "Src :  hi , how are you doing ? i'm getting ready to do some cheetah chasing to stay in shape . i am ! for my hobby i like to do canning or some whittling .\n",
            "Trg :  that's neat . when i was in high school i placed 6th in 100m dash !\n",
            "Pred:  i like to read and play with my dog .\n",
            "\n",
            "Example #2\n",
            "Src :  i am ! for my hobby i like to do canning or some whittling . that's neat . when i was in high school i placed 6th in 100m dash !\n",
            "Trg :  i do not . but i do have a favorite meat since that is all i eat exclusively .\n",
            "Pred:  i am a student . i am a receptionist .\n",
            "\n",
            "Example #3\n",
            "Src :  that's neat . when i was in high school i placed 6th in 100m dash ! i do not . but i do have a favorite meat since that is all i eat exclusively .\n",
            "Trg :  i would have to say its prime rib . do you have any favorite foods ?\n",
            "Pred:  i like to eat meat . i love to eat meat .\n",
            "\n",
            "Example #4\n",
            "Src :  i do not . but i do have a favorite meat since that is all i eat exclusively . i would have to say its prime rib . do you have any favorite foods ?\n",
            "Trg :  do you have anything planned for today ? i think i am going to do some canning .\n",
            "Pred:  i like to eat meat . i love to eat meat .\n",
            "\n",
            "Example #5\n",
            "Src :  i would have to say its prime rib . do you have any favorite foods ? do you have anything planned for today ? i think i am going to do some canning .\n",
            "Trg :  i think i will can some jam . do you also play footfall for fun ?\n",
            "Pred:  i like to go to the beach . i love to go to the beach .\n",
            "\n",
            "Example #6\n",
            "Src :  do you have anything planned for today ? i think i am going to do some canning . i think i will can some jam . do you also play footfall for fun ?\n",
            "Trg :  hi , how are you doing today ?\n",
            "Pred:  i like to eat meat . i love to eat meat .\n",
            "\n",
            "Example #7\n",
            "Src :  i think i will can some jam . do you also play footfall for fun ? hi , how are you doing today ?\n",
            "Trg :  wow , four sisters . just watching game of thrones .\n",
            "Pred:  i am doing well . i am just getting ready to go to work .\n",
            "\n",
            "Example #8\n",
            "Src :  hi , how are you doing today ? wow , four sisters . just watching game of thrones .\n",
            "Trg :  i agree . what do you do for a living ?\n",
            "Pred:  i am a dancer , i love to read .\n",
            "\n",
            "Example #9\n",
            "Src :  wow , four sisters . just watching game of thrones . i agree . what do you do for a living ?\n",
            "Trg :  interesting . i'm a website designer . pretty much spend all my time on the computer .\n",
            "Pred:  i like to read . i am a waitress .\n",
            "\n",
            "Example #10\n",
            "Src :  i agree . what do you do for a living ? interesting . i'm a website designer . pretty much spend all my time on the computer .\n",
            "Trg :  that's awesome . i have always had a love for technology .\n",
            "Pred:  i am a teacher . i am a mom . i am a mom .\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXaBISR_riud"
      },
      "source": [
        "# ATTENTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyn4R5Kprm6y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}