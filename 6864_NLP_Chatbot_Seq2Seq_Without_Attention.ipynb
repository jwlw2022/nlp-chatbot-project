{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Emma - Copy of 6864_NLP_Chatbot_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jwlw2022/nlp-chatbot-project/blob/main/6864_NLP_Chatbot_Seq2Seq_Without_Attention.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EKujomfNfXLN"
      },
      "source": [
        "# Installations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prUa1Udrb3Vi",
        "outputId": "11cee6d5-ddc2-453e-b035-3a2f9304c171"
      },
      "source": [
        "# %%bash\n",
        "# Logistics #2: install the transformers package, create a folder, download the dataset and a patch\n",
        "!pip install pytorch-pretrained-bert\n",
        "!pip -q install transformers\n",
        "!pip -q install datasets\n",
        "!pip -q install tqdm\n",
        "!pip -q install sentencepiece "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pytorch-pretrained-bert\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/e0/c08d5553b89973d9a240605b9c12404bcf8227590de62bae27acbcfe076b/pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123kB)\n",
            "\r\u001b[K     |██▋                             | 10kB 24.4MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20kB 32.6MB/s eta 0:00:01\r\u001b[K     |████████                        | 30kB 25.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40kB 19.7MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51kB 10.3MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61kB 10.6MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71kB 10.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81kB 11.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92kB 12.3MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102kB 9.9MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112kB 9.9MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122kB 9.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 133kB 9.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n",
            "Collecting boto3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6e/58/4b422e8ecb8d6775fa2c5ce97f2b52fd8931b3417106ac56fef5b4ed2d4e/boto3-1.17.76-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 49.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.41.1)\n",
            "Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.8.1+cu101)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (1.24.3)\n",
            "Collecting jmespath<1.0.0,>=0.7.1\n",
            "  Downloading https://files.pythonhosted.org/packages/07/cb/5f001272b6faeb23c1c9e0acc04d48eaaf5c862c17709d20e3469c6e0139/jmespath-0.10.0-py2.py3-none-any.whl\n",
            "Collecting botocore<1.21.0,>=1.20.76\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bd/eb/1bf881ea46013687789a7c0ee0d76ff709772f11952194814730022fcca5/botocore-1.20.76-py2.py3-none-any.whl (7.5MB)\n",
            "\u001b[K     |████████████████████████████████| 7.5MB 37.4MB/s \n",
            "\u001b[?25hCollecting s3transfer<0.5.0,>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/63/d0/693477c688348654ddc21dcdce0817653a294aa43f41771084c25e7ff9c7/s3transfer-0.4.2-py2.py3-none-any.whl (79kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 12.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.7.4.3)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.21.0,>=1.20.76->boto3->pytorch-pretrained-bert) (2.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.21.0,>=1.20.76->boto3->pytorch-pretrained-bert) (1.15.0)\n",
            "\u001b[31mERROR: botocore 1.20.76 has requirement urllib3<1.27,>=1.25.4, but you'll have urllib3 1.24.3 which is incompatible.\u001b[0m\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n",
            "Successfully installed boto3-1.17.76 botocore-1.20.76 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.4.2\n",
            "\u001b[K     |████████████████████████████████| 2.3MB 7.9MB/s \n",
            "\u001b[K     |████████████████████████████████| 901kB 50.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.3MB 51.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 225kB 7.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 112kB 12.5MB/s \n",
            "\u001b[K     |████████████████████████████████| 245kB 12.4MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2MB 7.1MB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXbQMJ1ZfdOh"
      },
      "source": [
        "# Pretrained tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXVSBuG_eWBW"
      },
      "source": [
        "# import transformers\n",
        "\n",
        "# Use a pretrained tokenizer with CLASS.from_pretrained() function\n",
        "# tokenizer = transformers.AutoTokenizer.from_pretrained('distilbert-base-cased')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7BAhfXsifgTN"
      },
      "source": [
        "# Download PersonaChat dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YxHQxGgYZWM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cd1e43c-8a10-47e4-8722-59b1f6bfc2b4"
      },
      "source": [
        "import json\n",
        "from pytorch_pretrained_bert import cached_path\n",
        "\n",
        "url = \"https://s3.amazonaws.com/datasets.huggingface.co/personachat/personachat_self_original.json\"\n",
        "\n",
        "# Download and load JSON dataset\n",
        "personachat_file = cached_path(url)\n",
        "with open(personachat_file, \"r\", encoding=\"utf-8\") as f:\n",
        "    dataset = json.loads(f.read())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 209850483/209850483 [00:06<00:00, 32488461.63B/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AEfiMNY-0oat",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d2e5bc0-95b3-45a9-cad5-34b54fe056c2"
      },
      "source": [
        "for i in range(5):\n",
        "    print('Dialogue ', i)\n",
        "    # for dialogue in dataset['train'][i]['utterances']:\n",
        "    #     print(dialogue['history'])\n",
        "    print('ENTIRE CONVO: ', dataset['train'][i]['utterances'][-1]['history'])\n",
        "    print()\n",
        "\n",
        "# Old tokenizer\n",
        "# for i in range(5):\n",
        "#     print('Dialogue ', i)\n",
        "    # print('Persona: ')\n",
        "    # for persona in dataset['train'][i]['personality']:\n",
        "    #     print(persona)\n",
        "    # print('Utterances: ')\n",
        "    # for dialogue in dataset['train'][i]['utterances']:\n",
        "    #     print(dialogue['history'][-1])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dialogue  0\n",
            "ENTIRE CONVO:  [\"hi , how are you doing ? i'm getting ready to do some cheetah chasing to stay in shape .\", 'you must be very fast . hunting is one of my favorite hobbies .', 'i am ! for my hobby i like to do canning or some whittling .', 'i also remodel homes when i am not out bow hunting .', \"that's neat . when i was in high school i placed 6th in 100m dash !\", \"that's awesome . do you have a favorite season or time of year ?\", 'i do not . but i do have a favorite meat since that is all i eat exclusively .', 'what is your favorite meat to eat ?', 'i would have to say its prime rib . do you have any favorite foods ?', 'i like chicken or macaroni and cheese .', 'do you have anything planned for today ? i think i am going to do some canning .', 'i am going to watch football . what are you canning ?', 'i think i will can some jam . do you also play footfall for fun ?']\n",
            "\n",
            "Dialogue  1\n",
            "ENTIRE CONVO:  ['hi , how are you doing today ?', 'i am spending time with my 4 sisters what are you up to', 'wow , four sisters . just watching game of thrones .', 'that is a good show i watch that while drinking iced tea', 'i agree . what do you do for a living ?', \"i'm a researcher i'm researching the fact that mermaids are real\", \"interesting . i'm a website designer . pretty much spend all my time on the computer .\", \"that's cool my mom does the same thing\", \"that's awesome . i have always had a love for technology .\", 'tell me more about yourself', 'i really enjoy free diving , how about you , have any hobbies ?', \"i enjoy hanging with my mother she's my best friend\", \"that's nice . moms are pretty cool too .\"]\n",
            "\n",
            "Dialogue  2\n",
            "ENTIRE CONVO:  ['we all live in a yellow submarine , a yellow submarine . morning !', 'hi ! that is a great line for my next stand up .', 'lol . i am shy , anything to break the ice , and i am a beatles fan .', 'i can tell . i am not , you can see me in some tv shows', 'really ? what shows ? i like tv , it makes me forget i do not like my family', 'wow , i wish i had a big family . i grew up in a very small town .', 'i did too . i do not get along with mine . they have no class .', \"just drink some cola with rum and you'll forget about them !\", 'put the lime in the coconut as well . . .', \"nah , plain cuba libre , that's what we drank yesterday at the theater .\", 'i prefer mojitos . watermelon or cucumber .']\n",
            "\n",
            "Dialogue  3\n",
            "ENTIRE CONVO:  ['hi ! i work as a gourmet cook .', \"i don't like carrots . i throw them away .\", 'really . but , i can sing pitch perfect .', 'i also cook , and i ride my bike to work .', 'great ! i had won an award for spelling bee .', 'my contacts can see through what you are trying to sell me .', 'okay but i was published in new yorker once', 'you better not make any spelling mistakes .', 'i have not . i can cook any word you want me to', \"what is your ethnicity ? i'm white , and my hair is brown .\", \"i'm asian and have no hair .\", 'i love hairless asians . do you like carrots ?', 'i love carrots . i eat carrots like a horse .', 'are you male or female ?', 'i work as a gourmet cook who also has a pitch perfect voice .']\n",
            "\n",
            "Dialogue  4\n",
            "ENTIRE CONVO:  ['how are you doing today', 'what do you do for career ? i have a ton of hobbies if you are interested !', 'i like to watch kids', 'i actually play guitar and do a lot of manly things , like welding .', 'what do you weld ? houses ?', \"everything ! i am actually manly . but i've a secret i am hiding .\", 'what is your secret that you have', \"my parents don't know that i'm . . . homosexual .\", 'how does that feel for you', 'makes me secure with my manly hobby skills .', 'i bet that it does', 'anyway . what do you do ?', 'i watch kids for a living']\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhdDvpg3-FcI",
        "outputId": "3ac3ede7-d0b6-44c9-8f55-fb1683763a30"
      },
      "source": [
        "def tokenize(dataset):\n",
        "    train_tokens = []\n",
        "    for i in range(len(dataset['train'])):  # dialogues\n",
        "        train_dialogue_history = dataset['train'][i]['utterances'][-1]['history']\n",
        "        for dialogue in train_dialogue_history:\n",
        "            tokens = dialogue.split(' ')\n",
        "            train_tokens.append(tokens)\n",
        "\n",
        "    valid_tokens = []\n",
        "    for i in range(len(dataset['valid'])):  # dialogues\n",
        "        valid_dialogue_history = dataset['valid'][i]['utterances'][-1]['history']\n",
        "        for dialogue in valid_dialogue_history:\n",
        "            tokens = dialogue.split(' ')\n",
        "            valid_tokens.append(tokens)\n",
        "\n",
        "    train_source = []\n",
        "    train_target = []\n",
        "    for i in range(len(train_tokens)-2):\n",
        "        copy = train_tokens[i].copy()\n",
        "        copy.extend(['|'])\n",
        "        copy.extend(train_tokens[i+1])\n",
        "        train_source.append(copy)\n",
        "        train_target.append(train_tokens[i+2])\n",
        "\n",
        "    valid_source = []\n",
        "    valid_target = []\n",
        "    for i in range(len(valid_tokens)-2):\n",
        "        copy = valid_tokens[i].copy()\n",
        "        copy.extend(['|'])\n",
        "        copy.extend(valid_tokens[i+1])\n",
        "        valid_source.append(copy)\n",
        "        valid_target.append(valid_tokens[i+2])\n",
        "\n",
        "    return train_source, train_target, valid_source, valid_target\n",
        "\n",
        "print(len(dataset['train']))\n",
        "train_source, train_target, valid_source, valid_target = tokenize(dataset)\n",
        "print(train_source[0])\n",
        "print(len(train_source))\n",
        "print(len(train_target))\n",
        "print(len(valid_source))\n",
        "print(len(valid_target))\n",
        "# print(train_target)\n",
        "# print(valid_source)\n",
        "# print(valid_target)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17878\n",
            "['hi', ',', 'how', 'are', 'you', 'doing', '?', \"i'm\", 'getting', 'ready', 'to', 'do', 'some', 'cheetah', 'chasing', 'to', 'stay', 'in', 'shape', '.', '|', 'you', 'must', 'be', 'very', 'fast', '.', 'hunting', 'is', 'one', 'of', 'my', 'favorite', 'hobbies', '.']\n",
            "244996\n",
            "244996\n",
            "14600\n",
            "14600\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E7IfNMtlEYQ7",
        "outputId": "4f419538-a868-40d1-e6db-167beea0eb30"
      },
      "source": [
        "# a = max(list(map(len, train_source)))\n",
        "# print(a)\n",
        "\n",
        "a = max([len(x) for x in train_source])\n",
        "print(a)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "76\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NkLOulsuge-2"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "import torch.optim as optim"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T6fgzQE6hA8p"
      },
      "source": [
        "# Seq2Seq Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TvKACvyBRH2q"
      },
      "source": [
        "## Download Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SBa2QRPi7AIu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0a4c70f0-e717-4ec9-9d19-18fba918ad56"
      },
      "source": [
        "from google.colab import drive \n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "MODEL_FOLDER = \"/content/drive/My Drive/mit-6864/hw3\"\n",
        "!mkdir -p \"/content/drive/My Drive/mit-6864/hw3\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HzzPi7ouR9rJ",
        "outputId": "3b5de4bf-e273-4321-c055-b1d3771b06be"
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/mit-6864/hw3.git\n",
        "mkdir -p /content/hw3/data\n",
        "\n",
        "pip install sacrebleu"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sacrebleu\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/57/0c7ca4e31a126189dab99c19951910bd081dea5bbd25f24b77107750eae7/sacrebleu-1.5.1-py3-none-any.whl (54kB)\n",
            "Collecting portalocker==2.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/89/a6/3814b7107e0788040870e8825eebf214d72166adf656ba7d4bf14759a06a/portalocker-2.0.0-py2.py3-none-any.whl\n",
            "Installing collected packages: portalocker, sacrebleu\n",
            "Successfully installed portalocker-2.0.0 sacrebleu-1.5.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'hw3'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suepLPcGR27_"
      },
      "source": [
        "# Download data\n",
        "DATA_DIR = \"/content/hw3/data\"\n",
        "\n",
        "# !wget -nv -O \"$DATA_DIR/train.en\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.en\n",
        "# !wget -nv -O \"$DATA_DIR/train.vi\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/train.vi\n",
        "# !wget -nv -O \"$DATA_DIR/vocab.en\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.en\n",
        "# !wget -nv -O \"$DATA_DIR/vocab.vi\" https://nlp.stanford.edu/projects/nmt/data/iwslt15.en-vi/vocab.vi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qsGbOGWTs3sB",
        "outputId": "1f897375-54ab-4d94-8f8f-7dbb49587705"
      },
      "source": [
        "words = set()\n",
        "for sentence in train_source:\n",
        "  words.update(sentence)\n",
        "for sentence in train_target:\n",
        "  words.update(sentence)\n",
        "for sentence in valid_source:\n",
        "  words.update(sentence)\n",
        "for sentence in valid_target:\n",
        "  words.update(sentence)\n",
        "print(len(words))\n",
        "with open('/content/hw3/data/vocab.txt', 'w') as writefile:\n",
        "    writefile.write('\\n'.join(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "18975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bFWyGs6kRxD7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "b42ffd1e-c0df-42ef-bac2-1e17b36a5d06"
      },
      "source": [
        "import sys\n",
        "sys.path.append(\"/content/hw3/\")\n",
        "#sys.path.append('/hw3')\n",
        "\n",
        "import lab_utils\n",
        "\n",
        "import torch \n",
        "# !pip install torch==1.6.0 torchvision==0.7.0\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "assert device == \"cuda\"   # use gpu whenever you can!\n",
        "\n",
        "seed = 42\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "from lab_utils import read_vocab_file, read_sentence_file, filter_data, show_some_data_stats\n",
        "\n",
        "# src_vocab_set = read_vocab_file(\"vocab.vi\")\n",
        "# trg_vocab_set = read_vocab_file(\"vocab.en\")\n",
        "trg_vocab_set = list(words)\n",
        "# trg_vocab_set = ['<pad>'] + ['<unk>'] + ['<s>'] + ['</s>'] + list(words)\n",
        "src_vocab_set = trg_vocab_set\n",
        "\n",
        "train_src_sentences_list = train_source\n",
        "train_trg_sentences_list = train_target\n",
        "assert len(train_src_sentences_list) == len(train_trg_sentences_list)\n",
        "\n",
        "# test_src_sentences_list = read_sentence_file(\"tst2013.vi\")\n",
        "# test_trg_sentences_list = read_sentence_file(\"tst2013.en\")\n",
        "test_src_sentences_list = valid_source\n",
        "test_trg_sentences_list = valid_target\n",
        "assert len(test_src_sentences_list) == len(test_trg_sentences_list)\n",
        "\n",
        "# Filter out sentences over 48 words long\n",
        "MAX_SENT_LENGTH = 45\n",
        "MAX_SENT_LENGTH_PLUS_SOS_EOS = 46\n",
        "\n",
        "train_src_sentences_list, train_trg_sentences_list = filter_data(\n",
        "    train_src_sentences_list, train_trg_sentences_list, MAX_SENT_LENGTH)\n",
        "test_src_sentences_list, test_trg_sentences_list = filter_data(\n",
        "    test_src_sentences_list, test_trg_sentences_list, MAX_SENT_LENGTH)\n",
        "\n",
        "# We take 10% of training data as validation set.\n",
        "num_val = int(len(train_src_sentences_list) * 0.1)\n",
        "val_src_sentences_list = train_src_sentences_list[:num_val]\n",
        "val_trg_sentences_list = train_trg_sentences_list[:num_val]\n",
        "train_src_sentences_list = train_src_sentences_list[num_val:]\n",
        "train_trg_sentences_list = train_trg_sentences_list[num_val:]\n",
        "\n",
        "show_some_data_stats(train_src_sentences_list, val_src_sentences_list, \n",
        "                     test_src_sentences_list, train_trg_sentences_list,\n",
        "                     src_vocab_set, trg_vocab_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training (src, trg) sentence pairs: 220473\n",
            "Number of validation (src, trg) sentence pairs: 24496\n",
            "Number of testing (src, trg) sentence pairs: 14600\n",
            "Size of en vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 18976\n",
            "Size of vi vocab set (including '<pad>', '<unk>', '<s>', '</s>'): 18976\n",
            "Training sentence avg. length: 23 \n",
            "Training sentence length at 95-percentile: 34\n",
            "Training sentence length distribution (x-axis is length range and y-axis is count):\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Example Vietnamese input: ['thank', 'you', ',', 'you', 'are', 'so', 'sweet', '.', 'do', 'you', 'exercise', '?', '|', 'i', 'do', 'sometimes', 'but', 'i', 'mostly', 'do', 'crafts', 'its', 'where', 'most', 'my', 'money', 'goes']\n",
            "Its target English output: ['oh', 'that', 'sounds', 'super', 'neat', ',', 'what', 'kind', 'of', 'things', 'do', 'you', 'make', '?']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDcyruI0UcXe"
      },
      "source": [
        "import torch\n",
        "from torch.utils import data\n",
        "\n",
        "# These IDs are reserved.\n",
        "PAD_INDEX = 0\n",
        "UNK_INDEX = 1\n",
        "SOS_INDEX = 2\n",
        "EOS_INDEX = 3\n",
        "\n",
        "\n",
        "class MTDataset(data.Dataset):\n",
        "  def __init__(self, src_sentences, src_vocabs, trg_sentences, trg_vocabs,\n",
        "               sampling=1.):\n",
        "    self.src_sentences = src_sentences[:int(len(src_sentences) * sampling)]\n",
        "    self.trg_sentences = trg_sentences[:int(len(src_sentences) * sampling)]\n",
        "\n",
        "    self.max_src_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n",
        "    self.max_trg_seq_length = MAX_SENT_LENGTH_PLUS_SOS_EOS\n",
        "\n",
        "    self.src_vocabs = src_vocabs\n",
        "    self.trg_vocabs = trg_vocabs\n",
        "\n",
        "    self.src_v2id = {v : i for i, v in enumerate(src_vocabs)}\n",
        "    self.src_id2v = {val : key for key, val in self.src_v2id.items()}\n",
        "    self.trg_v2id = {v : i for i, v in enumerate(trg_vocabs)}\n",
        "    self.trg_id2v = {val : key for key, val in self.trg_v2id.items()}\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.src_sentences)\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    src_sent = self.src_sentences[index]\n",
        "    src_len = len(src_sent) + 1\n",
        "    src_id = []\n",
        "    for w in src_sent:\n",
        "      if w == '|':\n",
        "        # src_id.append(EOS_INDEX)\n",
        "        # src_id.append(SOS_INDEX)\n",
        "        pass\n",
        "      else:\n",
        "        if w not in self.src_vocabs:\n",
        "          w = '<unk>'\n",
        "        src_id.append(self.src_v2id[w])\n",
        "    src_id = ([SOS_INDEX] + src_id + [EOS_INDEX] + [PAD_INDEX] *\n",
        "               (self.max_src_seq_length - src_len))\n",
        "    #src_id = (src_id + [PAD_INDEX] * (self.max_src_seq_length - src_len))\n",
        "\n",
        "    trg_sent = self.trg_sentences[index]\n",
        "    trg_len = len(trg_sent) + 2\n",
        "    trg_id = []\n",
        "    for w in trg_sent:\n",
        "      if w not in self.trg_vocabs:\n",
        "        w = '<unk>'\n",
        "      trg_id.append(self.trg_v2id[w])\n",
        "    trg_id = ([SOS_INDEX] + trg_id + [EOS_INDEX] + [PAD_INDEX] *\n",
        "               (self.max_trg_seq_length - trg_len))\n",
        "    #trg_id = (trg_id + [PAD_INDEX] * (self.max_trg_seq_length - trg_len))\n",
        "\n",
        "    return torch.tensor(src_id), src_len, torch.tensor(trg_id), trg_len"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j1p99-z4UeJH"
      },
      "source": [
        "## Encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xuHHZDDhCD9"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, dropout=0.):\n",
        "    \"\"\"\n",
        "    Inputs: \n",
        "      - `input_size`: an int representing the RNN input size.\n",
        "      - `hidden_size`: an int representing the RNN hidden size.\n",
        "      - `dropout`: a float representing the dropout rate during training. Note\n",
        "          that for 1-layer RNN this has no effect since dropout only applies to\n",
        "          outputs of intermediate layers.\n",
        "    \"\"\"\n",
        "    \n",
        "    super(Encoder, self).__init__()\n",
        "\n",
        "    # --------- Your code here --------- #\n",
        "    # feel free to use a pre-implemented pytorch GRU\n",
        "    # https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "\n",
        "    self.rnn = nn.GRU(input_size=input_size,\n",
        "                      hidden_size=hidden_size,\n",
        "                      num_layers=1,\n",
        "                      batch_first=True,\n",
        "                      dropout=dropout)\n",
        "    \n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "  def forward(self, inputs, lengths):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of source\n",
        "          sentences.\n",
        "      - `lengths`: a 1d-tensor of shape (batch_size,) representing the sequence\n",
        "          lengths of `inputs`.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "        (batch_size, max_seq_length, hidden_size).\n",
        "      - `finals`: a 3d-tensor of shape (num_layers, batch_size, hidden_size).\n",
        "\n",
        "      Hint: `outputs` and `finals` are both standard GRU outputs.\n",
        "    \"\"\"\n",
        "    outputs = None\n",
        "    finals = None\n",
        "    \n",
        "    # --------- Your code here --------- #\n",
        "    # hint: you probably want to pack the inputs and outputs (see note below)\n",
        "    #       https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html\n",
        "    # hint2: given the shape of the inputs and outputs, \n",
        "    #        it might be helpful to specify batch_first=True (also in __init___)\n",
        "    # hint3: MAX_SENT_LENGTH_PLUS_SOS_EOS is a global variable that exists if \n",
        "    #        you ever need to specify a total_length for outputs\n",
        "\n",
        "    packed_sequence = pack_padded_sequence(inputs,\n",
        "                                           lengths.cpu(),\n",
        "                                           batch_first=True,\n",
        "                                           enforce_sorted=False)\n",
        "    outputs, finals = self.rnn(packed_sequence)\n",
        "    outputs, _ = pad_packed_sequence(outputs, batch_first=True)\n",
        "    \n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "    return outputs, finals"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOGgHO0HRSiW"
      },
      "source": [
        "## Decoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SCzMw3ghhKcw"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "  \"\"\"An RNN decoder without attention.\"\"\"\n",
        "\n",
        "  def __init__(self, input_size, hidden_size, dropout=0.):\n",
        "    \"\"\"\n",
        "      Inputs:\n",
        "        - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n",
        "    \"\"\"\n",
        "    super(Decoder, self).__init__()\n",
        "\n",
        "    # --------- Your code here --------- #\n",
        "    # hint: you need more layers than the encoder\n",
        "    #       again, feel free to use pytorch implemetnations\n",
        "    #       https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "    \n",
        "    # To initialize from the final encoder state.\n",
        "\n",
        "    self.rnn = nn.GRU(input_size=input_size,\n",
        "                      hidden_size=hidden_size,\n",
        "                      num_layers=1,\n",
        "                      batch_first=True,\n",
        "                      dropout=dropout)\n",
        "\n",
        "    self.bridge = nn.Linear(hidden_size, input_size, bias=True)\n",
        "\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "  def forward_step(self, prev_embed, hidden):\n",
        "    \"\"\"Helper function for forward below:\n",
        "       Perform a single decoder step (1 word).\n",
        "\n",
        "       Inputs:\n",
        "      - `prev_embed`: a 3d-tensor of shape (batch_size, 1, embed_size)\n",
        "          representing the padded embedded word vectors at this step in training\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the current hidden state.\n",
        "\n",
        "      Returns:\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the current decoder hidden state.\n",
        "      - `pre_output`: a 3d-tensor of shape (batch_size, 1, hidden_size)\n",
        "          representing the total decoder output for one step\n",
        "    \"\"\"\n",
        "    pre_output = None\n",
        "    # --------- Your code here --------- #\n",
        "    \n",
        "    pre_output, hidden = self.rnn(prev_embed, hidden)\n",
        "\n",
        "    # --------- Your code ends --------- #\n",
        "    return hidden, pre_output\n",
        "\n",
        "  def forward(self, inputs, encoder_finals, hidden=None, max_len=None):\n",
        "    \"\"\"Unroll the decoder one step at a time.\n",
        "\n",
        "    Inputs:\n",
        "      - `inputs`: a 3d-tensor of shape (batch_size, max_seq_length, embed_size)\n",
        "          representing a batch of padded embedded word vectors of target\n",
        "          sentences (for teacher-forcing during training).\n",
        "      - `encoder_finals`: a 3d-tensor of shape\n",
        "          (num_enc_layers, batch_size, hidden_size) representing the final\n",
        "          encoder hidden states used to initialize the initial decoder hidden\n",
        "          states.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size) representing\n",
        "          the value to be used to initialize the initial decoder hidden states.\n",
        "          If None, then use `encoder_finals`.\n",
        "      - `max_len`: an int representing the maximum decoding length.\n",
        "\n",
        "    Returns:\n",
        "      - `outputs`: a 3d-tensor of shape\n",
        "          (batch_size, max_seq_length, hidden_size) representing the raw\n",
        "          decoder outputs (before converting to a `trg_vocab_size`-dim vector).\n",
        "          We will convert it later in a `Generator` below.\n",
        "      - `hidden`: a 3d-tensor of shape (1, batch_size, hidden_size)\n",
        "          representing the last decoder hidden state.\n",
        "    \"\"\"\n",
        "\n",
        "    # The maximum number of steps to unroll the RNN.\n",
        "    if max_len is None:\n",
        "      max_len = inputs.size(1)\n",
        "\n",
        "    # Initialize decoder hidden state.\n",
        "    if hidden is None:\n",
        "      hidden = self.init_hidden(encoder_finals)\n",
        "\n",
        "    outputs = []\n",
        "    \n",
        "    # --------- Your code here --------- #\n",
        "\n",
        "    # Unroll the decoder RNN for `max_len` steps.\n",
        "    # hint: use the above helper function forward_step that \n",
        "    #       performs a single decoder step (1 word).\n",
        "\n",
        "    for i in range(max_len):\n",
        "        prev_embed = inputs[:, i:i+1, :] # get embeddings from inputs\n",
        "        hidden, pre_output = self.forward_step(prev_embed, hidden)\n",
        "        outputs.append(pre_output)\n",
        "    \n",
        "    # final output concatenates outputs for each word\n",
        "    outputs = torch.cat(outputs, dim=1)\n",
        "\n",
        "    # --------- Your code ends --------- #\n",
        "\n",
        "    return hidden, outputs\n",
        "\n",
        "  def init_hidden(self, encoder_finals):\n",
        "    \"\"\"Use encoder final hidden state to initialize decoder's first hidden\n",
        "    state.\"\"\"\n",
        "    decoder_init_hiddens = torch.tanh(self.bridge(encoder_finals))\n",
        "\n",
        "    return decoder_init_hiddens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmzMRvO0fhbU"
      },
      "source": [
        "# class CustomAttnDecoder(nn.Module):\n",
        "#     \"\"\"An RNN decoder without attention.\"\"\"\n",
        "\n",
        "#     def __init__(self, input_size, hidden_size, dropout=0.):\n",
        "#         \"\"\"\n",
        "#           Inputs:\n",
        "#             - `input_size`, `hidden_size`, and `dropout` the same as in Encoder.\n",
        "#         \"\"\"\n",
        "#         super(Decoder, self).__init__()\n",
        "\n",
        "#         # --------- Your code here --------- #\n",
        "#         # hint: you need more layers than the encoder\n",
        "#         #       again, feel free to use pytorch implemetnations\n",
        "#         #       https://pytorch.org/docs/stable/generated/torch.nn.GRU.html\n",
        "        \n",
        "#         # To initialize from the final encoder state.\n",
        "\n",
        "#         self.rnn = nn.GRU(input_size=input_size,\n",
        "#                           hidden_size=hidden_size,\n",
        "#                           num_layers=3,\n",
        "#                           batch_first=True,\n",
        "#                           dropout=dropout)\n",
        "\n",
        "#         self.bridge = nn.Linear(hidden_size, input_size, bias=True)\n",
        "#         self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "#         self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "#         self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "#         self.dropout = nn.Dropout(self.dropout_p)\n",
        "#         self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "#         self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "#     def forward(self, input, hidden, encoder_outputs):\n",
        "#         embedded = self.embedding(input).view(1, 1, -1)\n",
        "#         embedded = self.dropout(embedded)\n",
        "\n",
        "#         attn_weights = F.softmax(\n",
        "#             self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "#         attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "#                                  encoder_outputs.unsqueeze(0))\n",
        "\n",
        "#         output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "#         output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "#         output = F.relu(output)\n",
        "#         output, hidden = self.gru(output, hidden)\n",
        "\n",
        "#         output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "#         return output, hidden, attn_weights\n",
        "\n",
        "#     def initHidden(self):\n",
        "#         return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_g_Mh7mRULr"
      },
      "source": [
        "## EncoderDecoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtN_OJxAhLVH"
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  \"\"\"A standard Encoder-Decoder architecture without attention.\n",
        "  \"\"\"\n",
        "  def __init__(self, encoder, decoder, src_embed, trg_embed, generator):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      - `encoder`: an `Encoder` object.\n",
        "      - `decoder`: a `Decoder` object.\n",
        "      - `src_embed`: an nn.Embedding object representing the lookup table for\n",
        "          input (source) sentences.\n",
        "      - `trg_embed`: an nn.Embedding object representing the lookup table for\n",
        "          output (target) sentences.\n",
        "      - `generator`: a `Generator` object. Essentially a linear mapping. See\n",
        "          the next code cell.\n",
        "    \"\"\"\n",
        "    super(EncoderDecoder, self).__init__()\n",
        "\n",
        "    self.encoder = encoder\n",
        "    self.decoder = decoder\n",
        "    self.src_embed = src_embed\n",
        "    self.trg_embed = trg_embed\n",
        "    self.generator = generator\n",
        "\n",
        "  def forward(self, src_ids, trg_ids, src_lengths):\n",
        "    \"\"\"Take in and process masked source and target sequences.\n",
        "\n",
        "    Inputs:\n",
        "      `src_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of source sentences of word ids.\n",
        "      `trg_ids`: a 2d-tensor of shape (batch_size, max_seq_length) representing\n",
        "        a batch of target sentences of word ids.\n",
        "      `src_lengths`: a 1d-tensor of shape (batch_size,) representing the\n",
        "        sequence length of `src_ids`.\n",
        "\n",
        "    Returns the decoder outputs, see the above cell.\n",
        "    \"\"\"\n",
        "    encoder_hiddens, encoder_finals = self.encode(src_ids, src_lengths)\n",
        "    del encoder_hiddens   # unused\n",
        "    return self.decode(encoder_finals, trg_ids[:, :-1])\n",
        "\n",
        "  def encode(self, src_ids, src_lengths):\n",
        "    return self.encoder(self.src_embed(src_ids), src_lengths)\n",
        "    \n",
        "  def decode(self, encoder_finals, trg_ids, decoder_hidden=None):\n",
        "    return self.decoder(self.trg_embed(trg_ids), encoder_finals, decoder_hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wSwpEe1ORV4-"
      },
      "source": [
        "## Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx55R2LihLcp"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "  \"\"\"Define standard linear + softmax generation step.\"\"\"\n",
        "  def __init__(self, hidden_size, vocab_size):\n",
        "    super(Generator, self).__init__()\n",
        "    self.proj = nn.Linear(hidden_size, vocab_size, bias=False)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return F.log_softmax(self.proj(x), dim=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kv_504_2RZF3"
      },
      "source": [
        "## Training loop"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmTIw_g8TfW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc3dfc9c-882c-4392-b875-b55ba004e273"
      },
      "source": [
        "from torch.utils import data\n",
        "\n",
        "batch_size = 128\n",
        "\n",
        "# You can try on a smaller training set by setting a smaller `sampling`.\n",
        "train_set = MTDataset(train_src_sentences_list, src_vocab_set,\n",
        "                      train_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "# print('TRAIN')\n",
        "# for i in range(len(train_set)):\n",
        "#   if len(train_set[i][0]) != 48 or len(train_set[i][2]) != 48:\n",
        "#     print(train_set[i])\n",
        "#     break\n",
        "# print(len(train_set))\n",
        "# print(train_set[0])\n",
        "# print(len(train_set[0][0])) # source\n",
        "# print(len(train_set[0][2])) # target\n",
        "# print(train_set[126])\n",
        "# print(len(train_set[126][0])) # source\n",
        "# print(len(train_set[126][2])) # target\n",
        "train_data_loader = data.DataLoader(train_set, batch_size=batch_size,\n",
        "                                    num_workers=8, shuffle=True)\n",
        "\n",
        "val_set = MTDataset(val_src_sentences_list, src_vocab_set,\n",
        "                    val_trg_sentences_list, trg_vocab_set, sampling=1.)\n",
        "# print('VAL')\n",
        "# for i in range(len(val_set)):\n",
        "#   if len(val_set[i][0]) != 48 or len(val_set[i][2]) != 48:\n",
        "#     print(val_set[i])\n",
        "#     break\n",
        "val_data_loader = data.DataLoader(val_set, batch_size=batch_size, num_workers=8,\n",
        "                                  shuffle=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_9-IjIoghKan"
      },
      "source": [
        "import math\n",
        "\n",
        "\n",
        "class SimpleLossCompute:\n",
        "  \"\"\"A simple loss compute and train function.\"\"\"\n",
        "\n",
        "  def __init__(self, generator, criterion, opt=None):\n",
        "    self.generator = generator\n",
        "    self.criterion = criterion\n",
        "    self.opt = opt\n",
        "\n",
        "  def __call__(self, x, y, norm):\n",
        "    x = self.generator(x)\n",
        "    loss = self.criterion(x.contiguous().view(-1, x.size(-1)),\n",
        "                          y.contiguous().view(-1))\n",
        "    loss = loss / norm\n",
        "\n",
        "    if self.opt is not None:  # training mode\n",
        "      loss.backward()          \n",
        "      self.opt.step()\n",
        "      self.opt.zero_grad()\n",
        "\n",
        "    return loss.data.item() * norm\n",
        "\n",
        "\n",
        "def run_epoch(data_loader, model, loss_compute, print_every):\n",
        "  \"\"\"Standard Training and Logging Function\"\"\"\n",
        "\n",
        "  total_tokens = 0\n",
        "  total_loss = 0\n",
        "\n",
        "  for i, (src_ids_BxT, src_lengths_B, trg_ids_BxL, trg_lengths_B) in enumerate(data_loader):\n",
        "    # We define some notations here to help you understand the loaded tensor\n",
        "    # shapes:\n",
        "    #   `B`: batch size\n",
        "    #   `T`: max sequence length of source sentences\n",
        "    #   `L`: max sequence length of target sentences; due to our preprocessing\n",
        "    #        in the beginning, `L` == `T` == 50\n",
        "    # An example of `src_ids_BxT` (when B = 2):\n",
        "    #   [[2, 4, 6, 7, ..., 4, 3, 0, 0, 0],\n",
        "    #    [2, 8, 6, 5, ..., 9, 5, 4, 3, 0]]\n",
        "    # The corresponding `src_lengths_B` would be [47, 49].\n",
        "    # Note that SOS_INDEX == 2, EOS_INDEX == 3, and PAD_INDEX = 0.\n",
        "\n",
        "    src_ids_BxT = src_ids_BxT.to(device)\n",
        "    src_lengths_B = src_lengths_B.to(device)\n",
        "    trg_ids_BxL = trg_ids_BxL.to(device)\n",
        "\n",
        "    del trg_lengths_B   # unused\n",
        "\n",
        "    _, output = model(src_ids_BxT, trg_ids_BxL, src_lengths_B)\n",
        "\n",
        "    loss = loss_compute(x=output, y=trg_ids_BxL[:, 1:],\n",
        "                        norm=src_ids_BxT.size(0))\n",
        "    total_loss += loss\n",
        "    total_tokens += (trg_ids_BxL[:, 1:] != PAD_INDEX).data.sum().item()\n",
        "\n",
        "    if model.training and i % print_every == 0:\n",
        "      print(\"Epoch Step: %d Loss: %f\" % (i, loss / src_ids_BxT.size(0)))\n",
        "\n",
        "  return math.exp(total_loss / float(total_tokens))\n",
        "\n",
        "\n",
        "def train(model, num_epochs, learning_rate, print_every):\n",
        "  # Set `ignore_index` as PAD_INDEX so that pad tokens won't be included when\n",
        "  # computing the loss.\n",
        "  criterion = nn.NLLLoss(reduction=\"sum\", ignore_index=PAD_INDEX)\n",
        "  optim = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "  # Keep track of dev ppl for each epoch.\n",
        "  dev_ppls = []\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print(\"Epoch\", epoch)\n",
        "\n",
        "    model.train()\n",
        "    train_ppl = run_epoch(data_loader=train_data_loader, model=model,\n",
        "                          loss_compute=SimpleLossCompute(model.generator,\n",
        "                                                         criterion, optim),\n",
        "                          print_every=print_every)\n",
        "        \n",
        "    model.eval()\n",
        "    with torch.no_grad():      \n",
        "      dev_ppl = run_epoch(data_loader=val_data_loader, model=model, \n",
        "                          loss_compute=SimpleLossCompute(model.generator,\n",
        "                                                         criterion, None),\n",
        "                          print_every=print_every)\n",
        "      print(\"Validation perplexity: %f\" % dev_ppl)\n",
        "      dev_ppls.append(dev_ppl)\n",
        "        \n",
        "  return dev_ppls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zg0HolDAhKYJ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bd355882-806e-43e4-9ba6-28b4ef757ac9"
      },
      "source": [
        "device = 'cuda'\n",
        "\n",
        "# Hyperparameters for contructing the encoder-decoder model.\n",
        "embed_size = 1024   # Each word will be represented as a `embed_size`-dim vector.\n",
        "hidden_size = 1024  # RNN hidden size.\n",
        "dropout = 0.2\n",
        "\n",
        "pure_seq2seq = EncoderDecoder(\n",
        "    encoder=Encoder(embed_size, hidden_size, dropout=dropout),\n",
        "    decoder=Decoder(embed_size, hidden_size, dropout=dropout),\n",
        "    src_embed=nn.Embedding(len(src_vocab_set), embed_size),\n",
        "    trg_embed=nn.Embedding(len(trg_vocab_set), embed_size),\n",
        "    generator=Generator(hidden_size, len(trg_vocab_set))).to(device)\n",
        "\n",
        "train_model = True\n",
        "if train_model:\n",
        "  # Start training. The returned `dev_ppls` is a list of dev perplexity for each\n",
        "  # epoch.\n",
        "  pure_dev_ppls = train(pure_seq2seq, num_epochs=10, learning_rate=1e-3,\n",
        "                        print_every=100)\n",
        "  \n",
        "  torch.save(pure_seq2seq.state_dict(), MODEL_FOLDER+\"/\" + \"pure_seq2seq_embed_size_512_3layer.pt\")\n",
        "\n",
        "  # Plot perplexity\n",
        "  lab_utils.plot_perplexity(pure_dev_ppls)\n",
        "else:\n",
        "  pure_seq2seq.load_state_dict(torch.load(MODEL_FOLDER+\"/\" + \"pure_seq2seq_embed_size_256_1layer.pt\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:63: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:477: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 4, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch Step: 0 Loss: 118.145546\n",
            "Epoch Step: 100 Loss: 52.229900\n",
            "Epoch Step: 200 Loss: 48.452335\n",
            "Epoch Step: 300 Loss: 47.190815\n",
            "Epoch Step: 400 Loss: 44.711319\n",
            "Epoch Step: 500 Loss: 43.243710\n",
            "Epoch Step: 600 Loss: 42.643974\n",
            "Epoch Step: 700 Loss: 49.299541\n",
            "Epoch Step: 800 Loss: 41.201199\n",
            "Epoch Step: 900 Loss: 45.548534\n",
            "Epoch Step: 1000 Loss: 41.589634\n",
            "Epoch Step: 1100 Loss: 41.783340\n",
            "Epoch Step: 1200 Loss: 42.537537\n",
            "Epoch Step: 1300 Loss: 45.322777\n",
            "Epoch Step: 1400 Loss: 40.566673\n",
            "Epoch Step: 1500 Loss: 41.506481\n",
            "Epoch Step: 1600 Loss: 43.690800\n",
            "Epoch Step: 1700 Loss: 41.795918\n",
            "Validation perplexity: 27.969140\n",
            "Epoch 1\n",
            "Epoch Step: 0 Loss: 37.560841\n",
            "Epoch Step: 100 Loss: 35.682961\n",
            "Epoch Step: 200 Loss: 34.733070\n",
            "Epoch Step: 300 Loss: 31.181545\n",
            "Epoch Step: 400 Loss: 33.573673\n",
            "Epoch Step: 500 Loss: 34.589542\n",
            "Epoch Step: 600 Loss: 33.508762\n",
            "Epoch Step: 700 Loss: 33.884396\n",
            "Epoch Step: 800 Loss: 38.170837\n",
            "Epoch Step: 900 Loss: 32.885002\n",
            "Epoch Step: 1000 Loss: 32.774441\n",
            "Epoch Step: 1100 Loss: 38.343624\n",
            "Epoch Step: 1200 Loss: 36.177925\n",
            "Epoch Step: 1300 Loss: 35.967854\n",
            "Epoch Step: 1400 Loss: 33.241325\n",
            "Epoch Step: 1500 Loss: 32.674625\n",
            "Epoch Step: 1600 Loss: 33.367233\n",
            "Epoch Step: 1700 Loss: 35.202911\n",
            "Validation perplexity: 19.533072\n",
            "Epoch 2\n",
            "Epoch Step: 0 Loss: 26.744333\n",
            "Epoch Step: 100 Loss: 29.687342\n",
            "Epoch Step: 200 Loss: 29.665344\n",
            "Epoch Step: 300 Loss: 29.336830\n",
            "Epoch Step: 400 Loss: 29.731764\n",
            "Epoch Step: 500 Loss: 29.784433\n",
            "Epoch Step: 600 Loss: 26.571354\n",
            "Epoch Step: 700 Loss: 28.174431\n",
            "Epoch Step: 800 Loss: 31.577085\n",
            "Epoch Step: 900 Loss: 31.778473\n",
            "Epoch Step: 1000 Loss: 30.401525\n",
            "Epoch Step: 1100 Loss: 28.511768\n",
            "Epoch Step: 1200 Loss: 29.980196\n",
            "Epoch Step: 1300 Loss: 29.668354\n",
            "Epoch Step: 1400 Loss: 27.945414\n",
            "Epoch Step: 1500 Loss: 30.635988\n",
            "Epoch Step: 1600 Loss: 29.916067\n",
            "Epoch Step: 1700 Loss: 30.311960\n",
            "Validation perplexity: 15.457281\n",
            "Epoch 3\n",
            "Epoch Step: 0 Loss: 25.585442\n",
            "Epoch Step: 100 Loss: 24.437822\n",
            "Epoch Step: 200 Loss: 23.755852\n",
            "Epoch Step: 300 Loss: 25.790859\n",
            "Epoch Step: 400 Loss: 24.470764\n",
            "Epoch Step: 500 Loss: 26.611048\n",
            "Epoch Step: 600 Loss: 24.570278\n",
            "Epoch Step: 700 Loss: 25.811628\n",
            "Epoch Step: 800 Loss: 25.748259\n",
            "Epoch Step: 900 Loss: 27.010345\n",
            "Epoch Step: 1000 Loss: 24.607185\n",
            "Epoch Step: 1100 Loss: 24.600330\n",
            "Epoch Step: 1200 Loss: 24.361757\n",
            "Epoch Step: 1300 Loss: 24.152302\n",
            "Epoch Step: 1400 Loss: 25.714212\n",
            "Epoch Step: 1500 Loss: 27.477600\n",
            "Epoch Step: 1600 Loss: 25.844402\n",
            "Epoch Step: 1700 Loss: 24.729263\n",
            "Validation perplexity: 12.856840\n",
            "Epoch 4\n",
            "Epoch Step: 0 Loss: 23.393820\n",
            "Epoch Step: 100 Loss: 21.005856\n",
            "Epoch Step: 200 Loss: 22.618677\n",
            "Epoch Step: 300 Loss: 22.557459\n",
            "Epoch Step: 400 Loss: 21.080429\n",
            "Epoch Step: 500 Loss: 23.650932\n",
            "Epoch Step: 600 Loss: 22.969355\n",
            "Epoch Step: 700 Loss: 22.489908\n",
            "Epoch Step: 800 Loss: 24.313477\n",
            "Epoch Step: 900 Loss: 23.459362\n",
            "Epoch Step: 1000 Loss: 23.756796\n",
            "Epoch Step: 1100 Loss: 22.850376\n",
            "Epoch Step: 1200 Loss: 24.977495\n",
            "Epoch Step: 1300 Loss: 23.794876\n",
            "Epoch Step: 1400 Loss: 24.608221\n",
            "Epoch Step: 1500 Loss: 23.897768\n",
            "Epoch Step: 1600 Loss: 23.499594\n",
            "Epoch Step: 1700 Loss: 23.574789\n",
            "Validation perplexity: 11.177674\n",
            "Epoch 5\n",
            "Epoch Step: 0 Loss: 19.913334\n",
            "Epoch Step: 100 Loss: 19.595705\n",
            "Epoch Step: 200 Loss: 18.746689\n",
            "Epoch Step: 300 Loss: 20.135035\n",
            "Epoch Step: 400 Loss: 19.811045\n",
            "Epoch Step: 500 Loss: 19.546598\n",
            "Epoch Step: 600 Loss: 19.429571\n",
            "Epoch Step: 700 Loss: 21.027428\n",
            "Epoch Step: 800 Loss: 20.390411\n",
            "Epoch Step: 900 Loss: 20.410723\n",
            "Epoch Step: 1000 Loss: 20.569012\n",
            "Epoch Step: 1100 Loss: 21.104258\n",
            "Epoch Step: 1200 Loss: 21.283270\n",
            "Epoch Step: 1300 Loss: 22.220528\n",
            "Epoch Step: 1400 Loss: 21.877636\n",
            "Epoch Step: 1500 Loss: 23.109165\n",
            "Epoch Step: 1600 Loss: 20.284019\n",
            "Epoch Step: 1700 Loss: 19.789171\n",
            "Validation perplexity: 10.124308\n",
            "Epoch 6\n",
            "Epoch Step: 0 Loss: 19.224567\n",
            "Epoch Step: 100 Loss: 18.841959\n",
            "Epoch Step: 200 Loss: 18.219069\n",
            "Epoch Step: 300 Loss: 17.904778\n",
            "Epoch Step: 400 Loss: 17.933325\n",
            "Epoch Step: 500 Loss: 17.858284\n",
            "Epoch Step: 600 Loss: 18.849159\n",
            "Epoch Step: 700 Loss: 19.485136\n",
            "Epoch Step: 800 Loss: 18.439238\n",
            "Epoch Step: 900 Loss: 20.730291\n",
            "Epoch Step: 1000 Loss: 18.443155\n",
            "Epoch Step: 1100 Loss: 19.791924\n",
            "Epoch Step: 1200 Loss: 19.387159\n",
            "Epoch Step: 1300 Loss: 19.097630\n",
            "Epoch Step: 1400 Loss: 19.683537\n",
            "Epoch Step: 1500 Loss: 19.989258\n",
            "Epoch Step: 1600 Loss: 19.058052\n",
            "Epoch Step: 1700 Loss: 20.964828\n",
            "Validation perplexity: 9.486037\n",
            "Epoch 7\n",
            "Epoch Step: 0 Loss: 16.451662\n",
            "Epoch Step: 100 Loss: 16.750757\n",
            "Epoch Step: 200 Loss: 16.153025\n",
            "Epoch Step: 300 Loss: 18.043182\n",
            "Epoch Step: 400 Loss: 16.464048\n",
            "Epoch Step: 500 Loss: 16.644897\n",
            "Epoch Step: 600 Loss: 16.808826\n",
            "Epoch Step: 700 Loss: 18.097904\n",
            "Epoch Step: 800 Loss: 17.204248\n",
            "Epoch Step: 900 Loss: 19.154240\n",
            "Epoch Step: 1000 Loss: 18.223873\n",
            "Epoch Step: 1100 Loss: 19.451511\n",
            "Epoch Step: 1200 Loss: 18.581194\n",
            "Epoch Step: 1300 Loss: 16.944893\n",
            "Epoch Step: 1400 Loss: 19.610226\n",
            "Epoch Step: 1500 Loss: 21.483162\n",
            "Epoch Step: 1600 Loss: 19.860832\n",
            "Epoch Step: 1700 Loss: 19.718571\n",
            "Validation perplexity: 8.986047\n",
            "Epoch 8\n",
            "Epoch Step: 0 Loss: 15.222916\n",
            "Epoch Step: 100 Loss: 15.577730\n",
            "Epoch Step: 200 Loss: 15.616595\n",
            "Epoch Step: 300 Loss: 16.248096\n",
            "Epoch Step: 400 Loss: 17.692648\n",
            "Epoch Step: 500 Loss: 15.970112\n",
            "Epoch Step: 600 Loss: 16.764488\n",
            "Epoch Step: 700 Loss: 17.482092\n",
            "Epoch Step: 800 Loss: 18.746004\n",
            "Epoch Step: 900 Loss: 17.501017\n",
            "Epoch Step: 1000 Loss: 18.652176\n",
            "Epoch Step: 1100 Loss: 18.297844\n",
            "Epoch Step: 1200 Loss: 17.840353\n",
            "Epoch Step: 1300 Loss: 16.924110\n",
            "Epoch Step: 1400 Loss: 19.611380\n",
            "Epoch Step: 1500 Loss: 18.432276\n",
            "Epoch Step: 1600 Loss: 18.158903\n",
            "Epoch Step: 1700 Loss: 18.882996\n",
            "Validation perplexity: 8.688331\n",
            "Epoch 9\n",
            "Epoch Step: 0 Loss: 14.448585\n",
            "Epoch Step: 100 Loss: 14.982356\n",
            "Epoch Step: 200 Loss: 15.926925\n",
            "Epoch Step: 300 Loss: 14.888178\n",
            "Epoch Step: 400 Loss: 15.260056\n",
            "Epoch Step: 500 Loss: 16.111780\n",
            "Epoch Step: 600 Loss: 17.088121\n",
            "Epoch Step: 700 Loss: 15.733336\n",
            "Epoch Step: 800 Loss: 16.453205\n",
            "Epoch Step: 900 Loss: 17.641319\n",
            "Epoch Step: 1000 Loss: 18.001127\n",
            "Epoch Step: 1100 Loss: 18.768770\n",
            "Epoch Step: 1200 Loss: 17.070786\n",
            "Epoch Step: 1300 Loss: 17.330608\n",
            "Epoch Step: 1400 Loss: 17.243690\n",
            "Epoch Step: 1500 Loss: 17.621721\n",
            "Epoch Step: 1600 Loss: 17.474382\n",
            "Epoch Step: 1700 Loss: 17.964331\n",
            "Validation perplexity: 8.524385\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-B83Wyihlor"
      },
      "source": [
        "def greedy_decode(model, src_ids, src_lengths, max_len):\n",
        "  \"\"\"Greedily decode a sentence for EncoderDecoder. Make sure to chop off the \n",
        "     EOS token!\"\"\"\n",
        "\n",
        "  with torch.no_grad():\n",
        "    _, encoder_finals = model.encode(src_ids, src_lengths)\n",
        "    prev_y = torch.ones(1, 1).fill_(SOS_INDEX).type_as(src_ids)\n",
        "\n",
        "  output = []\n",
        "  hidden = None\n",
        "  \n",
        "  # --------- Your code here --------- #\n",
        "\n",
        "  for i in range(max_len):\n",
        "      with torch.no_grad():\n",
        "          hidden, out = model.decode(encoder_finals, prev_y, hidden)\n",
        "          output.append(torch.argmax(model.generator(out)))\n",
        "          prev_y[0][0] = output[-1]\n",
        "\n",
        "          if output[-1] == EOS_INDEX:\n",
        "              output = output[:-1]\n",
        "              break\n",
        "\n",
        "  # --------- Your code ends --------- #\n",
        "\n",
        "  return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q7IYcUjgdjeO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c366a6f9-e091-4fb9-a9a3-0a3d4c8ff364"
      },
      "source": [
        "# test on train src & trg sentences lists\n",
        "example_set = MTDataset(val_src_sentences_list, src_vocab_set,\n",
        "                        val_trg_sentences_list, trg_vocab_set)\n",
        "example_data_loader = data.DataLoader(example_set, batch_size=1, num_workers=1,\n",
        "                                      shuffle=False)\n",
        "\n",
        "\n",
        "print(\"EncoderDecoder Results:\")\n",
        "lab_utils.print_examples(pure_seq2seq, src_vocab_set, trg_vocab_set,\n",
        "                         example_data_loader, greedy_decode, n=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "EncoderDecoder Results:\n",
            "Example #1\n",
            "Src :  hi , how are you doing ? i'm getting ready to do some cheetah chasing to stay in shape . you must be very fast . hunting is one of my favorite hobbies .\n",
            "Trg :  i am ! for my hobby i like to do canning or some whittling .\n",
            "Pred:  cool . i like to play football and go to ireland and prefer concerts .\n",
            "\n",
            "Example #2\n",
            "Src :  you must be very fast . hunting is one of my favorite hobbies . i am ! for my hobby i like to do canning or some whittling .\n",
            "Trg :  i also remodel homes when i am not out bow hunting .\n",
            "Pred:  i love pixar they have the best movies .\n",
            "\n",
            "Example #3\n",
            "Src :  i am ! for my hobby i like to do canning or some whittling . i also remodel homes when i am not out bow hunting .\n",
            "Trg :  that's neat . when i was in high school i placed 6th in 100m dash !\n",
            "Pred:  aww i have not thought of that much , went cloud watching was tv .\n",
            "\n",
            "Example #4\n",
            "Src :  i also remodel homes when i am not out bow hunting . that's neat . when i was in high school i placed 6th in 100m dash !\n",
            "Trg :  that's awesome . do you have a favorite season or time of year ?\n",
            "Pred:  that's awesome . i just wish i was free in the military .\n",
            "\n",
            "Example #5\n",
            "Src :  that's neat . when i was in high school i placed 6th in 100m dash ! that's awesome . do you have a favorite season or time of year ?\n",
            "Trg :  i do not . but i do have a favorite meat since that is all i eat exclusively .\n",
            "Pred:  yes , i love them , but i do not like them , i'm more of them !\n",
            "\n",
            "Example #6\n",
            "Src :  that's awesome . do you have a favorite season or time of year ? i do not . but i do have a favorite meat since that is all i eat exclusively .\n",
            "Trg :  what is your favorite meat to eat ?\n",
            "Pred:  do you like to travel ?\n",
            "\n",
            "Example #7\n",
            "Src :  i do not . but i do have a favorite meat since that is all i eat exclusively . what is your favorite meat to eat ?\n",
            "Trg :  i would have to say its prime rib . do you have any favorite foods ?\n",
            "Pred:  i love watermelon ! what about you ?\n",
            "\n",
            "Example #8\n",
            "Src :  what is your favorite meat to eat ? i would have to say its prime rib . do you have any favorite foods ?\n",
            "Trg :  i like chicken or macaroni and cheese .\n",
            "Pred:  hmm , i spent time with my pet lizard . we are best friends .\n",
            "\n",
            "Example #9\n",
            "Src :  i would have to say its prime rib . do you have any favorite foods ? i like chicken or macaroni and cheese .\n",
            "Trg :  do you have anything planned for today ? i think i am going to do some canning .\n",
            "Pred:  i like to eat but when i can\n",
            "\n",
            "Example #10\n",
            "Src :  i like chicken or macaroni and cheese . do you have anything planned for today ? i think i am going to do some canning .\n",
            "Trg :  i am going to watch football . what are you canning ?\n",
            "Pred:  i am a professional soccer player . i wonder how its when i get married .\n",
            "\n",
            "Example #11\n",
            "Src :  do you have anything planned for today ? i think i am going to do some canning . i am going to watch football . what are you canning ?\n",
            "Trg :  i think i will can some jam . do you also play footfall for fun ?\n",
            "Pred:  i am not christian . i am a bit crazy and i love to shop .\n",
            "\n",
            "Example #12\n",
            "Src :  i am going to watch football . what are you canning ? i think i will can some jam . do you also play footfall for fun ?\n",
            "Trg :  hi , how are you doing today ?\n",
            "Pred:  i do not have any hobbies . i enjoy crocheting . watching it with my future on our farm forever\n",
            "\n",
            "Example #13\n",
            "Src :  i think i will can some jam . do you also play footfall for fun ? hi , how are you doing today ?\n",
            "Trg :  i am spending time with my 4 sisters what are you up to\n",
            "Pred:  i am doing ok . how about you ?\n",
            "\n",
            "Example #14\n",
            "Src :  hi , how are you doing today ? i am spending time with my 4 sisters what are you up to\n",
            "Trg :  wow , four sisters . just watching game of thrones .\n",
            "Pred:  i am watching television , i love cooking for family\n",
            "\n",
            "Example #15\n",
            "Src :  i am spending time with my 4 sisters what are you up to wow , four sisters . just watching game of thrones .\n",
            "Trg :  that is a good show i watch that while drinking iced tea\n",
            "Pred:  i am a barista , i love to make pies\n",
            "\n",
            "Example #16\n",
            "Src :  wow , four sisters . just watching game of thrones . that is a good show i watch that while drinking iced tea\n",
            "Trg :  i agree . what do you do for a living ?\n",
            "Pred:  i am a short order cook at the diner i work at\n",
            "\n",
            "Example #17\n",
            "Src :  that is a good show i watch that while drinking iced tea i agree . what do you do for a living ?\n",
            "Trg :  i'm a researcher i'm researching the fact that mermaids are real\n",
            "Pred:  i work at a community garden center , and do some volunteering at an animal shelter .\n",
            "\n",
            "Example #18\n",
            "Src :  i agree . what do you do for a living ? i'm a researcher i'm researching the fact that mermaids are real\n",
            "Trg :  interesting . i'm a website designer . pretty much spend all my time on the computer .\n",
            "Pred:  i could never lean you a train set .\n",
            "\n",
            "Example #19\n",
            "Src :  i'm a researcher i'm researching the fact that mermaids are real interesting . i'm a website designer . pretty much spend all my time on the computer .\n",
            "Trg :  that's cool my mom does the same thing\n",
            "Pred:  sounds like you value your solitude .\n",
            "\n",
            "Example #20\n",
            "Src :  interesting . i'm a website designer . pretty much spend all my time on the computer . that's cool my mom does the same thing\n",
            "Trg :  that's awesome . i have always had a love for technology .\n",
            "Pred:  yeah . i hear you like the show i watch .\n",
            "\n",
            "Example #21\n",
            "Src :  that's cool my mom does the same thing that's awesome . i have always had a love for technology .\n",
            "Trg :  tell me more about yourself\n",
            "Pred:  do you have any pets ?\n",
            "\n",
            "Example #22\n",
            "Src :  that's awesome . i have always had a love for technology . tell me more about yourself\n",
            "Trg :  i really enjoy free diving , how about you , have any hobbies ?\n",
            "Pred:  i really like to draw portraits of my parents\n",
            "\n",
            "Example #23\n",
            "Src :  tell me more about yourself i really enjoy free diving , how about you , have any hobbies ?\n",
            "Trg :  i enjoy hanging with my mother she's my best friend\n",
            "Pred:  i read a lot , i love books about the nicholas sparks . my favorite is my favorite .\n",
            "\n",
            "Example #24\n",
            "Src :  i really enjoy free diving , how about you , have any hobbies ? i enjoy hanging with my mother she's my best friend\n",
            "Trg :  that's nice . moms are pretty cool too .\n",
            "Pred:  i collect them , baseball and japanese cartoons , lol\n",
            "\n",
            "Example #25\n",
            "Src :  i enjoy hanging with my mother she's my best friend that's nice . moms are pretty cool too .\n",
            "Trg :  we all live in a yellow submarine , a yellow submarine . morning !\n",
            "Pred:  yeah , i am an engineer and basketball was super fun\n",
            "\n",
            "Example #26\n",
            "Src :  that's nice . moms are pretty cool too . we all live in a yellow submarine , a yellow submarine . morning !\n",
            "Trg :  hi ! that is a great line for my next stand up .\n",
            "Pred:  i love antique stores ! i love listening to country music !\n",
            "\n",
            "Example #27\n",
            "Src :  we all live in a yellow submarine , a yellow submarine . morning ! hi ! that is a great line for my next stand up .\n",
            "Trg :  lol . i am shy , anything to break the ice , and i am a beatles fan .\n",
            "Pred:  thanks ! i am studying to be a doctor . what kind of earrings ?\n",
            "\n",
            "Example #28\n",
            "Src :  hi ! that is a great line for my next stand up . lol . i am shy , anything to break the ice , and i am a beatles fan .\n",
            "Trg :  i can tell . i am not , you can see me in some tv shows\n",
            "Pred:  i admire that , but i struggle with my bipolar dog .\n",
            "\n",
            "Example #29\n",
            "Src :  lol . i am shy , anything to break the ice , and i am a beatles fan . i can tell . i am not , you can see me in some tv shows\n",
            "Trg :  really ? what shows ? i like tv , it makes me forget i do not like my family\n",
            "Pred:  do you have any hobbies ?\n",
            "\n",
            "Example #30\n",
            "Src :  i can tell . i am not , you can see me in some tv shows really ? what shows ? i like tv , it makes me forget i do not like my family\n",
            "Trg :  wow , i wish i had a big family . i grew up in a very small town .\n",
            "Pred:  i grew up in a small town in kansas and a big city .\n",
            "\n",
            "Example #31\n",
            "Src :  really ? what shows ? i like tv , it makes me forget i do not like my family wow , i wish i had a big family . i grew up in a very small town .\n",
            "Trg :  i did too . i do not get along with mine . they have no class .\n",
            "Pred:  my mom is a nurse .\n",
            "\n",
            "Example #32\n",
            "Src :  wow , i wish i had a big family . i grew up in a very small town . i did too . i do not get along with mine . they have no class .\n",
            "Trg :  just drink some cola with rum and you'll forget about them !\n",
            "Pred:  do you have any hobbies ?\n",
            "\n",
            "Example #33\n",
            "Src :  i did too . i do not get along with mine . they have no class . just drink some cola with rum and you'll forget about them !\n",
            "Trg :  put the lime in the coconut as well . . .\n",
            "Pred:  my mom and i are close , and i cannot help you with that .\n",
            "\n",
            "Example #34\n",
            "Src :  just drink some cola with rum and you'll forget about them ! put the lime in the coconut as well . . .\n",
            "Trg :  nah , plain cuba libre , that's what we drank yesterday at the theater .\n",
            "Pred:  yeah , you are right , gotta stop your big truck !\n",
            "\n",
            "Example #35\n",
            "Src :  put the lime in the coconut as well . . . nah , plain cuba libre , that's what we drank yesterday at the theater .\n",
            "Trg :  i prefer mojitos . watermelon or cucumber .\n",
            "Pred:  i am not sure if i ever do it in the best\n",
            "\n",
            "Example #36\n",
            "Src :  nah , plain cuba libre , that's what we drank yesterday at the theater . i prefer mojitos . watermelon or cucumber .\n",
            "Trg :  hi ! i work as a gourmet cook .\n",
            "Pred:  me too , i love to eat kale and cook\n",
            "\n",
            "Example #37\n",
            "Src :  i prefer mojitos . watermelon or cucumber . hi ! i work as a gourmet cook .\n",
            "Trg :  i don't like carrots . i throw them away .\n",
            "Pred:  mm i'm good eater , what is your favorite tv show ?\n",
            "\n",
            "Example #38\n",
            "Src :  hi ! i work as a gourmet cook . i don't like carrots . i throw them away .\n",
            "Trg :  really . but , i can sing pitch perfect .\n",
            "Pred:  checkers too , but other than that i don't eat much fast food .\n",
            "\n",
            "Example #39\n",
            "Src :  i don't like carrots . i throw them away . really . but , i can sing pitch perfect .\n",
            "Trg :  i also cook , and i ride my bike to work .\n",
            "Pred:  i feel you . i live in florida and hate it\n",
            "\n",
            "Example #40\n",
            "Src :  really . but , i can sing pitch perfect . i also cook , and i ride my bike to work .\n",
            "Trg :  great ! i had won an award for spelling bee .\n",
            "Pred:  i feel you . i am fat and need to meet my parents .\n",
            "\n",
            "Example #41\n",
            "Src :  i also cook , and i ride my bike to work . great ! i had won an award for spelling bee .\n",
            "Trg :  my contacts can see through what you are trying to sell me .\n",
            "Pred:  i like to swim . i am an english teacher\n",
            "\n",
            "Example #42\n",
            "Src :  great ! i had won an award for spelling bee . my contacts can see through what you are trying to sell me .\n",
            "Trg :  okay but i was published in new yorker once\n",
            "Pred:  i feel you . i am a nursing student\n",
            "\n",
            "Example #43\n",
            "Src :  my contacts can see through what you are trying to sell me . okay but i was published in new yorker once\n",
            "Trg :  you better not make any spelling mistakes .\n",
            "Pred:  not sure if i can get that from my parents in school .\n",
            "\n",
            "Example #44\n",
            "Src :  okay but i was published in new yorker once you better not make any spelling mistakes .\n",
            "Trg :  i have not . i can cook any word you want me to\n",
            "Pred:  i am in the south . on my lucky side of room .\n",
            "\n",
            "Example #45\n",
            "Src :  you better not make any spelling mistakes . i have not . i can cook any word you want me to\n",
            "Trg :  what is your ethnicity ? i'm white , and my hair is brown .\n",
            "Pred:  why would you like them ?\n",
            "\n",
            "Example #46\n",
            "Src :  i have not . i can cook any word you want me to what is your ethnicity ? i'm white , and my hair is brown .\n",
            "Trg :  i'm asian and have no hair .\n",
            "Pred:  i love them . i've 3 dogs .\n",
            "\n",
            "Example #47\n",
            "Src :  what is your ethnicity ? i'm white , and my hair is brown . i'm asian and have no hair .\n",
            "Trg :  i love hairless asians . do you like carrots ?\n",
            "Pred:  i just got hired for a brand new job next week , it was my dream\n",
            "\n",
            "Example #48\n",
            "Src :  i'm asian and have no hair . i love hairless asians . do you like carrots ?\n",
            "Trg :  i love carrots . i eat carrots like a horse .\n",
            "Pred:  i like folk music . i am a huge fan of the government though .\n",
            "\n",
            "Example #49\n",
            "Src :  i love hairless asians . do you like carrots ? i love carrots . i eat carrots like a horse .\n",
            "Trg :  are you male or female ?\n",
            "Pred:  cool . i don't have a car right now i drive to black ford truck .\n",
            "\n",
            "Example #50\n",
            "Src :  i love carrots . i eat carrots like a horse . are you male or female ?\n",
            "Trg :  i work as a gourmet cook who also has a pitch perfect voice .\n",
            "Pred:  i am female . i havea house\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXaBISR_riud"
      },
      "source": [
        "# ATTENTION\n",
        "Source code: https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vyn4R5Kprm6y"
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division\n",
        "from io import open\n",
        "import unicodedata\n",
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjP2Juxv3b-W"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.embedding = nn.Embedding(input_size, hidden_size)\n",
        "        self.gru = nn.GRU(hidden_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output = embedded\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HXKnfQTY3hf6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 351
        },
        "outputId": "5ca9afb6-e053-4c58-e742-e39232e9772f"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, self.max_length)\n",
        "        self.attn_combine = nn.Linear(self.hidden_size * 2, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.out = nn.Linear(self.hidden_size, self.output_size)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_outputs):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        attn_weights = F.softmax(\n",
        "            self.attn(torch.cat((embedded[0], hidden[0]), 1)), dim=1)\n",
        "        attn_applied = torch.bmm(attn_weights.unsqueeze(0),\n",
        "                                 encoder_outputs.unsqueeze(0))\n",
        "\n",
        "        output = torch.cat((embedded[0], attn_applied[0]), 1)\n",
        "        output = self.attn_combine(output).unsqueeze(0)\n",
        "\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.gru(output, hidden)\n",
        "\n",
        "        output = F.log_softmax(self.out(output[0]), dim=1)\n",
        "        return output, hidden, attn_weights\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-26-0b46da400ec9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-0b46da400ec9>\u001b[0m in \u001b[0;36mAttnDecoderRNN\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_p\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mMAX_LENGTH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAttnDecoderRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput_size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MAX_LENGTH' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qpsdvHq338cO"
      },
      "source": [
        "## Training Attention Encoder-Decoder Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OOV5f-3w3jXt"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_outputs[ei] = encoder_output[0, 0]\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        # Teacher forcing: Feed the target as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        # Without teacher forcing: use its own predictions as the next input\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.squeeze().detach()  # detach from history as input\n",
        "\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JghSgf9j37Qo"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, plot_every=100, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    plot_losses = []\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "    plot_loss_total = 0  # Reset every plot_every\n",
        "\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate)\n",
        "    training_pairs = [tensorsFromPair(random.choice(pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    for iter in range(1, n_iters + 1):\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        print_loss_total += loss\n",
        "        plot_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n",
        "\n",
        "        if iter % plot_every == 0:\n",
        "            plot_loss_avg = plot_loss_total / plot_every\n",
        "            plot_losses.append(plot_loss_avg)\n",
        "            plot_loss_total = 0\n",
        "\n",
        "    showPlot(plot_losses)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYfypZmi4Fr6"
      },
      "source": [
        "## Plotting results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XndPKWys4DS9"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.switch_backend('agg')\n",
        "import matplotlib.ticker as ticker\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "def showPlot(points):\n",
        "    plt.figure()\n",
        "    fig, ax = plt.subplots()\n",
        "    # this locator puts ticks at regular intervals\n",
        "    loc = ticker.MultipleLocator(base=0.2)\n",
        "    ax.yaxis.set_major_locator(loc)\n",
        "    plt.plot(points)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25nN0eZe4H98"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RN4GUusO4Hka"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_outputs = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "            encoder_outputs[ei] += encoder_output[0, 0]\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)  # SOS\n",
        "\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_words = []\n",
        "        decoder_attentions = torch.zeros(max_length, max_length)\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden, decoder_attention = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_outputs)\n",
        "            decoder_attentions[di] = decoder_attention.data\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_words, decoder_attentions[:di + 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V5xawQF84QjM"
      },
      "source": [
        "def evaluateRandomly(encoder, decoder, n=10):\n",
        "    for i in range(n):\n",
        "        pair = random.choice(pairs)\n",
        "        print('>', pair[0])\n",
        "        print('=', pair[1])\n",
        "        output_words, attentions = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words)\n",
        "        print('<', output_sentence)\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpb1c9oP4RFR"
      },
      "source": [
        "## Training and Evaluating"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_qAn5VV4T0x"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 75000, print_every=5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wc_TcYRc4Xo4"
      },
      "source": [
        "## Visualizing Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c6fv6Bsq4W9u"
      },
      "source": [
        "output_words, attentions = evaluate(\n",
        "    encoder1, attn_decoder1, \"je suis trop froid .\")\n",
        "plt.matshow(attentions.numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJWM6Ycd4bx-"
      },
      "source": [
        "def showAttention(input_sentence, output_words, attentions):\n",
        "    # Set up figure with colorbar\n",
        "    fig = plt.figure()\n",
        "    ax = fig.add_subplot(111)\n",
        "    cax = ax.matshow(attentions.numpy(), cmap='bone')\n",
        "    fig.colorbar(cax)\n",
        "\n",
        "    # Set up axes\n",
        "    ax.set_xticklabels([''] + input_sentence.split(' ') +\n",
        "                       ['<EOS>'], rotation=90)\n",
        "    ax.set_yticklabels([''] + output_words)\n",
        "\n",
        "    # Show label at every tick\n",
        "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def evaluateAndShowAttention(input_sentence):\n",
        "    output_words, attentions = evaluate(\n",
        "        encoder1, attn_decoder1, input_sentence)\n",
        "    print('input =', input_sentence)\n",
        "    print('output =', ' '.join(output_words))\n",
        "    showAttention(input_sentence, output_words, attentions)\n",
        "\n",
        "\n",
        "evaluateAndShowAttention(\"elle a cinq ans de moins que moi .\")\n",
        "\n",
        "evaluateAndShowAttention(\"elle est trop petit .\")\n",
        "\n",
        "evaluateAndShowAttention(\"je ne crains pas de mourir .\")\n",
        "\n",
        "evaluateAndShowAttention(\"c est un jeune directeur plein de talent .\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}